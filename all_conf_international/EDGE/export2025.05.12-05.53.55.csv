"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Title Page i","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","1","1","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234255","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Title Page iii","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","3","3","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234271","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Copyright Page","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","4","4","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234285","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Table of Contents","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","5","12","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234320","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Steering Committee Chair Message","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","13","13","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234258","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Congress General Chairs Message","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","14","14","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234314","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Congress Program Chairs Message","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","15","15","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234239","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"TCSVC Chair Message","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","16","16","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234307","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"EDGE 2023 General Chairs Message","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","17","18","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234292","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"EDGE 2023 Program Chairs Message","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","19","19","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234274","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"iEDGE 2023 Symposium Chairs Message","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","20","20","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234260","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"EDGE 2023 Committees","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","21","23","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234289","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"iEDGE 2023 Committees","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","24","24","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234238","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Containerized Computer Vision Applications on Edge Devices","O. I. Alqaisi; A. Şaman Tosun; T. Korkmaz","Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA; Dep. of Math. and Computer Science, The University of North Carolina, Pembroke, NC, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","1","11","The proliferation of IoT devices has led to various computer vision applications, where addressing bandwidth and latency challenges through edge nodes presents significant benefits. However, there are still existing gaps and a need for improvements to optimize IoT applications, especially in the field of computer vision, by overcoming limited resources and enhancing device performance. Addressing these challenges is essential to unlock the full potential of IoT applications in real-world scenarios. This paper evaluates the use of lightweight container technology for computer vision applications which using different algorithms, such as Haar Cascades, HOG and CNN with YOLO algorithm, on edge devices and provides a comprehensive comparison and analysis of different versions of computer vision applications in containers in terms of processing ability, and performance. It focuses on containerizing computer vision applications using Docker to achieve safe execution of multiple applications on these devices without interference and to enable flexibility, efficiency, portability, scalability, and isolation. The study also examines the resource usage, execution time, and receiving time of containerized computer vision applications. The research findings significantly advance our understanding of computer vision processing in IoT and edge computing, thereby opening up new avenues for real-time computing scenarios. These insights have the potential to drive transformative advancements in the field, enabling more efficient and accurate computer vision applications in IoT and paving the way for enhanced real-time decision-making, automation, and intelligent systems.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234300","Edge Computing;Embedded Vision;Container Technology;OpenCV Container;Docker;IoT","Performance evaluation;Computer vision;Machine vision;Scalability;Containers;Real-time systems;Libraries","","7","","46","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"COGNIT: Challenges and Vision for a Serverless and Multi-Provider Cognitive Cloud-Edge Continuum","P. Townend; A. P. Martí; I. De La Iglesia; N. Matskanis; T. O. Timoudas; T. Hallmann; A. Lalaguna; K. Swat; F. Renzi; D. Bocheński; M. Mancini; M. Bhuyan; M. González-Hierro; S. Dupont; J. Kristiansson; R. S. Montero; E. Elmroth; I. Valdés; P. Massonet; D. Olsson; I. M. Llorente; P. -O. Östberg; M. Abdou","Department of Computing Science, Umeå University, Sweden; OpenNebula Systems, Spain; IKERLAN, Spain; Centre d’Excellence en Technologies de l’Information et de la Communication (CETIC), Belgium; Digital Systems Division, Computer Science, RISE Research Institutes of Sweden, Sweden; SUSE, Germany; Aeronaval de Construcciones e Instalaciones (ACISA), Spain; Phoenix Systems, Poland; Nature 4.0 SB Srl & University of Tuscia DIBAF, Italy; Atende Industries, Poland; OpenNebula Systems, Spain & CMCC Foundation, Italy; Department of Computing Science, Umeå University, Sweden; IKERLAN, Spain; Centre d’Excellence en Technologies de l’Information et de la Communication (CETIC), Belgium; Digital Systems Division, Computer Science, RISE Research Institutes of Sweden, Sweden; OpenNebula Systems & Universidad Complutense de Madrid, Spain; Department of Computing Science, Umeå University, Sweden; IKERLAN, Spain; Centre d’Excellence en Technologies de l’Information et de la Communication (CETIC), Belgium; Digital Systems Division, Computer Science, RISE Research Institutes of Sweden, Sweden; OpenNebula Systems, Spain; Department of Computing Science, Umeå University, Sweden; OpenNebula Systems, Spain",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","12","22","Use of the serverless paradigm in cloud application development is growing rapidly, primarily driven by its promise to free developers from the responsibility of provisioning, operating, and scaling the underlying infrastructure. However, modern cloud-edge infrastructures are characterized by large numbers of disparate providers, constrained resource devices, platform heterogeneity, infrastructural dynamicity, and the need to orchestrate geographically distributed nodes and devices over public networks. This presents significant management complexity that must be addressed if serverless technologies are to be used in production systems. This position paper introduces COGNIT, a major new European initiative aiming to integrate AI technology into cloud-edge management systems to create a Cognitive Cloud reference framework and associated tools for serverless computing at the edge. COGNIT aims to: 1) support an innovative new serverless paradigm for edge application management and enhanced digital sovereignty for users and developers; 2) enable on-demand deployment of large-scale, highly distributed and self-adaptive serverless environments using existing cloud resources; 3) optimize data placement according to changes in energy efficiency heuristics and application demands and behavior; 4) enable secure and trusted execution of serverless runtimes. We identify and discuss seven research challenges related to the integration of serverless technologies with multi-provider Edge infrastructures and present our vision for how these challenges can be solved. We introduce a high-level view of our reference architecture for serverless cloud-edge continuum systems, and detail four motivating real-world use cases that will be used for validation, drawing from domains within Smart Cities, Agriculture and Environment, Energy, and Cybersecurity.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00015","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234326","serverless;FaaS;edge computing;resource management;multi-provider;cognitive cloud;open source","Adaptation models;Smart cities;Biological system modeling;Computational modeling;Europe;Serverless computing;Distributed databases","","5","","32","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"When Edge Meets FaaS: Opportunities and Challenges","R. Jin; Q. Yang; M. Zhao","IBM Almaden Research Center, San Jose, California; Arizona State University, Tempe, Arizona; Arizona State University, Tempe, Arizona",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","23","25","The proliferation of edge devices and the rapid growth of IoT data have called forth the edge computing paradigm. Function-as-a-service (FaaS) is a promising computing paradigm to realize edge computing. This paper explores the feasibility and advantages of FaaS-based edge computing. It also studies the research challenges that should be addressed in the design of such systems, which are 1) the quick decomposing and recomposing of applications, 2) the trade-off between performance and isolation of sandbox mechanisms, and 3) distributed scheduling. The challenges are illustrated by evaluating existing FaaS-based edge platforms, AWS IoT Greengrass, and OpenFaaS.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00016","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234282","Function-as-a-Service;Edge Computing;Cloud Computing;function","Processor scheduling;Edge computing","","1","","8","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Transfer-Once-For-All: AI Model Optimization for Edge","A. Kundu; L. Wynter; R. D. Lee; L. Angel Bathen","IBM Research, Singapore; IBM Research, Singapore; IBM Research, Singapore; IBM Research, USA",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","26","35","Weight-sharing neural architecture search aims to optimize a configurable neural network model (supernet) for a variety of deployment scenarios across many devices with different resource constraints. Existing approaches use evolutionary search to extract models of different sizes from a supernet trained on a very large data set, and then fine-tune the extracted models on the typically small, real-world data set of interest. The computational cost of training thus grows linearly with the number of different model deployment scenarios. Hence, we propose Transfer-Once-For-All (TOFA) for supernet-style training on small data sets with constant computational training cost over any number of edge deployment scenarios. Given a task, TOFA obtains custom neural networks, both the topology and the weights, optimized for any number of edge deployment scenarios. To overcome the challenges arising from small data, TOFA utilizes a unified semi-supervised training loss to simultaneously train all subnets within the supernet, coupled with on-the-fly architecture selection at deployment time.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234286","neural architecture search;inference at the edge;semi-supervised supernet training;weight-sharing NAS","Training;Network topology;Computational modeling;Neural networks;Computer architecture;Data models;Topology","","4","","20","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Spica: Exploring FPGA Optimizations to Enable an Efficient SpMV Implementation for Computations at Edge","D. Ramchandani; B. Asgari; H. Kim","HPE; University of Maryland, College Park; Georgia Institute of Technology",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","36","42","With the emergence of FPGA boards equipped with high bandwidth memory (HBM), these boards have become more attractive for implementing memory-intensive computational kernels such as sparse matrix-vector multiplication (SpMV), with a wide range of applications in edge computations from deep learning to robotics. Specialized implementation of SpMV on FPGAs enables efficient utilization of the limited resources in edge systems. High-level synthesis (HLS) compilers, on the other hand, have eased the programming of FPGAs, leading to a faster development cycle. Even though the programming of FPGAs has become easier, obtaining maximum throughput even for the straightforward kernel of SpMV still requires careful optimizations. Therefore, this paper explores the impact of deploying various optimization techniques such as temporal parallelism, spatial parallelism, and memory alignment to help SpMV fully utilize the available memory bandwidth of HBM on a Xilinx FPGA board to achieve close-to-peak throughput without wasting the resources. We conclude the optimizations by suggesting Spica, a high-throughput tree-based SpMV implementation.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234304","High Bandwidth Memory (HBM);FPGA;Sparse Problem Acceleration;SpMV;High-Level Synthesis (HLS)","Bandwidth;Programming;Parallel processing;Throughput;Computational efficiency;Sparse matrices;Kernel","","1","","32","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"DOSA: Organic Compilation for Neural Network Inference on Distributed FPGAs","B. Ringlein; F. Abel; D. Diamantopoulos; B. Weiss; C. Hagleitner; D. Fey",IBM Research Europe; IBM Research Europe; IBM Research Europe; IBM Research Europe; IBM Research Europe; Friedrich-Alexander University Erlangen-Nürnberg,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","43","50","The computational requirements of artificial intelligence workloads are growing exponentially. In addition, more and more compute is moved towards the edge due to latency or localization constraints. At the same time, Dennard scaling has ended and Moore’s law is winding down. These trends created an opportunity for specialized accelerators including field-programmable gate arrays (FPGAs), but the poor support and usability of today’s tools prevents FPGAs from being deployed at scale for deep neural network (DNN) inference applications. In this work, we propose an organic compiler — DOSA — that drastically lowers the barrier for deploying FPGAs. DOSA builds on the operation set architecture concept and integrates the DNN accelerator components generated by existing DNN-to-FPGA frameworks to produce an overall efficient solution. DOSA starts from DNNs represented in the community standard ONNX and automatically implements model- and data-parallelism, based on the performance targets and resource footprints provided by the user. Deploying a DNN using DOSA on 9 FPGAs exhibits a speedup of up to 52 times compared to a CPU and 18 times compared to a GPU.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234270","MLSys;Reconfigurable hardware;Domain-specific architectures;Compilers;Distributed Artificial Intelligence;Design Tools and Techniques","Windings;Graphics processing units;Artificial neural networks;Computer architecture;Market research;Energy efficiency;Artificial intelligence","","","","60","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"SQuBA: Social Quorum Based Access Control for Open IoT Environments","Y. Wang; A. Chandra; J. Weissman",University of Minnesota; University of Minnesota; University of Minnesota,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","51","62","Internet of things (IoT) devices have been ubiquitous in recent years. An emerging model for IoT deployment is an open edge-based infrastructure. Edge resources are commonly used to coordinate capabilities and manage access due to IoT device resource limitations and IoT vendor heterogeneity. The open IoT environment often exists in a multi-user setting, where multiple users interact with a single IoT device. In this setting, we assume that none of the users or the edges are fully trusted, thus IoT data privacy may be compromised. Limited attention has been paid to authorization and auditing in this environment. However, exploiting inter-user relationships gives us leverage. In this work, we propose a social quorum based architecture, SQuBA, as an access control mechanism for IoT which provides relationship-driven authorization and auditing. We present a tiered approach to support access control rules and relationship-based trustworthiness. We implemented a prototype and carried out experiments using a real-world dataset under various scenarios and configurations. The results demonstrate both SQuBA’s promising near real-time response latency that is in the order of milliseconds, and good resilience to different edge faulty models. We also compare with various baselines and SQuBA is able to improve end-to-end latency by up to 10X and tolerate the number of faulty edges by up to 2X.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234264","Edge Computing;Internet of Things;Privacy;Access Control;Distributed Ledger Technologies","Authorization;Privacy;Data privacy;Prototypes;Computer architecture;Real-time systems;Internet of Things","","","","58","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"A Survey of Faults and Fault-Injection Techniques in Edge Computing Systems","M. Pourreza; P. Narasimhan","Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, USA; Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, USA",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","63","71","Edge computing has emerged in recent years to reduce latency, conserve bandwidth, and enhance privacy for applications. As more edge computing applications are being deployed, there is a growing need to ensure fault tolerance for such systems. To achieve fault-tolerant edge computing, understanding potential faults and fault-injection methods is crucial. In this paper, we surveyed 76 research publications in over 25 top conferences and journals published during the last 6 years with a focus on fault-tolerant edge computing. Through our analysis, we identified the most relevant faults as well as the common practical fault-injection techniques, simulation frameworks, and benchmarks used in edge computing. Our key insights from this survey underscore the importance of handling edge-centric faults including performance, resource-stressing, and network-partition faults as well as developing edge-centric fault-injection methods and dependability benchmarks in future studies.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234283","Edge Computing;Fault Model;Fault Injection;Fault Tolerance;Dependability","Surveys;Fault diagnosis;Fault tolerance;Privacy;Computational modeling;Fault tolerant systems;Bandwidth","","4","","93","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"ConPrEF: A Context-based Privacy Enforcement Framework for Edge Computing","G. Sirigu; B. Carminati; E. Ferrari","University of Insubria, Varese, Italy; University of Insubria, Varese, Italy; University of Insubria, Varese, Italy",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","72","78","Edge computing is an emerging computational paradigm where edge nodes provide services to users performing computation-in-place. It allows faster computation, better support for real-time applications and can simplify the implementation of security measures. Concerning individual privacy, a relevant requirement is giving users more control over how their data is used. It is important to check compliance between user privacy preferences and provider privacy policy. However, the typical edge computing application scenario is dynamic, with users in constant motion, changing their location and time at which they connect to the edge node as well as the situation under which they connect. This makes the common notion of privacy preference compliance insufficient. To address this issue, we provide a framework for allowing users to define their privacy preferences according to a rich set of contextual features. We also demonstrate the feasibility of our solution through realistic and synthetic tests.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234241","Edge Computing;Privacy","Privacy;Data privacy;Image edge detection;Dynamics;Real-time systems;Inference algorithms;Blocklists","","1","","28","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"NEOS: Non-intrusive Edge Observability stack based on Zero Trust security model for Ubiquitous Computing","A. Kumar; T. Ahmed; K. Saini; J. Kumar","Intel Corporation, Bengaluru, India; Intel Corporation, Bengaluru, India; Intel Corporation, Bengaluru, India; Intel Corporation, Bengaluru, India",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","79","84","The Edge computing paradigm has emerged as the new industrial norm for creating distributed applications. These distributed applications need to target high reliability and scalability to meet the goals and requirements of the users. Achieving this definitely requires a real time observability stack to closely observe, track, debug and improve the application. In this paper we introduce the Non-Intrusive Edge Observability Stack(NEOS) that simplifies the process of collecting, analyzing, and visualizing telemetry data. It reduces the amount of code instrumentation needed to collect telemetry data up to 80% and offers extensive configuration capabilities within the subcomponents of the process. It offers a set of user-friendly abstractions and easy-to-use APIs, which minimizes the effort needed for manual instrumentation of application code. NEOS leverages popular open-source tools such as OpenTelemetry, Grafana, Prometheus, Jaeger, and Loki, for the collecting and visualizing of telemetry data. Furthermore, NEOS implements security based on the zero-trust model, which means that we assume that no user or system can be trusted by default. The security of every connection establised in NEOS employs mutual Transport Layer Security (mTLS) to prevent unauthorized access and safeguard sensitive data. Experiments were conducted to assess the efficiency of the stack by comparing the time and effort needed to instrument code with and without the stack. The outcomes showed a considerable reduction in instrumentation code. NEOS can be used by product managers, engineering and operation team for system and application health monitoring, real-time business insights, and debugging system.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234294","Observability;Edge Observability Stack;monitoring;zero-trust security model;e2e solution","Codes;Instruments;Computational modeling;Data visualization;Real-time systems;Libraries;Telemetry","","1","","6","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"MECBench: A Framework for Benchmarking Multi-Access Edge Computing Platforms","O. Naman; H. Qadi; M. Karsten; S. Al-Kiswany","University of Waterloo, Canada; University of Waterloo, Canada; University of Waterloo, Canada; University of Waterloo, Canada",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","85","95","We present MECBench, an extensible benchmarking framework for multi-access edge computing. MECBench is configurable, and can emulate networks with different capabilities and conditions, can scale the generated workloads to mimic a large number of clients, and can generate a range of workload patterns. MECBench is extensible; it can be extended to change the generated workload, use new datasets, and integrate new applications. MECBench’s implementation includes machine learning and synthetic edge applications.We demonstrate MECBench’s capabilities through two scenarios: an object detection scheme for drone navigation and a natural language processing application. Our evaluation shows that MECBench can be used to answer complex what-if questions pertaining to design and deployment decisions of MEC platforms and applications. Our evaluation explores the impact of different combinations of applications, hardware, and network conditions, as well as the cost-benefit tradeoff of different designs and configurations.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234277","Multi Access Edge Computing;Benchmarking;Edge Computing;Machine learning performance;Network Emulation;Containers","Multi-access edge computing;Navigation;Image edge detection;Computational modeling;Object detection;Pricing;Machine learning","","1","","32","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"IEEE P1935 Edge/Fog Manageability and Orchestration: Standard and Usage Example","T. -Y. Chen; Y. Chiang; J. -H. Wu; H. -T. Chen; C. -C. Chen; H. -Y. Wei","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","96","103","With the innovation of mobile applications and the arrival of the fifth generation of telecommunication, edge computing has become a popular scheme due to its geographical proximity to end users; thus, the overall architecture has the advantage of lower latency and higher user experience. However, because of the physical limitation, management and orchestration in the edge system are necessary to maintain operations, standard functionalities and application lifecycle. Accordingly, the IEEE P1935 working group introduced a standardized, orchestration-wise design to simplify the process and offer better system performance and user experience. In this work, we first give an overview of the three-level Edge/Fog architecture in the P1935 standard and briefly show the components in each level. Moreover, we describe the mechanisms for managing and orchestrating resources and applications. Last but not least, we implement an actual testbed following the standard. To show that the P1935 standard can benefit the Edge/Fog systems, we deploy an edge-based live-streaming service and a real-time transcoding mechanism on the testbed. The evaluation results show that the P1935-aligned system performs well regarding the overall Quality of Experience (QoE).","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234251","edge computing;management and orchestration;standardization;live streaming;application lifecycle management;resource lifecycle management","Technological innovation;System performance;Computer architecture;Transcoding;User experience;Real-time systems;Mobile applications","","4","","20","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Perception Workload Characterization and Prediction on the Edges with Memory Contention for Connected Autonomous Vehicles","S. Tang; S. Wang; S. Fu; Q. Yang","Department of Computer Science and Engineering, University of North Texas, Denton, Texas, USA; Department of Computer Science and Engineering, University of North Texas, Denton, Texas, USA; Department of Computer Science and Engineering, University of North Texas, Denton, Texas, USA; Department of Computer Science and Engineering, University of North Texas, Denton, Texas, USA",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","104","114","Vehicular Edge computing requires computational power from connected Edge devices in the network to process incoming vehicle work requests. This connection and offloading allows for faster and more efficient data processing and thus improves the safety, performance, and reliability of the connected vehicles. Existing works focus on the processor and its characterization, but they forgo the connecting components. Memory resource and storage resource is limited on Edge devices, and the two combined incur a heavy impact on deep learning. This is prominent as perception-based workloads have yet to be studied deeply. In our characterization, we have found that memory contention can be split into 3 behaviors. Each of these behaviors interacts with the other resources differently. Then, in our deep neural network (DNN) layer analysis, we find several layers that see computation time increases of over 2849% for convolutional layers and 1173.34% for activation layers. Through the characterization, we can model the workload behavior for the Edge based on the device configuration and the workload requirements. Through this, the impacts of memory contention and its impacts are quantified. To the best of our knowledge, this is the first such work that characterizes the memory impacts towards vehicular edge computational workloads with a deep focus on memory and DNN layers.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00026","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234325","Edge Computing;Deep Learning;Autonomous Vehicles;Object Detection;Workload Characterization;Memory Contention","Performance evaluation;Deep learning;Connected vehicles;Image edge detection;Artificial neural networks;Data processing;Behavioral sciences","","","","42","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Edge Computing Tasks Orchestration: An Energy-Aware Approach","J. L. Thomsen; K. Dragsbæk Schmidt Thomsen; R. B. Schmidt; S. D. Jakobsgaard; T. Beregaard; M. Albano; S. Moreschini; D. Taibi","Aalborg University, Denmark; Aalborg University, Denmark; Aalborg University, Denmark; Aalborg University, Denmark; Aalborg University, Denmark; Aalborg University, Denmark; Tampere University, Tampere, Finland; University of Oulu Oulu. Finland Tampere University, Tampere, Finland",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","115","117","In this paper, we investigate experimentally the use of auctioning as a method for optimizing task orchestration in distributed computing systems by making selfish agents compete to execute computational tasks. Our goal is to find an approach that can improve the performance of these systems, using a deadline, fines, and reward limits in a reverse second-price sealed bid auction, to incentive and control the system, specifically in terms of improving task throughput and power consumption. With improvements to both energy consumption and task throughput, we have developed a promising approach, that is able to scale with the number of machines in the system. Results suggest that this type of auction may be useful for improving the implementation of these systems in a wide range of scenarios.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234280","auction;deadline;game theory;task offloading;experiments","Energy consumption;Power demand;Throughput;Control systems;Task analysis;Distributed computing;Edge computing","","1","","11","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Domain modeling for scenario sensing and edge decision-making","H. Shi; S. Liu; L. Pan","School of Software Shandong University, Jinan, China; School of Software Shandong University, Jinan, China; School of Software Shandong University, Jinan, China",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","118","125","The introduction of numerous edge computing nodes allows application systems to sense and make decisions in real-time but also brings new challenges. The diversity of application scenarios and the complexity of application processes can be effectively addressed through modeling. This paper proposes a modeling approach for manufacturing scenario sensing and edge decision-making. Firstly, an abstract meta-model (SMM) is defined, which provides a unified description of the resources and processes in the scenario and the interaction between the scenario and the edge. Based on the meta-model, an application scenario model (ASM) can be constructed for a specific scenario to support edge data feedback and decision-making for abnormal events. In addition, the model is constructed in a scenario modeling tool and validated in a simulated manufacturing production line, that is, whether the models can provide effective support for decision-making of abnormal events. The results demonstrate that mapping normalized models into codes at the edge computing nodes can improve the accuracy and real-time performance of decision-making.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00028","Ministry of Science and Technology; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234295","Scenario modeling;Edge computing;Flexible manufacturing;Meta-model;Knowledge graph","Codes;Computational modeling;Decision making;Production;Real-time systems;Data models;Sensors","","1","","25","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Creating Robust Deep Neural Networks with Coded Distributed Computing for IoT","R. Hadidi; J. Cao; B. Asgari; H. Kim",Rain AI; Georgia Tech; University of Maryland; Georgia Tech,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","126","132","The increasing interest in serverless computation and ubiquitous wireless networks has led to numerous connected devices in our surroundings. Such IoT devices have access to an abundance of raw data, but their inadequate resources in computing limit their capabilities. With the emergence of deep neural networks (DNNs), the demand for the computing power of IoT devices is increasing. To overcome inadequate resources, several studies have proposed distribution methods for IoT devices that harvest the aggregated computing power of idle IoT devices in an environment. However, since such a distributed system strongly relies on each device, unstable latency, and intermittent failures, the common characteristics of IoT devices and wireless networks, cause high recovery overheads. To reduce this overhead, we propose a novel robustness method with a close-to-zero recovery latency for DNN computations. Our solution never loses a request or spends time recovering from a failure. To do so, first, we analyze how matrix computations in DNNs are affected by distribution. Then, we introduce a novel coded distributed computing (CDC) method, the cost of which, unlike that of modular redundancies, is constant when the number of devices increases. Our method is applied at the library level, without requiring extensive changes to the program, while still ensuring a balanced work assignment during distribution.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234298","Edge AI;Reliability;IoT;Edge;Distributed Computing;Collaborative Edge & Robotics","Computational modeling;Atmospheric modeling;Wireless networks;Redundancy;Artificial neural networks;Ubiquitous computing;Robustness","","2","","27","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Seque: Lean and Energy-aware Data Management for IoT Gateways","P. -L. Sixdenier; S. Wildermann; M. Ottens; J. Teich","Friedrich-Alexander-Universität Erlangen-Nurnberg, Erlangen, Germany; Friedrich-Alexander-Universität Erlangen-Nurnberg, Erlangen, Germany; Friedrich-Alexander-Universität Erlangen-Nurnberg, Erlangen, Germany; Friedrich-Alexander-Universität Erlangen-Nurnberg, Erlangen, Germany",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","133","139","IoT systems with multiple deployed sensor nodes often use gateways to gather, fuse, transform and transmit diverse data acquired from the sensor nodes, e.g., to a cloud server. When being deployed in remote environments, not only the memory and storage, but also energy can be scarce and supply be time-dependent and often unpredictable, e.g. when obtained by energy harvesting. In this realm, this paper proposes a lean and energy-aware methodology called Seque for data management for such gateways. Rather than processing multiple sensor requests at a time and being unconscious of the level of available energy, Seque schedules only one request at a time. Moreover, Seque dynamically decides whether to directly process and transmit data of a request to a cloud server, or alternatively compress and persist data locally on the gateway in expectation of a power failure to postpone the upload to time of recovery from a power shortage. With this scheduling technique, a guarantee can be given that no sensor request admitted will suffer from partial or full loss of data. A reference implementation of Seque is provided with scheduling decisions being calibrated based on energy models of sensor interfaces, CPU system and upload interfaces of a real embedded gateway platform. Presented analysis on whether energy can be sent most by selective compression of data. Finally, the lightweight approach is evaluated in terms of energy consumption, and storage footprint and compared with commercially available database management systems including MongoDB and SQLite. The evaluation shows that Seque provides on average between 51% and 63% lower energy consumption for different data schemas per sensor request and also between 63% and 78% of lower storage requirements, pronouncing its leanness.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00030","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234263","IoT;Power Management;Gateway;Energy Harvesting;Data Management","Energy consumption;Cloud computing;Schedules;Transforms;Logic gates;Market research;Servers","","1","","21","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Probabilistic Error Reasoning on IoT Edge Devices","C. Q. Cao; Y. Feng","Department of Electrical Engineering and Computer Science, University of Tennessee; Department of Computer Science and Engineering, University of North Texas",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","140","149","Existing IoT applications are increasingly using sensors to collect real-world measurements to make decisions. Such measurements are inherently limited by the accuracy of ADC devices, hence, introduce noise and errors. However, application developers often choose scalar data to represent sensor readings without regard to the errors associated with such data. This gives the illusion that the measurements are error-free, leading to error accumulation and false positive results. In this paper, we present a new type of programming abstraction for modeling errors and performing inference tasks in measurements of the physical world on resource-constrained IoT devices, which we call approximation variables (approxes). Using approxes does not require any changes to the programming language itself. Instead, it is designed as a suite of library functions that can be integrated directly into existing programming practices. We demonstrate how to use it in C programs. This framework makes decisions about the distributions of parameter values and inherently supports sampling and hypothesis testing to evaluate the accuracy of computational results. We compare its use to traditional programming practices and show how the library can be used to reveal uncertainty to the user, so that it can handle errors, reduce false positive results, and lead to better decision-making. These benefits make approxes a compelling and promising solution for programming with noisy sensor measurements for modern IoT applications.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234318","IoT;Approximate Programming","Uncertainty;Measurement uncertainty;Decision making;Programming;Probabilistic logic;Libraries;Sensors","","","","35","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"LightESD: Fully-Automated and Lightweight Anomaly Detection Framework for Edge Computing","R. Das; T. Luo","Department of Computer Engineering, Missouri University of Science and Technology, Rolla, USA; Department of Computer Science, Missouri University of Science and Technology, Rolla, USA",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","150","158","Anomaly detection is widely used in a broad range of domains from cybersecurity to manufacturing, finance, and so on. Deep learning based anomaly detection has recently drawn much attention because of its superior capability of recognizing complex data patterns and identifying outliers accurately. However, deep learning models are typically iteratively optimized in a central server with input data gathered from edge devices, and such data transfer between edge devices and the central server impose substantial overhead on the network and incur additional latency and energy consumption. To overcome this problem, we propose a fully-automated, lightweight, statistical learning based anomaly detection framework called LightESD. It is an on-device learning method without the need for data transfer between edge and server, and is extremely lightweight that most low-end edge devices can easily afford with negligible delay, CPU/memory utilization, and power consumption. Yet, it achieves highly competitive detection accuracy. Another salient feature is that it can auto-adapt to probably any dataset without manually setting or configuring model parameters or hyperparameters, which is a drawback of most existing methods. We focus on time series data due to its pervasiveness in edge applications such as IoT. Our evaluation demonstrates that LightESD outperforms other SOTA methods on detection accuracy, efficiency, and resource consumption. Additionally, its fully automated feature gives it another competitive advantage in terms of practical usability and generalizability.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234322","Extreme studentized deviate;anomaly detection;on-device learning;periodicity detection;edge computing","Training;Deep learning;Image edge detection;Time series analysis;Data transfer;Data models;Servers","","2","","40","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Towards Autonomous Anomaly Management Using Semantic Technologies at the Edge","J. de Oliveira; C. Calle; P. Calvez; O. Curé","Université Gustave Eiffel, LIGM, France; Université Gustave Eiffel, LIGM, France; Engie Crigen - Lab CSAI; Université Gustave Eiffel, LIGM, France",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","159","165","We present an approach that autonomously adapts sensor monitoring of an IoT environment. Based on semantic technologies, our solution supports the generation of relevant continuous queries when certain anomalies are identified. The generation consists of a query graph extension which is triggered when some rules are fired. These queries are executed on a graph database system designed for Edge computing. We evaluate the accuracy of the generated queries, the robustness, and the latency of our system in a real use case consisting of a smart building context equipped with multiple sensors.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234272","Edge Computing;knowledge graph;Adaptability;SHACL","Smart buildings;Image edge detection;Robustness;Database systems;Servers;Monitoring;Anomaly detection","","1","","17","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Accurate Calibration of Power Measurements from Internal Power Sensors on NVIDIA Jetson Devices","N. Shalavi; A. Khoshsirat; M. Stellini; A. Zanella; M. Rossi","Department of Information Engineering, University of Padova, Italy; Department of Information Engineering, University of Padova, Italy; Department of Information Engineering, University of Padova, Italy; Department of Information Engineering, University of Padova, Italy; Department of Information Engineering, University of Padova, Italy",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","166","170","Power efficiency is a crucial consideration for embedded systems design, particularly in the field of edge computing and IoT devices. This study aims to calibrate the power measurements obtained from the built-in sensors of NVIDIA Jetson devices, facilitating the collection of reliable and precise power consumption data in real-time. To achieve this goal, accurate power readings are obtained using external hardware, and a regression model is proposed to map the sensor measurements to the true power values. Our results provide insights into the accuracy and reliability of the built-in power sensors for various Jetson edge boards and highlight the importance of calibrating their internal power readings. In detail, internal sensors underestimate the actual power by up to 50% in most cases, but this calibration reduces the error to within ±3%. By making the internal sensor data usable for precise online assessment of power and energy figures, the regression models presented in this paper have practical applications, for both practitioners and researchers, in accurately designing energy-efficient and autonomous edge services.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234257","Power measurements;IoT;Embedded systems;Edge Computing;NVIDIA Jetson","Power measurement;Power demand;Computational modeling;Sensor systems;Real-time systems;Sensors;Calibration","","1","","13","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Weighted Load Balancing Method for Heterogeneous Clusters on Hybrid Clouds","K. Hagiwara; Y. Li; M. Sugaya","Department of Computer Science and Engineering, Shibaura Institute of Technology, Tokyo, Japan; Department of Computer Science and Engineering, Shibaura Institute of Technology, Tokyo, Japan; Department of Computer Science and Engineering, Shibaura Institute of Technology, Tokyo, Japan",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","171","176","In recent years, edge device and AI services have been expected to utilize scalable cloud computing to handle large amounts of processing. In the cloud, load-balancing techniques distribute the load evenly to many nodes to achieve high throughput. At the same time, the shift to hybrid cloud computing requires additional nodes with different generations or different types of computational resources to achieve high performance in an environment with heterogeneous computational performance. Heterogeneity raises concerns that the current uniform load-balancing will result in overloaded or underloaded nodes, which degrade responsiveness. Therefore, this study proposes a weighted load-balancing method to improve responsiveness in clusters with nonuniform computational performance. The proposed method is effective in improving the average response time by about 20%, the maximum response time by about 45%, and the response time variance by about 70% compared to load-balancing with a load balancer developed by Google.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00035","Japan Science and Technology Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234248","Heterogeneous Cluster;Dynamic Weighted Load-Balancing;Maglev;Hybrid Cloud;Benchmark-based Load-Balancing","Cloud computing;Throughput;Load management;Hybrid power systems;Time factors;Artificial intelligence;Edge computing","","1","","15","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Fault Tolerant Horizontal Computation Offloading","A. Droob; D. Morratz; F. L. Jakobsen; J. Carstensen; M. Mathiesen; R. Bohnstedt; M. Albano; S. Moreschini; D. Taibi","Aalborg University, Aalborg, Denmark; Aalborg University, Aalborg, Denmark; Aalborg University, Aalborg, Denmark; Aalborg University, Aalborg, Denmark; Aalborg University, Aalborg, Denmark; Aalborg University, Aalborg, Denmark; Aalborg University, Aalborg, Denmark; Tampere University, Tampere, Finland; University of Oulu, Oulu, Finland",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","177","182","The broad development and usage of edge devices has highlighted the importance of creating resilient and computationally advanced edge-to-cloud continuum environments. When working with edge devices these desiderata are usually achieved through replication and offloading. This paper reports on the design and implementation of a fault-tolerant service that enables the offloading of jobs from devices with limited computational power. We propose a solution that allows users to upload jobs through a web service, which will be executed on edge nodes within the system. The solution is designed to be fault tolerant and scalable, with no single point of failure as well as the ability to accommodate growth, if the service is expanded. The use of Docker checkpointing on the worker machines ensures that jobs can be resumed in the event of a fault. We provide a mathematical approach to optimize the number of checkpoints that are created along a computation, given that we can forecast the time needed to execute a job. We present experiments that indicate in which scenarios checkpointing benefits job execution. Our experiments shows the benefits of using checkpointing and restore when the completion jobs’ time rises compared with the forecast fault rate.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234242","checkpointing;edge nodes;workers;orchestration;replication;totally ordered multicast","Checkpointing;Performance evaluation;Fault tolerance;Time-frequency analysis;Web services;Fault tolerant systems;Resumes","","6","","15","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Offload Shaping for Wearable Cognitive Assistance","R. Iyengar; Q. Dong; C. Nguyen; P. Pillai; M. Satyanarayanan",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Intel Labs; Carnegie Mellon University,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","183","189","Edge computing has much lower elasticity than cloud computing because cloudlets have much smaller physical and electrical footprints than a data center. This hurts the scalability of applications that involve low-latency edge offload. We show how this problem can be addressed by leveraging the growing sophistication and compute capability of recent wearable devices. We investigate four Wearable Cognitive Assistance applications on three wearable devices, and show that the technique of offload shaping can significantly reduce network utilization and cloudlet load without compromising accuracy or performance.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00037","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234234","Computer Vision;Machine Learning;Offloading;Wearable Computing;Mobile Computing;Edge Computing;IoT;Cloudlet;Augmented Reality;5G;Wi-Fi","Performance evaluation;Cloud computing;Power demand;Wearable computers;Scalability;Pipelines;Bandwidth","","1","","19","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"An Orchestrator Architecture for Multi-tier Edge/Cloud Video Streaming Services","E. S. Gama; N. B. V; R. Immich; L. F. Bittencourt","Institute of Computing, State University of Campinas (UNICAMP), Brazil; Data Science and Computer Applications, MIT, MAHE, Manipal, India; Federal University of Rio Grande do Norte (UFRN), Brazil; Institute of Computing, State University of Campinas (UNICAMP), Brazil",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","190","196","Video streaming has become a prevalent form of entertainment and a vital means of communication, but the challenges of delivering high quality video content over the internet are numerous. One of the key challenges is the varying network conditions that can significantly impact video streaming quality, such as bandwidth fluctuations and packet loss. To overcome these challenges, an adaptive video streaming architecture is needed to adjust the video streaming in real-time to match the changing network conditions and ensure a high Quality of Experience (QoE) for the end-user. This article presents MIGRATE, an orchestrator architecture for video streaming services capable of adapting to user demand in real-time. The study considers an edge/cloud multi-tier network infrastructure. In addition, an Integer Linear Programming (ILP) model and a Greedy solution are proposed to decide the distribution of connections between users and services. Experimental results show that based on the optimization strategy used, it is observed that there is a trade-off between the resources used and the QoE provided to users. Further, we discuss the importance of considering QoE metrics and user engagement in designing video streaming systems.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234301","Cloud computing;Edge computing;Quality of Experience;Video Streaming;Video-on-Demand","Fluctuations;Packet loss;Entertainment industry;Computer architecture;Streaming media;Integer linear programming;Real-time systems","","3","","22","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Performance Analysis of Real-Time Video Surveillance Application Leveraging Edge and Cloud","P. Thakkar; A. S. Patel; G. Shukla; A. A. Kherani; B. Lall","Bharti School of Telecomm Technology & Mgmt, Indian Institute of Technology Delhi, New Delhi, India; Bharti School of Telecomm Technology & Mgmt, Indian Institute of Technology Delhi, New Delhi, India; Bharti School of Telecomm Technology & Mgmt, Indian Institute of Technology Delhi, New Delhi, India; Department of EECS, Indian Institute of Technology Bhilai, Raipur, India; Department of Electrical Engineering, Indian Institute of Technology Delhi, New Delhi, India",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","197","203","With the advent of the Edge and Cloud server in the 5G system, an application needs to be designed to have multiple components, where a part of it (Data Intensive Component (DIC)) is executed on the Edge server while the other part (Computation Intensive Component (CIC)) is executed on the Cloud server. Such deployment of the applications’ components into the Edge and Cloud server opens up opportunities for managing the Edge, Cloud, and network resources. In this work, performance aspects of the simultaneous deployment of a video surveillance application on the Edge and Cloud server are explored. Furthermore, application placement approach at the Edge and Cloud server based on the service time requirement of an application is demonstrated. In addition, an adaptive data transmission mechanism at the Edge server is presented, where the components that run at the Edge server use a scaled-down version of the video based on Initial Analysis, reducing the bandwidth consumption between the Edge server and UE. As a use-case, a surveillance application to identify traffic violations (jumping signal) is deployed. The performance of the simultaneous deployment of video surveillance application (Edge-cloud approach) is evaluated by demonstrating bandwidth preserved and end-to-end bandwidth requirement in comparison with the different Cloud only approaches. To simulate actual deployments, the surveillance application is deployed on an ETSI-compliant 5G MEC testbed with the Edge and Cloud server.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234230","5G and beyond;Edge Computing;MEC;Video surveillance","5G mobile communication;Image edge detection;Bandwidth;Traffic control;Video surveillance;Real-time systems;Performance analysis","","","","22","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"eDashA: Edge-based Dash Cam Video Analytics","J. King; Y. C. Lee","School of Computing, Macquarie University, Australia; School of Computing, Macquarie University, Australia",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","204","206","While the real-time analysis of dash cam video is of great practical importance for improving road safety, commercial dash cams lack the resources necessary to perform such video analytics. It is impractical to use clouds for this due to high latency and high bandwidth consumption. In this paper, we present eDashA, the first edge-based system that demonstrates the potential of near real-time video analytics using a network of mobile devices, on the move. In particular, it simultaneously processes videos produced by two dash cams of different angles (outward facing and inward facing dash cams) with one or more mobile devices on the move. Further, we devise several optimization techniques and incorporated them into eDashA. These techniques are simultaneous download and analysis, scheduling, segmentation and early stopping. We have implemented eDashA as an Android app and evaluated it using two dash cams and several heterogeneous smartphones. Experiment results show the feasibility of real-time video analytics on the move.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234312","dashboard cameras;video analytics;edge computing;distributed processing;real-time","Performance evaluation;Visual analytics;Streaming media;Real-time systems;Road safety;Object tracking;Object recognition","","3","","13","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Improved Knowledge Distillation for Crowd Counting on IoT Devices","Z. Huang; R. O. Sinnott","School of Computing and Information Systems, The University of Melbourne, Parkville, Australia; School of Computing and Information Systems, The University of Melbourne, Parkville, Australia",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","207","214","Manual crowd counting for real-world problems is impossible or results in wildly inaccurate estimations. Deep learning is one area that has been applied to address this issue. Crowd counting is a computationally intensive task. Therefore, many crowd counting models employ large-scale deep convolutional neural networks (CNN) to achieve higher accuracy. However, these are typically at the cost of performance and inference speed. This makes such approaches difficult to apply in real-world settings, e.g., on Internet-of-Things (IoT) devices. To tackle this problem, one method is to compress models using pruning and quantization or use of lightweight model backbones. However, such methods often result in a significant loss of accuracy. To address this, some studies have explored knowledge distillation methods to extract useful information from large state-of-the-art (teacher) models to guide/train smaller (student) models. However, knowledge distillation methods suffer from the problem of information loss caused by hint-transformers. Furthermore, teacher models may have a negative impact on student models. In this work, we propose a method based on knowledge distillation that uses self-transformed hints and loss functions that ignore outliers to tackle real-world and challenging crowd counting tasks. Based on our approach, we achieve a MAE of 77.24 and a MSE of 276.17 using the JHU-CROWD++ [1] test set. This is comparable to state-of-the-art deep crowd counting models, but at a fraction of the original model size and complexity, thus making the solution suitable for IoT devices. The source code is available at https://github.com/huangzuo/effcc_distilled.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00041","University of Melbourne; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234317","Crowd counting;Deep learning;Knowledge distillation","Performance evaluation;Quantization (signal);Computational modeling;Source coding;Estimation;Manuals;Internet of Things","","","","35","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"FedCime: An Efficient Federated Learning Approach For Clients in Mobile Edge Computing","P. Agbaje; A. Anjum; Z. Talukder; M. Islam; E. Nwafor; H. Olufowobi","Department of Computer Science and Engineering, University of Texas at Arlington; Department of Computer Science and Engineering, University of Texas at Arlington; Department of Computer Science and Engineering, University of Texas at Arlington; Department of Computer Science and Engineering, University of Texas at Arlington; Department of Computing Sciences, Villanova University; Department of Computer Science and Engineering, University of Texas at Arlington",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","215","220","Federated learning (FL) enables collaborative training of a global model using localized data from multiple devices. However, in resource-constrained mobile edge computing (MEC) environments, non-independent and identically distributed (non-IID) data generated by these devices poses challenges for traditional FL algorithms like Federated Averaging (FedAvg), leading to decreased accuracy of the global model. In addition, dynamic mobile networks with intermittent connectivity, dropouts, and high migration rates hinder the communication of model updates to the central server. To address these challenges, we present FedCime, a novel tier-based FL approach that selects high-utility mobile clients likely to complete training to replace migrating clients during the round of training. Our evaluation shows that FedCime is scalable and significantly improves training performance in terms of accuracy and computational efficiency compared to state-of-the-art FL algorithms.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234316","Federated learning;machine learning;mobile edge computing;data heterogeneity","Training;Multi-access edge computing;Federated learning;Computational modeling;Heuristic algorithms;Distributed databases;Data models","","2","","16","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"A Dynamic and Collaborative Deep Inference Framework for Human Motion Analysis in Telemedicine","M. Boldo; D. Carra; D. Quaglia; N. Bombieri","Dept. of Computer Science, University of Verona, Verona, Italy; Dept. of Computer Science, University of Verona, Verona, Italy; Dept. of Computer Science, University of Verona, Verona, Italy; Dept. of Engineering for Innovation Medicine, University of Verona, Verona, Italy",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","221","226","Human pose estimation software has reached high levels of accuracy in extrapolating 3D spatial information of human keypoints from images and videos. Nevertheless, deploying such intelligent video analytic at a distance to infer kinematic data for clinical applications requires the system to satisfy, beside spatial accuracy, more stringent extra-functional constraints. These include real-time performance and robustness to the environment variability (i.e., computational workload, network bandwidth). In this paper we address these challenges by proposing a framework that implements accurate human motion analysis at a distance through collaborative and adaptive Edge-Cloud deep inference. We show how the framework adapts to edge workload variations and communication issues (e.g., delay and bandwidth variability) to preserve the global system accuracy. The paper presents the results obtained with two large datasets in which the framework accuracy and robustness are compared with a marker-based infra-red motion capture system.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234268","Human pose estimation;split computing;tensor quantization;run-length encoding;Edge-Cloud computing","Telemedicine;Pose estimation;Collaboration;Bandwidth;Robustness;Software;Motion capture","","2","","26","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Human-Centered Explainable AI at the Edge for eHealth","J. Dutta; D. Puthal","C2PS and Department of EECS, Khalifa University, Abu Dhabi, UAE; C2PS and Department of EECS, Khalifa University, Abu Dhabi, UAE",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","227","232","Explainable Artificial Intelligence (XAI) is a new paradigm of Artificial Intelligence (AI) that is giving different AI/ Machine Learning (ML) models a boost to penetrate sectors where people are thinking about adopting AI. This work focuses on the adoption of XAI in the health sector. It portrays that careful integration of XAI in both cloud and edge could change the whole healthcare industry and make humans more aware of their present health conditions, which is the need of the hour. To demonstrate the same, we have done an experiment based on the prediction of a particular medical condition called ""cardiac arrest"" in a specific subject group (patients who are 70 years old). Here, based on the explanation provided by the XAI model (e.g., SHAP, LIME) at Cloud and Edge, our system can predict the chances of a ""cardiac arrest"" for the subject with a valid explanation. This type of model will be the next big upgrade in the healthcare industry in terms of automation and a self-explanatory system that works as a personal health assistant for individuals.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234310","XAI;IoMT;Edge;Machine Learning;Interpretability;eHealth","Industries;Medical conditions;Databases;Computational modeling;Cardiac arrest;Machine learning;Predictive models","","4","","15","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"AnalogNAS: A Neural Network Design Framework for Accurate Inference with Analog In-Memory Computing","H. Benmeziane; C. Lammie; I. Boybat; M. Rasch; M. Le Gallo; H. Tsai; R. Muralidhar; S. Niar; O. Hamza; V. Narayanan; A. Sebastian; K. El Maghraoui","CNRS, UMR 8201 - LAMIH, Univ. Polytechnique Hauts-de-France, Valenciennes, France; IBM Research Europe, Rüschlikon, Switzerland; IBM Research Europe, Rüschlikon, Switzerland; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; IBM Research Europe, Rüschlikon, Switzerland; IBM Research Almaden, San Jose, CA, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; CNRS, UMR 8201 - LAMIH, Univ. Polytechnique Hauts-de-France, Valenciennes, France; CNRS, UMR 8201 - LAMIH, Univ. Polytechnique Hauts-de-France, Valenciennes, France; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; IBM Research Europe, Rüschlikon, Switzerland; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","233","244","The advancement of Deep Learning (DL) is driven by efficient Deep Neural Network (DNN) design and new hardware accelerators. Current DNN design is primarily tailored for general-purpose use and deployment on commercially viable platforms. Inference at the edge requires low latency, compact and power-efficient models, and must be cost-effective. Digital processors based on typical von Neumann architectures are not conducive to edge AI given the large amounts of required data movement in and out of memory. Conversely, analog/mixed-signal in-memory computing hardware accelerators can easily transcend the memory wall of von Neuman architectures when accelerating inference workloads. They offer increased area-and power efficiency, which are paramount in edge resource-constrained environments. In this paper, we propose AnalogNAS, a framework for automated DNN design targeting deployment on analog In-Memory Computing (IMC) inference accelerators. We conduct extensive hardware simulations to demonstrate the performance of AnalogNAS on State-Of-The-Art (SOTA) models in terms of accuracy and deployment efficiency on various Tiny Machine Learning (TinyML) tasks. We also present experimental results that show AnalogNAS models achieving higher accuracy than SOTA models when implemented on a 64-core IMC chip based on Phase Change Memory (PCM). The AnalogNAS search code is released1","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234291","Analog AI;Neural Architecture Search;Optimization;Edge AI;In-memory Computing","Program processors;Computational modeling;Artificial neural networks;Benchmark testing;In-memory computing;Search problems;Energy efficiency","","4","","46","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Reducing Inference Latency with Concurrent Architectures for Image Recognition at Edge","R. Hadidi; J. Cao; M. S. Ryoo; H. Kim",Rain AI; Georgia Tech; Stony Brook University and Google; Georgia Tech,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","245","254","Satisfying the high computation demand of modern deep learning architectures is challenging for achieving low inference latency. The current approaches in decreasing latency only increase parallelism within a layer. This is because architectures typically capture a single-chain dependency pattern that prevents efficient distribution with a higher concurrency (i.e., simultaneous execution of one inference among devices). Such single-chain dependencies are so widespread that even implicitly biases recent neural architecture search (NAS) studies. In this visionary paper, we draw attention to an entirely new space of NAS that relaxes the single-chain dependency to provide higher concurrency and distribution opportunities. To quantitatively compare these architectures, we propose a score that encapsulates crucial metrics such as communication, concurrency, and load balancing. Additionally, we propose a new generator and transformation block that consistently deliver superior architectures compared to current state-of-the-art methods. Finally, our preliminary results show that these new architectures reduce the inference latency and deserve more attention.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234297","Edge AI;Neural Architecture Search;Distributed and Collaborative Edge Computing;IoT;Collaborative Edge & Robotics","Concurrent computing;Measurement;Image edge detection;Deep architecture;Computer architecture;Parallel processing;Load management","","2","","54","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Context-Aware Task Handling in Resource-Constrained Robots with Virtualization","R. Hadidi; N. S. Ghaleshahi; B. Asgari; H. Kim",Rain AI; Georgia Tech; University of Maryland; Georgia Tech,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","255","261","Intelligent mobile robots are critical in several scenarios. However, as their computational resources are limited, mobile robots struggle to handle several tasks concurrently while guaranteeing real timeliness. To address this challenge and improve the real-timeliness of critical tasks under resource constraints, we propose a fast context-aware task handling technique. To effectively handle tasks in real-time, our proposed context-aware technique comprises three main ingredients: (i) a dynamic time-sharing mechanism, coupled with (ii) an event-driven task scheduling using reactive programming paradigm to mindfully use the limited resources; and, (iii) a lightweight virtualized execution to easily integrate functionalities and their dependencies. We showcase our technique on a Raspberry-Pi-based robot with a variety of tasks such as Simultaneous localization and mapping (SLAM), sign detection, and speech recognition with a 42% speedup in total execution time compared to the common Linux scheduler.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234269","Edge AI;Software;Mobile Robots;Middleware and Programming Environments;Reactive and Sensor-Based Planning","Simultaneous localization and mapping;Processor scheduling;Linux;Image edge detection;Speech recognition;Programming;Dynamic scheduling","","","","29","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Satellite Computing: A Case Study of Cloud-Native Satellites","C. Wang; Y. Zhang; Q. Li; A. Zhou; S. Wang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Peking University, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","262","270","The on-orbit processing of massive satellite-native data relies on powerful computing power. Satellite computing has started to gain attention, with researchers proposing various algorithms, applications, and simulation testbeds. Unfortunately, a practical platform for deploying satellite computing is currently lacking. As a result, the industry needs to make relentless efforts to achieve this goal. We suggest using cloud-native technology to enhance the computing power of LEO satellites. The first main satellite of the Tiansuan constellation, BUPT-1, is a significant example of a cloud-native satellite. Prior to delving into the details of BUPT-1, we define the essential concepts of cloud-native satellites, i.e., the cloud-native load and cloud-native platform. Afterwards, we present the design scheme of cloud-native satellites, including the architecture of BUPT-1 and the experimental subjects it can support. Two validation tests are shown to reflect the operation and capability of BUPT-1. Besides, we predict possible research fields that could shape the future of satellites in the next decade.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00048","China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234306","Satellite Computing;Cloud-Native Satellite;BUPT-1;Cloud-Native Load;Cloud-Native Platform","Industries;6G mobile communication;Satellites;Constellation diagram;Shape;Low earth orbit satellites;Computer architecture","","6","","29","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"A Comprehensive Performance Evaluation of Procedural Geometry Workloads on Resource-Constrained Devices","E. Govori; I. Murturi; S. Dustdar","Distributed Systems Group, TU Wien, Vienna, Austria; Distributed Systems Group, TU Wien, Vienna, Austria; Distributed Systems Group, TU Wien, Vienna, Austria",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","271","279","In recent years, visualizing high-quality 3D content within modern applications (e.g., Augmented or Virtual Reality) is increasingly being generated procedurally rather than explicitly. This manifests in producing highly detailed geometries entailing resource-intensive computational workloads (i.e., Procedural Geometry Workloads) with particular characteristics. Typically, workloads with resource-intensive demands are executed in environments with powerful resources (i.e., the cloud). However, the enormous amount of data transmission, heterogeneous devices, and networks involved impact overall latency and quality in user-facing applications. To tackle these challenges, computing entities (i.e., edge devices) located near end-users can be utilized to generate 3D content. Our objective within this paper is to evaluate performance and power consumption when executing procedural geometric workloads on resource-constrained edge devices. Through extensive experiments, we aim to comprehend the limitations of different edge devices when generating 3D content under different configurations.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234246","Edge Architectures;Computational Workloads;Computing Continuum;Resource-Constrained","Geometry;Performance evaluation;Cloud computing;Three-dimensional displays;Power demand;Virtual reality;Reliability","","1","","35","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"EIS: Edge Information-Aware Scheduler for Containerized IoT Applications","Z. Wang; X. Zhang; L. Yang","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","280","289","Edge computing has emerged as a powerful paradigm for Internet of Things (IoT) applications as it can provide computing and network services in close proximity to end devices. In an edge environment, leveraging container technology to package IoT applications offers significant benefits of flexibility and agility, while the incorporation of Kubernetes can effectively orchestrate large-scale containerized applications. However, the existing Kubernetes scheduling solutions mostly cannot satisfy IoT applications with stringent and diverse network, computing, and storage requirements, and they also lack the scalability to customize scheduling strategies. To address these, we develop an edge information-aware scheduler (EIS) based on the novel Kubernetes scheduling framework. EIS schedules containerized IoT applications by sensing the network topology and performance information of edge clusters. Moreover, EIS can make scheduling decisions according to application characteristics and resource requirements. By adopting a plug-in architecture, EIS not only provides an extensible programming interface, but is also compatible with Kubernetes’ default scheduler. We evaluate EIS in a real-world experimental environment, and the results show that EIS can reduce network latency by 18%, improve computing performance up to 140% and improve I/O performance up to 130%. These improvements are critical for IoT applications to provide high quality of service.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234296","Kubernetes;edge computing;containerized application scheduling;scheduling framework;Internet of Things","Schedules;Processor scheduling;Network topology;Scalability;Image processing;Quality of service;Programming","","","","34","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"EdgeRDV: A Framework for Edge Workload Management at Scale","G. Rubambiza; B. Dumba; A. J. Anderson; H. Weatherspoon","Cornell University, Ithaca, NY, U.S.A; IBM Research, NY, U.S.A; IBM Research, NY, U.S.A; Cornell University, Ithaca, NY, U.S.A",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","290","300","Edge computing is a distributed computing paradigm that moves data-intensive applications and services (e.g., AI) closer to the data source. The rapid growth of edge endpoints connected to the Internet today poses several challenges in scalable application life cycle management. That is, managing data and workloads on several thousand, up to millions of edge endpoints, challenged by limited connectivity, resource constraints, network and edge endpoint failures. In this work, we present EdgeRDV, a new edge abstraction that builds on the idea of rendezvous nodes to manage edge workloads at scale. The EdgeRDV architecture is comprised of a central cloud management endpoint (or cloud hub), a central gateway for each edge site (or edge hub), redundant gateways (or rendezvous nodes), and edge endpoints. Beyond its scalable architecture, EdgeRDV presents new techniques and algorithms that address single points of failures and provide adjustable levels of resilience and cost-effectiveness in edge network deployments. We conducted preliminary experiments to evaluate EdgeRDV, through simulations, and our results show that EdgeRDV requires one to three orders of magnitude fewer intermediate nodes compared to relay structures, can gracefully adapt to failures, and requires a constant number of messages during failure recovery in edge sites with up to 667K+ edge endpoints.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234275","edge computing;scalability;rendezvous;binary trees;IoT","Cloud computing;Adaptation models;Scalability;Soft sensors;Computer architecture;Bandwidth;Logic gates","","","","33","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Data-centric Edge-AI: A Symbolic Representation Use Case","S. Ilager; V. De Maio; I. Lujic; I. Brandic","Vienna University of Technology (TU Wien), Austria; Vienna University of Technology (TU Wien), Austria; Ericsson Nikola Tesla, Croatia; Vienna University of Technology (TU Wien), Austria",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","301","308","Today’s machine learning pipelines are primarily executed in the cloud, from data storage to data processing, model training, and deployment. However, machine learning is moving to edge devices, creating the demand for AI applications at the edge, known as Edge-AI. Traditional data management practices applied in the cloud are proving to be inefficient for Edge-AI, due to resource and energy constraints of edge devices and real-time requirements of applications. This paper identifies the challenges associated with data processing for Edge-AI. We then discuss methods for efficient data processing at the edge, leading to data-centric Edge-AI. As a use case scenario, we discuss the symbolic representation of time series data and explain how it could help save the cost of data storage and processing in developing Edge-AI applications.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00052","CHIST-ERA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234261","Edge-AI;Data-centric Edge;IoT data;Symbolic representation of data;Big data","Training;Time series analysis;Pipelines;Memory;Machine learning;Data processing;Real-time systems","","4","","50","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Service Classification of Network Traffic in 5G Core Networks using Machine Learning","R. Pell; M. Shojafar; D. Kosmanos; S. Moschoyiannis","Department of Computer Science, University of Surrey, Guildford, United Kingdom; 5G/6GIC, University of Surrey, Guildford, United Kingdom; Department of Digital Systems, University of Thessaly, Larissa, Greece; Department of Computer Science, University of Surrey, Guildford, United Kingdom",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","309","318","Fifth generation mobile networks (5G) leverage the power of edge computing to move vital services closer to end users. With critical 5G core network components located at the edge there is a need for detecting malicious signalling traffic to mitigate potential signalling attacks between the distributed Network Functions (NFs). A prerequisite for detecting anomalous signalling is a network traffic dataset for the identification and classification of normal traffic profiles. To this end, we utilise a 5G Core Network (5GC) simulator to execute test scenarios for different 5G procedures and use the captured network traffic to generate a dataset of normalised service interactions in the form of packet captures. We then apply machine learning techniques (supervised learning) and do a comparative analysis on accuracy, which uses three features from the traffic meta-data. Our results show that the identification of 5G service use by applying ML techniques offer a viable solution to classifying normal services from network traffic metadata alone. This has potential advantages in forecasting service demand for resource allocation in the dynamic 5GC environment and provide a baseline for performing anomaly detection of NF communication for detecting malicious traffic within the 5G Service Based Architecture (SBA).","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234287","Traffic Classification;5G Core Network;5G Services;Machine Learning;Supervised Learning","5G mobile communication;Image edge detection;Supervised learning;Telecommunication traffic;Machine learning;Metadata;Resource management","","","","34","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Big Data Analytics from the Rich Cloud to the Frugal Edge","F. M. Awaysheh; R. Tommasini; A. Awad","Institute of Computer Science, University of Tartu; Institut National des Sciences Appliquees, INSA, Lyon, France; University of Tartu, Tartu, Estonia Cairo University, Giza, Egypt",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","319","329","Modern systems and applications generate and consume an enormous amount of data from different sources, including mobile edge computing and IoT systems. Our ability to locate and analyze these massive amounts of data will shape the future, building next-generation Big Data Analytics (BDA) and artificial intelligence systems in critical domains. Traditionally, big data materialize in a centralized repository (e.g., the cloud) for running sophisticated analytics using decent computation. Nevertheless, many modern applications and critical domains require low-latency data analysis with the right decision at the right time standard for building trust. With the advent of edge computing, that traditional deployment model shifted closer to the data sources at the network’s edge. Such a shift was motivated by minimized latency, increased uptime, and enhanced efficiencies. This paper studies the BDA building blocks, analyzes the deployment requirements for edge-based BDA QoS, and drafts future trends. It also discusses critical open issues and further research directions for the next step of edge-based BDA.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00054","European Social Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234250","Big Data;Edge Computing;Cloud Computing;Resource Management;Data Management;Data Privacy","Multi-access edge computing;Shape;Soft sensors;Buildings;Quality of service;Big Data;Market research","","7","","50","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"The Potentials of AI Planning on the Edge","I. Georgievski; M. Aiello","Service Computing Department, IAAS University of Stuttgart, Stuttgart, Germany; Service Computing Department, IAAS University of Stuttgart, Stuttgart, Germany",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","330","336","Edge computing brings computation closer to sources of data and knowledge by embedding computation in the physical space and close to the end users. Edge computing is becoming the ultimate platform where modern applications based on IoT and AI are deployed in a truly distributed manner. When edge applications require goal-oriented behaviour, AI planning comes into play as a powerful tool for achieving such behaviour. In turn, this necessitates AI planning systems that can be deployed and operate on the edge possibly on a multitude of dispersed nodes. Current approaches to distributed AI planning are mainly designed around the requirements and peculiarities of multi-agent systems, such as communication constraints and the self-interest of agents. In this work, we postulate that edge computing provides new perspectives for distributing AI planning. We propose the concept of edge AI planning where multiple AI planning components are distributed on edge nodes and communicate over a vast network. These components need to have clearly defined requirements of what can be distributed and how in order for the overall AI planning to work effectively, in turn enabling correct and consistent executions across the whole system.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234265","AI Planning;Edge AI;Edge Computing;Distributed AI Planning;Distributed AI","Planning;Artificial intelligence;Edge computing;Multi-agent systems","","1","","30","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Realising the Power of Edge Intelligence: Addressing the Challenges in AI and tinyML Applications for Edge Computing","M. Gibbs; E. Kanjo","Department of Computer Science, Nottingham Trent University, Nottingham, United Kingdom; Department of Computer Science, Nottingham Trent University, Nottingham, United Kingdom",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","337","343","The edge computing paradigm has become increasingly popular due to its benefits over cloud computing, particularly in the context of AI and IoT applications. Its harmonising with AI to form Edge intelligence (EI) has opened up possible application areas for further development. Tiny machine learning (tinyML) is a specific focus within EI that targets machine learning algorithms deployed to constrained edge devices such as microcontrollers. However, despite the potential advantages of EI and tinyML, there are several challenges that researchers often overlook, especially when deploying on microcontrollers. These challenges include programming language choice, lack of support for development boards, neglect of preprocessing, choice of sensors, and insufficient labelled data. This paper assesses these previously unaddressed challenges, highlights their issues with a particular focus on microcontroller deployment, and offers potential solutions. By addressing these challenges, researchers can design more effective and efficient tinyML systems, pushing the boundaries of edge AI faster than before.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234245","Edge Computing;tinyML;Edge Intelligence;Edge AI;AIoT;Artificial Intelligence","Computer languages;Cloud computing;Machine learning algorithms;Microcontrollers;Machine learning;Sensors;Internet of Things","","9","","44","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"PATRIoTA: A Similarity-based IoT Malware Detection Method Robust Against Adversarial Samples","J. Sándor; R. Nagy; L. Buttyán","Budapest University of Technology and Economics, Hungary; Budapest University of Technology and Economics, Hungary; Budapest University of Technology and Economics, Hungary",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","344","353","Detecting malware targeting IoT devices has became an important challenge with the recent emergence of IoT botnets. Gateways at the edge between the Internet and IoT devices deployed in the field are particularly well-positioned for the task of malware detection, as malware typically spreads over the Internet and resource-constrained field devices may not have the means to protect themselves. Hence, we believe that, among other things, edge intelligence should also include effective and efficient IoT malware detection. A recently proposed similarity-based IoT malware detection method, called SIMBIoTA, would be suitable in this context, but its robustness against adversarial malware samples has been shown to be rather weak. In this paper, we propose PATRIoTA, a similarity-based IoT malware detection method inspired by SIMBIoTA, but being significantly more robust than SIMBIoTA is. We describe the operation of PATRIoTA, and compare its malware detection performance and robustness against adversarial samples to that of SIMBIoTA. We show that PATRIoTA outperforms SIMBIoTA with respect to both measures.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00057","Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234259","Internet-of-Things;malware detection;binary similarity;locality-sensitive hashing;robustness against adversarial samples","Performance evaluation;Image edge detection;Telecommunication traffic;Logic gates;Malware;Robustness;Libraries","","","","29","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"A Brief History of Liquid Software","C. Pautasso","Software Institute – USI, Lugano, Switzerland",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","354","363","The concept of liquid software, i.e., software with flexible deployment, over the past two decades has appeared in the fields of edge computing, Internet of Things (IoT), Human-Computer Interaction, DevOps and Web engineering. In this paper, we survey, compare, and provide a comprehensive definition of liquid software by analyzing how the metaphor has been used in existing literature and identifying gaps and inconsistencies in the current vs. past understanding of the concept. Overall, liquid software can be seamlessly deployed and redeployed within a dynamic and distributed runtime environment in response to changes applied to the set of available devices and to the software itself. Liquid software has been introduced in the context of active networks and intelligent environments, it has been applied to describe the user interaction with multi and cross-device user interfaces, it has found a promising foundation in Web technology, continuous software delivery pipelines, as well as isomorphic software architectures running across the IoT, edge and Cloud continuum.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234252","Liquid Software;Software Deployment;Isomorphic Software Architecture;Continuum;Liquid User Experience;Multi-Device User Interface;Cross-Device User Interface","Surveys;Runtime environment;Privacy;Liquids;Software architecture;Software;Internet of Things","","2","","79","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Real-Time Onboard Object Detection for Augmented Reality: Enhancing Head-Mounted Display with YOLOv8","M. Łysakowski; K. Żywanowski; A. Banaszczyk; M. R. Nowicki; P. Skrzypczyński; S. K. Tadeja","Centre for Artificial Intelligence and Cybersecurity, Poznań University of Technology; Centre for Artificial Intelligence and Cybersecurity, Poznań University of Technology; Centre for Artificial Intelligence and Cybersecurity, Poznań University of Technology; Centre for Artificial Intelligence and Cybersecurity, Poznań University of Technology; Centre for Artificial Intelligence and Cybersecurity, Poznań University of Technology; Department of Engineering, Institute for Manufacturing, University of Cambridge",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","364","371","This paper introduces a software architecture for real-time object detection using machine learning (ML) in an augmented reality (AR) environment. Our approach uses the recent state-of-the-art YOLOv8 network that runs onboard on the Microsoft HoloLens 2 head-mounted display (HMD). The primary motivation behind this research is to enable the application of advanced ML models for enhanced perception and situational awareness with a wearable, hands-free AR platform. We show the image processing pipeline for the YOLOv8 model and the techniques used to make it real-time on the resource-limited edge computing platform of the headset. The experimental results demonstrate that our solution achieves real-time processing without needing offloading tasks to the cloud or any other external servers while retaining satisfactory accuracy regarding the usual mAP metric and measured qualitative performance.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00059","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234302","augmented reality;machine learning;real-time object detection;edge computing","Headphones;Solid modeling;Head-mounted displays;Computational modeling;Image edge detection;Object detection;Resists","","11","","46","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Towards Intelligent Data Protocols for the Edge","P. K. Donta; S. Dustdar","Distributed Systems Group, TU Wien, Vienna, Austria; Distributed Systems Group, TU Wien, Vienna, Austria",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","372","380","The computing continuum is growing because multiple devices are added daily. Edge devices play a key role in this because computation is decentralized or distributed. Edge computing is advanced by using AI/ML algorithms to become more intelligent. Besides, Edge data protocols are useful for transmitting or receiving data between devices. Since, computation efficiency is possible when the data is received at the Edge timely, and it is possible only when the data protocols are efficient, reliable and fast. Most edge data protocols are defined with static set of rules and their primary purpose is to provide standardized and reliable data communications. Edge devices need autonomous or dynamic protocols that enable interoperability, autonomous decision making, scalability, and adaptability. This paper examines the limitations of popular data protocols used in edge networks, the need for intelligent data protocols, and their implications. We also explore possible ways to simplify learning for edge devices and discuss how intelligent data protocols can mitigate challenges such as congestion, message filtering, message expiration, prioritization, and resource handling.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00060","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234253","Data protocols;Intelligent protocols;Edge Computing;Computing Continuum;Edge Intelligence;Autonomous decision-making","Protocols;Filtering;Scalability;Decision making;Computational efficiency;Complexity theory;Reliability","","7","","41","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Intelligent Multi-Domain Edge Orchestration for Highly Distributed Immersive Services: An Immersive Virtual Touring Use Case","T. Z. Benmerar; T. Theodoropoulos; D. Fevereiro; L. Rosa; J. Rodrigues; T. Taleb; P. Barone; K. Tserpes; L. Cordeiro","ICTFICIAL Oy, Finland; Harokopio University of Athens, Greece; OneSource, Portugal; OneSource, Portugal; Cyango, Portugal; University of Oulu, Finland; Hewlett Packard Enterprise, Italy; Harokopio University of Athens, Greece; OneSource, Portugal",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","381","392","Edge cloud technologies in tandem with AI-enabled solutions can contribute to overcoming the challenges that pertain the distributed execution of immersive services and contribute towards providing a positive experience for the end-users. Intelligent resource management, orchestration, and prediction systems can optimize the deployment of services, adapt to changing demands, and ensure that the services are running smoothly. This paper introduces a novel architectural paradigm capable of facilitating multi-domain edge orchestration for highly distributed immersive services by incorporating a plethora of AI solutions and technological enablers that can support multi-domain edge deployments. The proposed architecture is designed to operate on the basis of multi-level specification blueprints, which decouple the simple high-level user-intent infrastructure definition from the AI-driven orchestration and the final execution plan. The Application Management Framework (AMF) offers a visual language and tool that can be used as an alternative to a formal method for creating the intent blueprint. In the frame of this work, the latter is validated by an immersive virtual touring use-case scenario.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234321","Edge cloud;immersive service;orchestration;cluster;Kubernetes;centralized management;and decentralized management","Visualization;Cloud computing;Computer architecture;Resource management;Artificial intelligence;Edge computing","","3","","44","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"DDoS-FOCUS: A Distributed DoS Attacks Mitigation using Deep Learning Approach for a Secure IoT Network","M. Al-Khafajiy; G. Al-Tameemi; T. Baker","School of Computer Science, University of Lincoln, Lincoln, UK; Faculty of Art, Science, and Technologies, University of Northampton, Northampton, UK; School of Architecture, Technology and Engineering, University of Brighton, Brighton, UK",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","393","399","The fast growth of the Internet of Things devices and communication protocols poses equal opportunities for lifestyle-boosting services and pools for cyber attacks. Usually, IoT network attackers gain access to a large number of IoT (e.g., things and fog nodes) by exploiting their vulnerabilities to set up attack armies, then attacking other devices/nodes in the IoT network. The Distributed Denial of Service (DDoS) flooding-attacks are prominent attacks on IoT. DDoS concerns security professionals due to its nature in forming sophisticated attacks that can be bandwidth-busting. DDoS can cause unplanned IoT-services outages, hence requiring prompt and efficient DDoS mitigation. In this paper, we propose a DDoS-FOCUS; a solution to mitigate DDoS attacks on fog nodes. The solution encompasses a machine learning model implanted at fog nodes to detect DDoS attackers. A hybrid deep learning model was developed using Conventional Neural Network and Bidirectional LSTM (CNN-BiLSTM) to mitigate future DDoS attacks. A preliminary test of the proposed model produced an accuracy of 99.8% in detecting DDoS attacks.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234278","DDoS;IoT;CNN-BiLSTM;Distributed fog/edge","Deep learning;Protocols;Image edge detection;Neural networks;Denial-of-service attack;Equal opportunities;Security","","3","","17","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Using Machine Learning for Detection and Classification of Cyber Attacks in Edge IoT","E. Becker; M. Gupta; K. Aryal","Department of Computer Science, Tennessee Tech University, Cookeville, TN, USA; Department of Computer Science, Tennessee Tech University, Cookeville, TN, USA; Department of Computer Science, Tennessee Tech University, Cookeville, TN, USA",2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","400","410","Internet of Things (IoT) devices are omnipresent due to their ease of use and level of connectivity. Because of wide deployment, IoT network traffic security is a large issue, especially as the devices become more common at the edge of the connected ecosystem. In general, low-powered IoT devices themselves are not inherently secure, so tailored security mechanisms are needed to make the ecosystem secure. The incorporation of the cloud also adds new security issues with the cloud service provider (CSP). In addition, several smart applications necessitate deploying edge-based infrastructure due to their real-time computation and communication requirements, while also having the ability to detect and mitigate different cyber attacks and remain light-weight. In this paper, we propose a machine learning-based approach to detect and classify different edge IoT network traffic driven cyber attacks, and evaluate their strengths and weaknesses. Particularly, we will compare eleven machine learning models to determine the best security agent trained for attack detection and classification on an edge IoT cyber security dataset with fourteen different attacks. We also provide experimental evaluation and analysis of our work, followed by our conclusion.","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234279","Internet of Things;Edge IoT;Machine Learning;Attack Detection and Classification","Cloud computing;Image edge detection;Ecosystems;Telecommunication traffic;Machine learning;Real-time systems;Internet of Things","","3","","39","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
"Author Index","",,2023 IEEE International Conference on Edge Computing and Communications (EDGE),"1 Sep 2023","2023","","","411","413","","2767-9918","979-8-3503-0483-1","10.1109/EDGE60047.2023.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234308","","","","","","","IEEE","1 Sep 2023","","","IEEE","IEEE Conferences"
