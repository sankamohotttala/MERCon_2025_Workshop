"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Title Page i","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","1","1","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646424","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Title Page iii","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","3","3","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646427","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Copyright Page","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","4","4","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646422","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Table of Contents","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","5","8","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646426","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Congress Steering Committee Chair Message","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","9","9","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646445","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Congress General Chair Message","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","10","11","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646419","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Congress Program Chairs Message","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","12","12","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646423","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"TCSVC Chair Message","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","13","13","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646440","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"EDGE 2024 General Chairs Message","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","14","15","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646429","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"EDGE 2024 Program Committee","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","16","17","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646442","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"EDGE 2024 Reviewers","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","18","18","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646421","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"An Empirical Study on Edge-to-Cloud Continuum for Smart Applications: Performance, Design Patterns, and Key Factors","N. Chen; A. N. Toosi; B. Javadi; D. Alqahtani; M. S. Aslanpour; M. Xu","Department of Software Systems and Cybersecurity, Monash University, Clayton, Australia; Department of Software Systems and Cybersecurity, Monash University, Clayton, Australia; School of Computer, Data and Mathematical Sciences, Western Sydney University; Department of Software Systems and Cybersecurity, Monash University, Clayton, Australia; Department of Software Systems and Cybersecurity, Monash University, Clayton, Australia; Chinese Academy of Sciences, Shenzhen Institute of Advanced Technology",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","1","11","The rapid evolution of cloud-native technologies has facilitated seamless application deployment and execution across the entire edge-to-cloud continuum. This continuum offers a myriad of benefits, including reduced latency, optimized bandwidth utilization, enhanced data privacy, improved reliability, scalability, and flexibility. However, realizing a coherent edge-to-cloud continuum poses challenges especially in resource management, due to the heterogeneous and dynamic nature of computing resources such as resource scheduling and load balancing. This paper focuses on the Container-as-a-Service model enabling independent execution of functions/microservices anywhere on the continuum. We propose an architectural design for constructing a practical edge-to-cloud infrastructure and conduct comprehensive performance evaluations using a real edge-to-cloud testbed. Through an empirical study, we aim to identify key factors impacting application performance and resource management within the continuum, with a specific focus on AI-based IoT applications. Our experiments explore various design patterns including load balancing techniques, scheduling algorithms, invocation methods, gateway and data source location, and factors such as bandwidth and delay, providing practical insights for practitioners and researchers alike.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646441","continuum;edge;cloud;kubernetes;performance;load balancing;edge-to-cloud","Performance evaluation;Scheduling algorithms;Soft sensors;Scalability;Bandwidth;Logic gates;Load management","","","","31","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Cross Network Layer Cognitive Service Orchestration in Edge Computing Systems","M. Rafiee; A. Taherkordi; Ã–. Alay","Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway; Department of Informatics, University of Oslo, Oslo, Norway",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","12","21","The edge architecture introduces challenges due to the presence of heterogeneous constrained devices and the dynamic nature of the network. Dynamic service orchestration is essential for efficient utilization of network resources in this context. Service orchestration plays a critical role in 5G networks in automating end-to-end service deployment and operations, and 5G network slicing. However, existing approaches on service orchestration primarily focus on resource availability, lacking a comprehensive understanding of network conditions-cross network layer orchestration. As a result, providing efficient service orchestration for delay-sensitive services remains a significant research area. To address this gap, this paper proposes a cognitive service orchestration framework that leverages not only application-level resource demands, but also the real-time status of the network infrastructure. This cognitive framework incorporates Reinforcement Learning, enabling it to dynamically interact with the network environment and continuously update policies for intelligent decision-making in complex 5G networks. The efficacy of the proposed framework is evaluated using an object detection application in smart cities. The evaluation results show that the proposed framework achieves a significant reduction in latency as compared to OpenELB, with a remarkable $58 \%$ decrease, which highlights its efficiency and effectiveness in meeting the requirements of delay-sensitive services.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646437","Service Orchestration;Cognitive Computing;Edge Computing;Reinforcement Learning;SDN;Kubernetes","5G mobile communication;Smart cities;Network slicing;Image edge detection;Heuristic algorithms;Surveillance;Real-time systems","","","","31","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"CargoSync: Efficient Containerd Image Updates in Low-Bandwidth Edge Environments with Rsync-based Delta Compression","I. Protogeros; M. Karamousadakis; M. Rizakis; A. Porichis",TWI Hellas & National Technical University of Athens; Plaixus Ltd; University of Essex; University of Essex,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","22","28","Containers have emerged as the standard for software deployment, offering simplicity, low overhead, and portability. In the context of updating containers in Edge environments, the constrained bandwidth poses the critical challenge of minimizing data transfer and optimizing the update process. This paper introduces CargoSync, a Containerd-based tool designed for efficient image updates. It leverages the rsync tool for delta compression, reducing the size of updates towards saving network resources. The proposed method is evaluated against traditional pull-based updates showcasing notable improvements in data transfer efficiency as well as update times even for large container images when bandwidth is severely limited. Our tool competes favourably with Moby-based counterpart Balena-engineâ€™s rsync delta updates in our performance evaluations, making it the first Containerd-based tool to effectively adopt this approach. Additionally, CargoSync addresses the challenge of distributing updates to multiple machines concurrently, demonstrating scalability benefits. The results indicate the potential of CargoSync for optimizing container image updates in resource-constrained environments.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00013","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646439","Containerization;Delta Encoding;Image Updates;Resource-constrained environments;Containerd;rsync","Image coding;Image edge detection;Scalability;Bandwidth;Containers;Predictive models;Data transfer","","","","38","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Do 5G Networks Achieve The Proclaimed Promises? An Empirical Study Using YouTube Edge Service","P. Song; J. Lee; A. M. Abdelmoniem; L. Mukhanov","University of Cambridge, United Kingdom; University of Essex, United Kingdom; Queen Mary University of London, United Kingdom; Queen Mary University of London, United Kingdom",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","29","34","The advancements in 5G mobile networks and Edge computing offer great potential for services like augmented reality and Cloud gaming, thanks to their low latency and high bandwidth capabilities. However, the practical limitations of achieving optimal latency on real applications remain uncertain This paper investigates the latency and bandwidth of 5G Networks and leverages YouTube Edge service as the practical use case. We analyze how latency and bandwidth differ between 4G LTE and 5G networks and how the location of YouTube Edge servers impacts these metrics in London, UK. Surprisingly, our observations show that 4G LTE provides lower latencies compared to 5G networks, which we attribute to the early stage of the 5G ecosystem in London, UK.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646449","5G;Mobile Network;Measurements;Performance Efficiency;Edge Services","Measurement;Video on demand;5G mobile communication;Ecosystems;Bandwidth;Web sites;Servers","","","","37","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Navigator: A Decentralized Scheduler for Latency-Sensitive AI Workflows","Y. Yang; A. Merlina; W. Song; T. Yuan; K. Birman; R. Vitenberg","dept. Computer Science, Cornell University, Ithaca, USA; dept. Informatics, University of Oslo, Norway; dept. Computer Science, Cornell University, USA; dept. Computer Science, Cornell University, USA; dept. Computer Science, Cornell University, USA; dept. Informatics, University of Oslo, Norway",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","35","47","We consider AI inference and generative query processing in distributed systems: an increasingly popular edgecomputing use case. In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity. We propose Navigator, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources. In one case, just half the servers were needed for processing the same workload.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646430","scheduling;Resource management;Artificial Intelligence;Edge computing","Schedules;Energy consumption;Navigation;Query processing;Memory management;Graphics processing units;Servers","","","","93","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Reliable Network Performance for Edge Networks with QoS-Aware Adaptive Routing","O. A. Hamdan; E. Arslan","Department of Computer Science and Engineering, University of Texas at Arlington; Department of Computer Science and Engineering, University of Texas at Arlington",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","48","58","Edge networks are commonly used to collect data from remote areas with limited accessibility such as deserts and mountaintops. They rely on wireless communication to stream the collected data to data centers, which makes them susceptible to bandwidth fluctuations due to extreme weather conditions such as snowstorms. Although these networks are designed to have alternate routes to recover from link failures, existing rigid network management solutions (i.e., OSPF) make it difficult to utilize alternate routes when the bandwidth of wireless links degrades. This paper proposes an adaptive routing algorithm, RENET, to quickly detect bandwidth fluctuations and re-adjust flow paths to increase the utilization of available links. RENET introduces a novel delay-based measurement technique to quickly detect bandwidth fluctuations in wireless links. Experimental results collected in real-world and emulated networks show that RENET offers a robust solution to bandwidth fluctuations and outperforms the state-of-the-art solutions by more than $30 \%$ in terms of QoS satisfaction rate. Moreover, RENET increases the quality of adaptive video streams by up to $54 \%$.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646435","Adaptive routing;wireless link bandwidth changes;Software Defined Networking;traffic engineering","Wireless communication;Fluctuations;Adaptive systems;Image edge detection;Bandwidth;Quality of service;Measurement techniques","","","","31","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Concierge: Towards Accuracy-Driven Bandwidth Allocation for Video Analytics Applications in Edge Network","Y. Huang; F. Zharfan; H. Hendrawan; H. S. Gunawi; J. Jiang","Department of Computer Science, University of Chicago, Chicago, USA; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; Department of Computer Science, University of Chicago, Chicago, USA; Department of Computer Science, University of Chicago, Chicago, USA",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","59","66","When performing inference on sensor data, edge video analytics applications may not always need high-fidelity data, since important information may not appear all the time. Consequently, each edge AI applicationâ€™s bandwidth demand is highly dynamic. Thus, a shared edge system should dynamically allocate more bandwidth to the applications in need to reach high accuracy at each moment. However, previous bandwidth allocators are ill-suited because they are agnostic to the timevarying impact of bandwidth on each applicationâ€™s accuracy.This short paper explores a new accuracy-driven approach to bandwidth allocation, which periodically re-allocates bandwidth across edge AI applications based on the sensitivity of each applicationâ€™s accuracy to its bandwidth share. To examine its practical benefit and technical challenges, we present a concrete accuracy-driven bandwidth allocator called ConciERGE, which exposes a simple yet efficient interface to estimate each applicationâ€™s sensitivity to a small change in its bandwidth share.We run CONCIERGE on state-of-the-art video-analytics applications with real video streams and show its early promise in greatly improving the inference accuracy of video analytics.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646431","Resource Allocation;System for ML;ML at Edge;Video Analytic;Network","Accuracy;Sensitivity;Visual analytics;Estimation;Bandwidth;Edge AI;Channel allocation","","","","33","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"QEdgeProxy: QoS-Aware Load Balancing for IoT Services in the Computing Continuum","I. ÄŒiliÄ‡; V. JukanoviÄ‡; I. P. Å½arko; P. Frangoudis; S. Dustdar","Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Faculty of Electrical Engineering and Computing, University of Zagreb, Zagreb, Croatia; Distributed Systems Group, TU Wien, Vienna, Austria; Distributed Systems Group, TU Wien, Vienna, Austria",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","67","73","While various service orchestration aspects within Computing Continuum (CC) systems have been extensively addressed, including service placement, replication, and scheduling, an open challenge lies in ensuring uninterrupted data delivery from IoT devices to running service instances in this dynamic environment, while adhering to specific Quality of Service (QoS) requirements and balancing the load on service instances. To address this challenge, we introduce QEdgeProxy, an adaptive and QoS-aware load balancing framework specifically designed for routing client requests to appropriate IoT service instances in the CC. QEdgeProxy integrates naturally within Kubernetes, adapts to changes in dynamic environments, and manages to seamlessly deliver data to IoT service instances while consistently meeting QoS requirements and effectively distributing load across them. This is verified by extensive experiments over a realistic K3s cluster with instance failures and network variability, where QEdgeProxy outperforms both Kubernetes built-in mechanisms and a state-of-the-art solution, while introducing minimal computational overhead.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00018","Horizon Europe; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646425","edge computing;request routing;load balancing;IoT;Kubernetes","Processor scheduling;Quality of service;Load management;Dynamic scheduling;Routing;Internet of Things;Edge computing","","","","18","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"The Impact of GPU on Containerized Computer Vision Applications on Edge Nodes","O. I. Alqaisi; A. Åž. Tosun; T. Korkmaz","Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA; Dep. of Math. and Computer Science, The University of North Carolina at Pembroke, NC, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX, USA",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","74","83","Edge computing revolutionizes computer vision, but faces challenges due to computational demands and complexities in deployment and platform integration. In addressing these hurdles, container technology emerges as a crucial solution, particularly in mitigating deployment and security issues. However, the impact of GPU acceleration on containerized computer vision applications remains understudied. This paper explores containerized computer vision applications across ARM edge devices, analyzing performance on different CPUs and GPUs. Insights cover processing time, CPU usage, memory utilization, and application performance, highlighting the importance of ARM processor selection. Integrating CPU and GPU enhances execution time but leads to significant memory usage increases. Workload escalation strains the GPU, resulting in slowed processing, and its performance becomes comparable to that of the CPU. Furthermore, the performance of computer vision applications varies with different containers. These findings underscore the delicate balance between CPU and GPU utilization and the pivotal role of algorithm selection in optimizing containerized computer vision applications on edge devices.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00019","Army Research Office; U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646436","Edge Computing;Container Technology;Docker;Computer Vision;Raspberry Pi;Jetson;Rock Pi","Performance evaluation;Computer vision;Graphics processing units;Containers;Complexity theory;Security;Faces","","","","59","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Split DNN Inference for Exploiting Near-Edge Accelerators","H. Liu; M. E. Fouda; A. M. Eltawil; S. A. Fahmy","CEMSE Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Rain Neuromorphics, Inc., San Francisco, CA, USA; CEMSE Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; CEMSE Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","84","91","The deployment of increasingly complex deep learning models for inference in real world settings requires dealing with the constrained computational capabilities of edge devices. Splitting inference between edge and cloud has been proposed to overcome these limitations, but entails significant communication latency. Newer edge accelerator devices can be distributed throughout layers of the network, supporting fine-grained offload. We propose a method for splitting a deep neural network (DNN) across the edge, near-edge accelerator, and cloud to exploit the combined computing capabilities of such devices while minimizing transmission bandwidth and, hence, energy. We formulate an approach to find near-optimal two-split configurations to optimize inference energy and latency. We thoroughly evaluate our approach on the VGG16 and ResNet50 models using the CIFAR-100 and ImageNet datasets to demonstrate that our method can navigate the trade-off space effectively.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00020","King Abdullah University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646420","Deep learning inference;hardware acceleration;edge computing","Deep learning;Navigation;Computational modeling;Image edge detection;Bandwidth;Residual neural networks;Edge computing","","","","26","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"A Case for Deploying Dynamic Neural Network on Edge-Cloud Continuum Environment","M. S. M. Pozi; Y. Sato","Department of Computer Science and Engineering, Toyohashi University of Technology, Japan; Department of Computer Science and Engineering, Toyohashi University of Technology, Japan",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","92","98","As Deep Neural Network models become more accurate, the associated AI applications experience higher inference runtimes. This results in increased overall communication latency within the edge-cloud continuum, where the computing tasks are distributed. To address this, we propose a Dynamic Neural Network based framework to improve inference accuracy with optimal latency overhead within the edge-cloud continuum environment. The framework consists of 2 main components: Deep Neural Network offloading mechanism to the computing resources on edge-cloud continuum, and parameter selection for inference-exiting mechanism. Based on evaluations using the ImageNet dataset, our Dynamic Neural Network-based inference framework successfully improves inference accuracy while maintaining optimal latency overhead within the edge-cloud continuum. The framework surpasses the inference accuracy of single model-based approaches, where our cascaded model obtained $\mathbf{7 4. 1 7 \%}$ accuracy compared to single model accuracy, which is $73.95 \%$.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646446","Edge;Cloud;Dynamic Neural Network;Deep Neural Network;Compiler","Accuracy;Runtime;Computational modeling;Image edge detection;Artificial neural networks;Task analysis;Artificial intelligence","","","","18","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"FedCode: Communication-Efficient Federated Learning via Transferring Codebooks","S. Khalilian; V. Tsouvalas; T. Ozcelebi; N. Meratnia",Eindhoven University of Technology; Eindhoven University of Technology; Eindhoven University of Technology; Eindhoven University of Technology,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","99","109","Federated Learning (FL) is a distributed machine learning paradigm that enables learning models from decentralized local data, offering significant benefits for clientsâ€™ data privacy. Despite its appealing privacy properties, FL faces the challenge of high communication burdens, necessitated by the continuous exchange of model weights between the server and clients. To mitigate these issues, existing communication-efficient FL approaches employ model compression techniques, such as pruning and weight clustering; yet, the need to transmit the entire set of weight updates at each federated round even in a compressed format - limits the potential for a substantial reduction in communication volume. In response, we propose FedCodeâ€¡, a novel FL training regime directly utilizing codebooks, i.e., the cluster centers of updated model weight values, to significantly reduce the bidirectional communication load, all while minimizing computational overhead and preventing substantial degradation in model performance. To ensure a smooth learning curve and proper calibration of clusters between the server and clients through the periodic transfer of compressed model weights, following multiple rounds of exclusive codebook communication. Our comprehensive evaluations across various publicly available vision and audio datasets on diverse neural architectures demonstrate that FedCode achieves a 12.4 -fold reduction in data transmission on average, while maintaining modelsâ€™ performance on par with FedAvg, incurring a mere average accuracy loss of just $1.65 \%$.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646450","Federated learning;communication efficiency;weight clustering;model compression;codebook transfer","Training;Accuracy;Federated learning;Computational modeling;Computer architecture;Propagation losses;Data models","","1","","39","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Characterizing Deep Learning Model Compression with Post-Training Quantization on Accelerated Edge Devices","R. D. Rachmanto; Z. Sukma; A. N. L. Nabhaan; A. Setyanto; T. Jiang; I. K. Kim",Universitas Amikom Yogyakarta; University of Georgia; Universitas Amikom Yogyakarta; Universitas Amikom Yogyakarta; University of Georgia; University of Georgia,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","110","120","Edge AI has increasingly been adopted due to the rapid development of deep learning and AI. At the same time, as AI models quickly grow in size and complexity, resource-constrained edge devices face significant challenges in executing such complex models. Model compression, particularly post-training quantization, offers a viable approach by reducing model size and resource demands, making these models more suitable for the deployment on edge devices. However, despite its significance, the effects of model compression on edge devices have yet to be thoroughly explored. We address this gap by thoroughly characterizing post-training quantization on accelerated edge devices. We use six different deep learning models with varied sizes (in MB) and resource demands. We first characterize on-device post-training quantization on edge devices. Subsequently, we perform a detailed characterization of the performance and behaviors of quantized deep learning models with different precision modes. We discuss the benefits of post-training quantization, including reduced model size, improved inference latency, and decreased resource consumption. We also provide a detailed analysis of the downside of the quantized models, focusing on the reduction of their inference accuracy.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646443","Edge AI;Edge Devices;On-Device Compression;Post-Training Quantization","Deep learning;Performance evaluation;Training;Quantization (signal);Accuracy;Computational modeling;Graphics processing units","","2","","47","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Classifier Reuse-Based Contrastive Knowledge Distillation","Z. Gao; S. Yang; X. Yu; Z. Mo; L. Rui; Y. Yang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Postdoctoral Research Center, Industrial and Commercial Bank of China, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","121","127","As a popular method for model lightweighting, knowledge distillation has seen extensive development in recent years. Nevertheless, it heavily relies on labeled data for training, and in the absence of labeled data support, traditional knowledge distillation encounters significant hurdles. Therefore, it is crucial to explore the application of knowledge distillation in this field, aiming to improve the performance of lightweight models within unsupervised edge scenarios. We integrate contrastive learning, the currently prevalent self-supervised learning technique, with knowledge distillation to enhance the modelâ€™s knowledge distillation task through the assessment of similarity between sample pairs. Concurrently, we utilize a projection head to align features between the student and teacher models, employing the Siamese Classifier method to enable the reuse of the pre-trained classifier in the student model. This obviates the need for retraining the classifier and allows the student to acquire more knowledge from the teacher. A series of experimental results showcases that our model exhibits state-of-the-art performance on the benchmark dataset CIFAR-100, particularly when the ResNet $32 \times 4$ serves as the teacher for instructing the ResNet8 $\times 4$, surpassing other methods on average by $3.8 \%$.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646444","edge intelligence;knowledge distillation;contrastive learning;image classification;deep neural networks","Training;Knowledge engineering;Image coding;Image edge detection;Neural networks;Contrastive learning;Streaming media","","","","29","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Markov Blanket Composition of SLOs","B. Sedlak; V. C. Pujol; P. K. Donta; S. Dustdar","Distributed Systems Group, Vienna University of Technology (TU Wien), Vienna, Austria; Distributed Systems Group, Vienna University of Technology (TU Wien), Vienna, Austria; Distributed Systems Group, Vienna University of Technology (TU Wien), Vienna, Austria; Distributed Systems Group, Vienna University of Technology (TU Wien), Vienna, Austria",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","128","138","Smart environments use composable microservices pipelines to process Internet of Things (IoT) data, where each service is dependent on the outcome of its predecessor. To ensure Quality of Service (QoS), individual services must fulfill Service Level Objectives (SLOs); however, SLO fulfillment is dependent on resources (e.g., processing or storage), which are scarcely available within the Edge. Hence, when distributing services over heterogeneous devices, this raises the question of where to deploy each service to best fulfill both its own SLOs as well as those imposed by dependent services. In this paper, we maximize SLO fulfillment of a pipeline-based application by analyzing these dependencies. To achieve this, services and hosting devices alike are extended with a Markov blanket (MB) - a probabilistic view into their internal processes - which are composed into one overarching model. Given a mutable set of services, hosts, and SLOs, the composed MB allows inferring the optimal assignment between services and edge devices. We evaluated our method for a smart city scenario, which assigned pipelined services (e.g., video processing) under constraints from subsequent services (e.g., consumer latency). The results showed how our method can support infrastructure providers by optimizing SLO fulfillment for arbitrary devices currently available.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646438","Service Level Objectives;Computing Continuum;Markov Blanket;Quality of Service;Bayesian Networks","Smart cities;Pipelines;Microservice architectures;Quality of service;Probabilistic logic;Internet of Things;Edge computing","","1","","39","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Data Sharing-Aware Online Algorithms for Task Allocation in Edge Computing Systems","S. Rabinia; D. Grosu","Dept. Computer Science, Wayne State University, Detroit, USA; Dept. Computer Science, Wayne State University, Detroit, USA",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","139","148","Many of the tasks offloaded to edge devices perform computation to analyze sensing data. Transferring this data from end-user devices to edge servers leads to increased latency and congestion in the edge network. Since many of the offloaded tasks may require processing the same data items, the task allocation algorithms can exploit this to reduce the traffic in the networks and the number of edge servers needed to execute the tasks. Therefore, in this paper we design online algorithms for task allocation in edge computing systems that take into account the sharing of data among the tasks offloaded to the same server. We perform an extensive performance analysis by comparing our proposed algorithms with several sharing-oblivious baseline algorithms. The results show that our algorithms are able to reduce the amount of data transferred in the network by $30.2 \%$ to $\mathbf{9 2. 8 \%}$ and the number of utilized servers by $\mathbf{1 \%}$ to $82.8 \%$ compared to the sharing-oblivious baseline algorithms.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646447","Edge computing;task allocation;online algorithms;data sharing","Performance evaluation;Sensors;Performance analysis;Servers;Resource management;Task analysis;Edge computing","","1","","18","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"DONNA: Distributed Optimized Neural Network Allocation on CIM-Based Heterogeneous Accelerators","M. F. AlShams; K. S. Smagulova; S. A. Fahmy; M. E. Fouda; A. M. Eltawil","CEMSE Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; CEMSE Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; CEMSE Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Rain Neuromorphics, Inc., San Francisco, CA, USA; CEMSE Division, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","149","156","The continued development of neural network architectures continues to drive demand for computing power. While data center scaling continues, inference away from the cloud will increasingly rely on distributed inference on multiple devices. Most prior efforts have focused on optimizing singledevice inference or partitioning models to enhance inference throughput. Meanwhile, energy consumption continues to grow in importance as a factor of consideration. This work proposes a framework that searches for optimal model splits and distributes the partitions across the combination of devices taking into account throughput and energy. Participating devices are strategically grouped into homogeneous and heterogeneous clusters consisting of general-purpose CPU and GPU architectures, as well as emerging Compute-In-Memory (CIM) accelerators. The framework simultaneously optimizes inference throughput and energy consumption. It is able to demonstrate up to $4 \times$ speedup with approximately $4 \times$ per-device energy reduction in a heterogeneous setup compared to single GPU inference. The algorithm also finds a smooth Pareto-like curve in the energy-throughput space for CIM devices.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00027","King Abdullah University of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646434","Distributed Inference;Model splitting;Heterogeneous Hardware;Compute-in-memory;Heterogeneous Devices;GPU;CPU;CIM;ReRAM","Performance evaluation;Energy consumption;Computational modeling;Neural networks;Graphics processing units;Computer architecture;In-memory computing","","","","22","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"SlimNet: A Lightweight Attentive Network for Speech-Music-Noise Classification and Voice Activity Detection","N. Shu; Y. Wang; D. Caulley; D. V. Anderson","School of Electrical & Computer Eng Georgia Tech, Atlanta, GA; School of Electrical & Computer Eng Georgia Tech, Atlanta, GA; School of Electrical & Computer Eng Georgia Tech, Atlanta, GA; School of Electrical & Computer Eng Georgia Tech, Atlanta, GA",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","157","161","In this work, we present SlimNet, a hybrid architecture that incorporates a long short-term memory with an attention mechanism for speech-music-noise classification and speech vs non-speech categorization. Deep neural network approaches for these tasks have achieved remarkable success. However, deep models are computationally and energy intensive. When compared to state-of-the-art models, SlimNet uses up to 7 times fewer parameters, with up to 5 times faster inference while achieving comparable performance. The small footprint of SlimNet makes it a simple yet powerful candidate for devices with limited computational resources, and for edge computing devices in particular.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646448","Audio classification;deep neural networks;long short-term memory network;attention network;edge computing","Performance evaluation;Voice activity detection;Attention mechanisms;Computational modeling;Computer architecture;Task analysis;Long short term memory","","","","44","USGov","28 Aug 2024","","","IEEE","IEEE Conferences"
"Are Large Language Models Good Neural Architecture Generators for Edge?","H. Benmeziane; K. E. Maghraoui","IBM Research Europe, RÃ¼schlikon, Switzerland; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","162","165","Neural Architecture Search (NAS) is an incredibly indispensable technique in edge computing scenarios where computational resources are limited. However, the vastness of search spaces presents a formidable obstacle, hindering efficient exploration and identification of optimal architectures. In response, our study introduces an innovative paradigm that characterizes hardware and deep learning architectures, addressing the urgent demand for streamlined and swift architectural design processes.Our approach pioneers the utilization of a fine-tuned Large Language Model (LLM) to navigate the intricate terrain of architectural space. By harnessing the innate capabilities of LLMs, traditionally associated with natural language processing tasks, we extend their application domain to the realm of neural architecture design. Empirical assessments conducted on benchmark datasets such as CIFAR-10 and ImageNet highlight the effectiveness of our fine-tuned LLM-based methodology. Notably, our findings reveal performance levels on par with stateof-the-art (SOTA) NAS techniques, achieving unparalleled speed and completing architectural searches within seconds.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646428","Large language models;neural architecture search;architecture generation","Navigation;Large language models;Image edge detection;Deep architecture;Computer architecture;Natural language processing;Hardware","","","","4","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"EfficientMedSAM: Accelerating Medical Image Segmentation via Neural Architecture Search and Knowledge Distillation","A. C. Asiimwe; W. Das; H. Benmeziane; K. E. Maghraoui","Columbia University, New York, USA; Columbia University, New York, USA; IBM Research Europe, RÃ¼schlikon, Switzerland; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA",2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","166","173","Medical image segmentation is crucial for precise diagnosis, treatment planning, and disease monitoring in clinical practice. Despite the advancements in segmentation models inspired by the Segment Anything Model (SAM) architecture, their real-time application is limited due to computational inefficiencies arising from transformer-based architectures. We introduce EfficientMedSAM, a suite of high-speed, memory-efficient foundation models for universal medical image segmentation. Our approach leverages differentiable neural architecture search (NAS) to explore a novel search space emphasizing efficient operations over traditional attention mechanisms. The generated candidate architectures are further refined through knowledge distillation from larger MedSAM models and evaluated on the Kvasir dataset of endoscopic images. EfficientMedSAM achieves competitive mean Average Precision (mAP) while substantially reducing Multiply-Accumulate operations (MACs) and model parameters, enhancing throughput. We integrate a knowledge distillation (KD) pipeline that transfers knowledge from logits and attention maps, using saliency maps as proxies for attention map-based distillation. Our findings establish a proof of concept for largescale distributed training on the SA-Med2D-20M dataset, paving the way for real-time medical image segmentation advancements.","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646432","Medical Image Segmentation;Neural Architecture Search;Knowledge Distillation;Edge Computing","Training;Image segmentation;Accuracy;Computational modeling;Pipelines;Transformers;Throughput","","","","39","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
"Author Index","",,2024 IEEE International Conference on Edge Computing and Communications (EDGE),"28 Aug 2024","2024","","","175","176","","2767-9918","979-8-3503-6849-9","10.1109/EDGE62653.2024.00031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646433","","","","","","","IEEE","28 Aug 2024","","","IEEE","IEEE Conferences"
