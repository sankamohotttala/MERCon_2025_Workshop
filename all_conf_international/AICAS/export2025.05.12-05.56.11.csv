"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"An Efficient and Fast Softmax Hardware Architecture (EFSHA) for Deep Neural Networks","M. A. Hussain; T. -H. Tsai","National Central University, Zhongli, Taiwan; National Central University, Zhongli, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Deep neural networks are widely used in computer vision applications due to their high performance. However, DNNs involve a large number of computations in the training and inference phase. Among the different layers of a DNN, the softmax layer has one of the most complex computations as it involves exponent and division operations. So, a hardware-efficient implementation is required to reduce the on-chip resources. In this paper, we propose a new hardware-efficient and fast implementation of the softmax activation function. The proposed hardware implementation consumes fewer hardware resources and works at high speed as compared to the state-of-the-art techniques.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458541","Softmax layer;FPGA;deep neural networks;learning on-chip;area-efficient implementation","Training;Computer vision;Circuits and systems;Conferences;Neural networks;Computer architecture;Hardware","","11","","16","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"NeuroSim Validation with 40nm RRAM Compute-in-Memory Macro","A. Lu; X. Peng; W. Li; H. Jiang; S. Yu","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Compute-in-memory (CIM) is an attractive solution to process the extensive workloads of multiply-and-accumulate (MAC) operations in deep neural network (DNN) hardware accelerators. A simulator with options of various mainstream and emerging memory technologies, architectures and networks can be a great convenience for fast early-stage design space exploration of CIM accelerators. DNN+NeuroSim is an integrated benchmark framework supporting flexible and hierarchical CIM array design options from device-level, to circuit-level and up to algorithm-level. In this paper, we validate and calibrate the prediction of NeuroSim against a 40nm RRAM-based CIM macro post-layout simulations. First, the parameters of memory device and CMOS transistor are extracted from the TSMC's PDK and employed on the NeuroSim settings; the peripheral modules and operating process are also configured to be the same as the actual chip. Next, the area, critical path and energy consumption values from the SPICE simulations at the module-level are compared with those from NeuroSim. Some adjustment factors are introduced to account for transistor sizing and wiring area in layout, gate switching activity and post-layout performance drop, etc. We show that the prediction from NeuroSim is precise with chip-level error under 1% after the calibration.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458501","Compute-in-memory;hardware accelerator;deep neural network;design automation;benchmarking and validation","Semiconductor device modeling;Wiring;Computational modeling;Wires;Predictive models;Common Information Model (computing);Calibration","","11","","13","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"MLFlash-CIM: Embedded Multi-Level NOR-Flash Cell based Computing in Memory Architecture for Edge AI Devices","S. Zeng; Y. Zhang; Z. Zhu; Z. Qin; C. Dou; X. Si; Q. Li","University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; Institute of Microelectronics of the Chinese Academy of Sciences, Beijing, China; Southeast University, University of Electronic Science and Technology of China, Nanjing, China; University of Electronic Science and Technology of China, Chengdu, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Computing-in-Memory (CIM) is a promising method to overcome the well-known “Von Neumann Bottleneck” with computation insides memory, especially in edge artificial intelligence (AI) devices. In this paper, we proposed a 40nm 1Mb Multi-Level NOR-Flash cell based CIM (MLFlash-CIM) architecture with hardware and software co-design. Modeling of proposed MLFlash-CIM was analyzed with the consideration of cell variation, number of activated cells, integral non-linear (INL) and differential non-linear (DNL) of input driver, and quantization error of readout circuits. We also proposed a multi-bit neural network mapping method with 1/n top values and an adaptive quantization scheme to improve the inference accuracy. When applied to a modified VGG-16 Network with 16 layers, the proposed MLFlash-CIM can achieve 92.73% inference accuracy under CIFAR-10 dataset. This CIM structure also achieved a peak throughput of 3.277 TOPS and an energy efficiency of 35.6 TOPS/W for 4-bit multiplication and accumulation (MAC) operations.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458438","Multi-Level NOR-Flash;Computing in memory;Artificial Intelligence;Convolutional Neural Network","Quantization (signal);Microprocessors;Neural networks;Computer architecture;Throughput;Energy efficiency;Software","","7","","13","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Energy-Efficient Intelligent EPTS Device using Novel DCNN-Based Dynamic Sensor Activation","H. Kim; J. Kim; Y. -S. Kim; M. Kim; Y. Lee","Department of Electrical Engineering, POSTECH, Pohang, Republic of Korea; Department of Electrical Engineering, POSTECH, Pohang, Republic of Korea; Institute of Artificial Intelligence, POSTECH, Pohang, Republic of Korea; Sports AIX Graduate Program, POSTECH, Pohang, Republic of Korea; Department of Electrical Engineering, POSTECH, Pohang, Republic of Korea",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","1","The electronic performance and tracking system (EPTS) device is one of the wearable devices widely used to gather sports data of athletes in real-time. This work demonstrates an adaptive sampling strategy controlled by the advanced on-device deep convolutional neural network (DCNN) operation, extending the lifetime of EPTS device even realized in a small form factor.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458469","electronic performance and tracking system;on-device DCNN processing;sports wearable device","Performance evaluation;Adaptive systems;Circuits and systems;Wearable computers;Conferences;Real-time systems;Energy efficiency","","","","2","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient Digital Implementation of n-mode Tensor-Matrix Multiplication","C. Gianoglio; E. Ragusa; R. Zunino; P. Gastaldo","DITEN Department, University of Genoa, Genova, Italy; DITEN Department, University of Genoa, Genova, Italy; DITEN Department, University of Genoa, Genova, Italy; DITEN Department, University of Genoa, Genova, Italy",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","With the growth of pervasive electronics, the availability of compact digital circuitry for the support of data processing is becoming a key requirement. This paper tackles the design of a digital architecture supporting the n-mode tensormatrix product in fixed point representation. The design aims to minimize the resources occupancy, targeting low cost and low power devices. Tests on a Kintex-7 FPGA confirm that the architecture leads to an efficient digital implementation, which can afford real-time performances on benchmark applications with power consumption lower than 100mW.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458404","Tensors;Multidimensional Array;Digital Architectures;Low-Power","Performance evaluation;VHDL;Power demand;Circuits and systems;Conferences;Benchmark testing;Data processing","","2","","19","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"An Ultra-Low-power Real-Time Hand-Gesture Recognition System for Edge Applications","Y. Lu; Z. Li; T. T. -H. Kim","Centre for Integrated Circuits and Systems School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Integrated Circuits and Systems School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Centre for Integrated Circuits and Systems School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","1","This demonstration presents an ultra-low-power real-time hand gesture recognition system for edge applications. The proposed design utilizes a hybrid classifier based recognition core for static gesture recognition and an error-tolerant sequence analyzer for dynamic gesture decision. The recognition core comprises a shallow decision tree and an Edge-CNN which categorizes different gestures only based on the pixel data at the edge of the hand region. As a result, the on-chip memory and computation intensity are dramatically reduced. The proposed system can recognize 24 dynamic hand gestures with an average accuracy of 92.6% with 184μW power consumption at 25MHz.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458436","low-power;high accuracy;hand gesture recognition;hybrid classifiers","Power demand;Circuits and systems;Conferences;Gesture recognition;Real-time systems;System-on-chip;Decision trees","","4","","2","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Real-time Language Recognition using Hyperdimensional Computing on Phase-change Memory Array","G. Karunaratne; A. Rahimi; M. L. Gallo; G. Cherubini; A. Sebastian","IBM Research, Zürich, Switzerland; IBM Research, Zürich, Switzerland; IBM Research, Zürich, Switzerland; IBM Research, Zürich, Switzerland; IBM Research, Zürich, Switzerland",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","1","In this demo, we show how Hyperdimensional(HD) computing can be used to recognize the language of a user-input sentence. Hyperdimensional computing (HDC) [1] is one promising brain-inspired computing approach that relies on representing entities using high dimensional (up to 10,000 dimensions) vectors called hypervectors. Similar to the brain, where representations are given by thousands of randomly originated neurons, a set of (pseudo)random quasi-orthogonal hypervectors forms the basis in the HDC framework. These hypervectors are then combined and compared using a well-defined set of algebraic operations to derive representations for composite entities and to find similarities, respectively.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458470","In-memory computing;Phase-change memory;HD computing","Phased arrays;Circuits and systems;Conferences;Neurons;Real-time systems;Phase change memory;Artificial intelligence","","4","","2","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"A Real-Time Face Recognition System by Efficient Hardware-Software Co-Design on FPGA SoCs","H. Wang; S. Cao; S. Xu","Key laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University, Shanghai, China; Key laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University, Shanghai, China; Key laboratory of Specialty Fiber Optics and Optical Access Networks, Shanghai University, Shanghai, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","2","With the development of deep learning, the accuracy of face recognition has been significantly improved. Current face recognition systems are mostly designed for CPU or GPU platforms, and faces significant latency and power constraints when migrated to embedded devices. In this live demonstration, a real-time face recognition system based on FPGA System-on-Chip (SoC) platforms is presented. To achieve real-time processing, the face recognition algorithm based on convolutional neural network is optimized first to a hardware-friendly network model and is accelerated on FPGA, while the face detection and face alignment are implemented on ARM. The latency of the entire system is 52 ms, and the face recognition accuracy on the LWF data set reaches 99.05%.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458462","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458462","","Deep learning;Circuits and systems;Face recognition;Conferences;Graphics processing units;Real-time systems;System-on-chip","","2","","2","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Design optimization for ADMM-Based SVM Training Processor for Edge Computing","S. -A. Huang; Y. -Y. Hsieh; C. -H. Yang","Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","5","This paper presents an optimized support vector machine (SVM) training processor employing the alternative direction method of multipliers (ADMM) optimizer. Low-rank approximation is exploited to reduce the dimension of the kernel matrix by employing the Nyström method. Verified in four datasets, the proposed ADMM-based training processor with rank approximation reduces 32× of matrix dimension with only 2% drop in inference accuracy. Compared to the conventional sequential minimal optimization (SMO) algorithm, the ADMM-based training algorithm is able to achieve a 9.8×107 shorter latency for training 2048 samples. Hardware optimization techniques, including pre-computation and memory sharing, are proposed to reduce the computational complexity by 62% and the memory usage by 60%. As a proof of concept, an epileptic seizure detector is designed to demonstrate the effectiveness of the proposed optimization techniques. The chip achieves a 153,310× higher energy efficiency and a 364× higher throughput-to-area ratio for SVM training than a high-end CPU. This work provides a promising solution for edge devices which require low-power and real-time training.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458569","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458569","Support vector machine (SVM);on-line training;alternative direction method of multipliers (ADMM);rank approximation;hardware-efficient realization","Training;Support vector machines;Detectors;Approximation algorithms;Hardware;Inference algorithms;Real-time systems","","","","16","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Quantization Strategy for Pareto-optimally Low-cost and Accurate CNN","K. Nakata; D. Miyashita; A. Maki; F. Tachibana; S. Sasaki; J. Deguchi; R. Fujimoto","Kioxia Corporation, Institute of Memory Technology Research & Development, Kawasaki, Japan; Kioxia Corporation, Institute of Memory Technology Research & Development, Kawasaki, Japan; Kioxia Corporation, Institute of Memory Technology Research & Development, Kawasaki, Japan; Kioxia Corporation, Institute of Memory Technology Research & Development, Kawasaki, Japan; Kioxia Corporation, Institute of Memory Technology Research & Development, Kawasaki, Japan; Kioxia Corporation, Institute of Memory Technology Research & Development, Kawasaki, Japan; Kioxia Corporation, Institute of Memory Technology Research & Development, Kawasaki, Japan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Quantization is an effective technique to reduce memory and computational costs for inference of convolutional neural networks (CNNs). However, it has not been clarified which model can achieve higher recognition accuracy with lower memory and computational costs: a fat model (large number of parameters) quantized to an extremely low bit width (e.g., 1 or 2 bits) or a slim model (small number of parameters) quantized to moderately low bit width (e.g., 4 or 5 bits). To answer this question, we define a metric that combines the number of parameters and computations with bit widths of quantized weight parameters. Using this metric, we demonstrate that Pareto-optimal performance, where the best accuracy is obtained at a given memory or computational cost, is achieved when a slim model is moderately quantized rather than when a fat model is extremely quantized. Moreover, employing a strategy based on this finding, we empirically show that the Pareto frontier is improved by 4.3× under a post-training quantization scenario on the ImageNet dataset.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458452","CNN;quantization;pruning","Measurement;Weight measurement;Quantization (signal);Correlation;Computational modeling;Hardware;Computational efficiency","","","","25","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"A Quality-Oriented Reconfigurable Convolution Engine Using Cross-Shaped Sparse Kernels for Highly-Parallel CNN Acceleration","C. -W. Weng; C. -T. Huang","Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Computational imaging CNNs are computationally intensive and need complexity reduction to support high-throughput applications. However, conventional compact model reduction tends to degrade image quality severely as models become too shallow. On the other hand, irregular pruning-based techniques induce considerable circuit overheads and imbalanced workloads, especially for highly-parallel accelerators. In this paper, we propose cross-shaped sparse kernels to regularly reduce model complexity while preserving image quality well. They improve PSNR (peak signal-to-noise ratio) by 0.03-0.31 dB on classic denoising and super-resolution networks compared to compact depth reduction. Moreover, we design a highly-parallel reconfigurable convolution engine to support three sparsity configurations (0%, 50% and 75% of sparsity) for our complexity-saving method. It can achieve high-quality inference for a wide complexity range with full utilization of MACs. With TSMC 40nm technology, the engine uses 9.85M of logic gates for delivering 8.2 TOPS of inference capability, and only needs 8.4% logic overheads and 14.9% additional power consumption for the quality-oriented reconfigurability. Finally, we do a case study on ERNets for real-time inference, and this work can achieve 10.114.8x higher area efficiency in terms of Mpixel/s/mm2 compared to SparTen.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458472","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458472","","Image quality;PSNR;Convolution;Noise reduction;Logic gates;Real-time systems;Complexity theory","","2","","12","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Adaptable Approximation Based on Bit Decomposition for Deep Neural Network Accelerators","T. Soliman; C. De La Parra; A. Guntoro; N. Wehn","Robert Bosch GmbH, Renningen, Germany; Robert Bosch GmbH, Renningen, Germany; Robert Bosch GmbH, Renningen, Germany; TU Kaiserslautern, Kaiserslautern, Germany",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","For optimized deployment of deep neural networks in embedded devices, hardware approximations offer acceptable trade-offs between computational resources, power consumption, and network accuracy. In this paper, we propose a novel approximation technique with variable error range, targeting various architectures that adopt bit-decomposition of the Multiply and Accumulate (MAC) operation, especially emerging in-memory computing based architectures. Through our experiments with state-of-the-art neural networks for image classification using CIFAR10 and ImageNet, we demonstrate that this approximation technique achieves 2x power efficiency compared to the base architecture, with an accuracy loss of less than 3%.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458456","in-memory computing;approximate computing;approximate neural networks","Power demand;Circuits and systems;Conferences;Neural networks;Computer architecture;Throughput;Hardware","","3","","17","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Single RRAM Cell-based In-Memory Accelerator Architecture for Binary Neural Networks","H. Oh; H. Kim; N. Kang; Y. Kim; J. Park; J. -J. Kim","Pohang University of Science and Technology, Pohang, Korea; Pohang University of Science and Technology, Pohang, Korea; Pohang University of Science and Technology, Pohang, Korea; Pohang University of Science and Technology, Pohang, Korea; Pohang University of Science and Technology, Pohang, Korea; Pohang University of Science and Technology, Pohang, Korea",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","As Binary Neural Networks (BNNs) started to show promising performance with limited memory and computational cost, various RRAM-based in-memory BNN accelerator designs have been proposed. While a single RRAM cell can represent a binary weight, previous designs had to use two RRAM cells for a weight to enable XNOR operation between a binary weight and a binary activation. In this work, we propose to convert the XNOR-based computation to RRAM-friendly multiplication without any accuracy loss so that we can reduce the required number of RRAM cells by half. As the required number of cells to compute a BNN model is reduced, the energy and area overhead is also reduced. Experimental results show that the proposed inmemory accelerator architecture achieves ~1.9x area efficiency improvement and ~1.8x energy efficiency improvement over previous architectures on various image classification benchmarks.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458444","National Research Foundation of Korea; Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458444","In-memory computing;RRAM;BNN","Microprocessors;Magnetic resonance imaging;Conferences;Neural networks;Accelerator architectures;Phase change random access memory;Energy efficiency","","4","","7","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"An AI AUV Enabling Vision-based Diver-following and Obstacle Avoidance with 3D-modeling Dataset","Y. -C. Chou; H. -H. Chen; C. -C. Wang; H. -M. Chou; C. -C. Wang","Institute of Undersea Technology, National Sun Yat-sen University, Kaohsiung, Taiwan; Institute of Undersea Technology, National Sun Yat-sen University, Kaohsiung, Taiwan; Institute of Undersea Technology, National Sun Yat-sen University, Kaohsiung, Taiwan; Institute of Undersea Technology, National Sun Yat-sen University, Kaohsiung, Taiwan; Department of Electrical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","This paper presents an AUV with AI (artificial intelligence), which is able to perform real-time optical vision-based diver-following and forward looking altimeter-based obstacle avoidance. The AI AUV is equipped with thrusters and a standard navigation-related sensor suite. A diver detection convolutional neural network, a suite of motion controllers, and a diver detection payload device are developed to enable diver-following functionality of the AUV. An obstacle avoidance algorithm based on forward looking altimeters is developed to enhance the waypoint navigation security of the AUV with obstacle avoidance functionality. The diver-following and the obstacle avoidance capabilities of the Taiwan Moonshot AUV under different scenarios are evaluated through hardware-in-the-loop simulations. In addition, the designated single diver following capability of the Taiwan Moonshot AUV is also verified through closed water experiments conducted in a towing tank.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458431","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458431","Artificial Intelligence;Autonomous Underwater Vehicle;Diver-following;Obstacle Avoidance","Navigation;Hardware-in-the-loop simulation;Optical network units;Real-time systems;Trajectory;Optical sensors;Security","","7","","9","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"A Memristor Model with Concise Window Function for Spiking Brain-Inspired Computation","J. Xu; D. Wang; F. Li; L. Zhang; D. Stathis; Y. Yang; Y. Jin; A. Lansner; A. Hemani; Z. Zou; L. -R. Zheng","School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; Department of Electrical Engineering, Technical University of Denmark, Kongens Lyngby, Denmark; KTH Royal Institution of Technology, Stockholm, Sweden; KTH Royal Institution of Technology, Stockholm, Sweden; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; KTH Royal Institution of Technology, Stockholm, Sweden; KTH Royal Institution of Technology, Stockholm, Sweden; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","This paper proposes a concise window function to build a memristor model, simulating the widely-observed nonlinear dopant drift phenomenon of the memristor. Exploiting the non-linearity, the memristor model is applied to the in-situ neuromorphic solution for a cortex-inspired spiking neural network (SNN), spike-based Bayesian Confidence Propagation Neural Network (BCPNN). The improved memristor model utilizing the proposed window function is able to retain the boundary effect and resolve the boundary lock and inflexibility problem, while it is simple in form that can facilitate large-scale neuromorphic model simulation. Compared with the state-of-the-art general memristor model, the proposed memristor model can achieve a $5.8 \times$ reduction of simulation time at a competitive fitting level in cortex-comparable large-scale software simulation. The evaluation results show an explicit similarity between the non-linear dopant drift phenomenon of the memristor and the BCPNN learning rule, and the memristor model is able to emulate the key traces of BCPNN with a correlation coefficient over 0.99.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458424","National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458424","Memristor;window function;non-linear dopant drift;spiking neural network (SNN);Bayesian confidence propagation neural network (BCPNN)","Neuromorphics;Computational modeling;Fitting;Emulation;Memristors;Brain modeling;Software","","8","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Unbalanced Bit-slicing Scheme for Accurate Memristor-based Neural Network Architecture","S. Diware; A. Gebregiorgis; R. V. Joshi; S. Hamdioui; R. Bishnoi","Computer Engineering Lab, Delft University of Technology, Delft, The Netherlands; Computer Engineering Lab, Delft University of Technology, Delft, The Netherlands; IBM Research Division, Yorktown Heights, NY, USA; Computer Engineering Lab, Delft University of Technology, Delft, The Netherlands; Computer Engineering Lab, Delft University of Technology, Delft, The Netherlands",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Emerging memristor-based computing has the potential to achieve higher computational efficiency over conventional architectures. Bit-slicing scheme, which represents a single neural weight using multiple memristive devices, is usually introduced in memristor-based neural networks to meet high bit-precision demands. However, the accuracy of such networks can be significantly degraded due to non-zero minimum conductance (Gmin) of memristive devices. This paper proposes an unbalanced bit-slicing scheme; it uses smaller slice sizes for more important bits to provide higher sensing margin and reduces the impact of non-zero (Gmin).Moreover, the unbalanced bit-slicing is assisted by 2's complement arithmetic which further improves the accuracy. Simulation results show that our proposed scheme can achieve up to 8.8× and 1.8× accuracy compared to state-of-the-art for single-bit and two-bit configurations respectively, at reasonable energy overheads.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458443","","Performance evaluation;Degradation;Circuits and systems;Simulation;Conferences;Neural networks;Memristors","","10","","13","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"A Flexible and Fast PyTorch Toolkit for Simulating Training and Inference on Analog Crossbar Arrays","M. J. Rasch; D. Moreda; T. Gokmen; M. Le Gallo; F. Carta; C. Goldberg; K. El Maghraoui; A. Sebastian; V. Narayanan",IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research; IBM Research,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","We introduce the IBM ANALOG HARDWARE ACCELERATION KIT, a new and first of a kind open source toolkit to simulate analog crossbar arrays in a convenient fashion from within PYTORCH (freely available at https://github.com/IBM/aihwkit). The toolkit is under active development and is centered around the concept of an “analog tile” which captures the computations performed on a crossbar array. Analog tiles are building blocks that can be used to extend existing network modules with analog components and compose arbitrary artificial neural networks (ANNs) using the flexibility of the PYTORCH framework. Analog tiles can be conveniently configured to emulate a plethora of different analog hardware characteristics and their non-idealities, such as device-to-device and cycle-to-cycle variations, resistive device response curves, and weight and output noise. Additionally, the toolkit makes it possible to design custom unit cell configurations and to use advanced analog optimization algorithms such as Tiki-Taka. Moreover, the backward and update behavior can be set to “ideal"" to enable hardware-aware training features for chips that target inference acceleration only. To evaluate the inference accuracy of such chips over time, we provide statistical programming noise and drift models calibrated on phase-change memory hardware. Our new toolkit is fully GPU accelerated and can be used to conveniently estimate the impact of material properties and non-idealities of future analog technology on the accuracy for arbitrary ANNs.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458494","Rensselaer Polytechnic Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458494","analog AI;non-volatile memory;in memory computing;neuromorphic;memristive crossbar arrays","Training;Tiles;Software algorithms;Artificial neural networks;Tools;Programming;Inference algorithms","","75","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Characterization of Drain Current Variations in FeFETs for PIM-based DNN Accelerators","N. E. Miller; Z. Wang; S. Dash; A. I. Khan; S. Mukhopadhyay","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, Georgia",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","We analyze the impact of drain current (IDS) variation in 28 nm high-K metal-gate Ferroelectric FET devices on FeFET-based processing-in-memory (PIM) deep neural network (DNN) accelerators. Non-Normal variation in IDS is observed due to repeated read operation on FeFET devices with different channel dimensions at various read frequencies. Device-circuit co-analysis using the measured current distribution shows a 1 to 3 percent accuracy degradation of an FeFET-based PIM platform when classifying the Fashion-MNIST dataset with the LeNET-5 DNN model. This accuracy drop can be fully recovered with variation-aware training methods, showing that individual FeFET device current variation over many read cycles is not prohibitive to the design of DNN accelerators.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458437","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458437","DNN Accelerator;FeFET;PIM","Training;Performance evaluation;Degradation;Conferences;Neural networks;Current distribution;Loss measurement","","11","","15","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"FPGA-accelerated Agent-Based Simulation for COVID-19","L. Fu; C. Guo; W. Luk","Imperial College London, United Kingdom; Imperial College London, United Kingdom; Imperial College London, United Kingdom",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Agent-based models (ABMs) can provide realistic dynamics for epidemics at the individual level so that users can observe and predict the spreading pattern and the effectiveness of intervention over time and space. This paper proposes an FPGA-based accelerator for agent-based epidemic modeling for COVID-19. The optimizations enabling the effective acceleration of the simulation procedure are presented. The key idea is to partition the calculation properly to decouple the on-chip resource usage from the population size. Also, an algorithmic adaptation is proposed to reduce the latency caused by conditional branches within loops. An experimental implementation on an Intel Arria 10 GX 10AX115S2F45I1SG FPGA running at 240MHz achieves 2.2 and 1.9 times speed-up respectively over a CPU reference using 10 cores on an Intel Xeon Gold 6230 CPU and a GPU reference on an Nvidia GeForce RTX 2080 Ti GPU.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458570","","COVID-19;Epidemics;Adaptation models;Sociology;Graphics processing units;Predictive models;Central Processing Unit","","1","","10","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Hardware Approximation of Exponential Decay for Spiking Neural Networks","S. Eissa; S. Stuijk; H. Corporaal","Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Spiking neural networks (SNNs) may enable low-power intelligence on the edge by combining the merits of deep learning with the computational paradigms found in the human neo-cortex. The choice of neuron model is an open research topic. Many spiking models implement neural dynamics from biology that involve one or more exponential decay functions. Previous work focused on accurate modeling of the exponential decay function on neuromorphic hardware to the last significant bit (LSB). In this paper, we explore the limits of error resilience in SNNs by aggressively approximating their exponential decay functions and allowing for losses within our bit precision. Three approximation methods are presented and implemented with varying degrees of precision resulting in 10 different implementations. Their hardware cost and inference accuracy on benchmark networks and applications are compared. To improve the inference accuracy, we implemented fine-tuning. We also introduced hardware programmability for certain time constants as hyperparameters. Our results show resilience to lossy approximation, fast fine-tuning, and a low energy consumption of 47 fJ per operation.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458560","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458560","Neuromorphic Computing;Spiking Neural Networks;Exponential function;Error Resilience;Deep Learning","Training;Neuromorphics;Computational modeling;Biological system modeling;Benchmark testing;Hardware;Robustness","","5","","21","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"iAMEC, an Intelligent Autonomous Mover for Navigation in Indoor People Rich Environments","Y. -T. Hwang; K. -H. Chen; C. -P. Fan; Y. -K. Lai; C. -B. Wu; H. -P. Tsai; W. -L. Lin; K. -H. Lin","Department of Electrical Engineering, National Chung Hsing University, Taichung; Department of Electronic Engineering, Feng-Chia University, Taichung; Department of Electrical Engineering, National Chung Hsing University, Taichung; Department of Electrical Engineering, National Chung Hsing University, Taichung; Department of Electrical Engineering, National Chung Hsing University, Taichung; Department of Electrical Engineering, National Chung Hsing University, Taichung; Department of Electrical Engineering, National Chung Hsing University, Taichung; Department of Electrical Engineering, National Formosa University, Yunlin",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","This paper presents the key sensing, navigation techniques, and edge AI computing chip design for an autonomous mover, iAMEC, tailored to indoor and people rich environments such as shopping centers. iAMEC aims at serving as a platform for service robots and features swift maneuverability and collision avoidance in navigation. It is equipped with a smart sensing module consisting of a Lidar, a camera and an ultrasonic array radar. Camera images are analyzed on the fly by using 2-stage CNN models for not only object recognition but also pedestrian behavior prediction. The ultrasonic array radar can detect both the distance and the direction of the surrounding objects in short distance. The low cost 1-ray Lidar performs SLAM as well as scanning for a wider range. Data fusion results of these three sensing techniques are passed to the navigation/control module, which determines the optimal path and also steers iAMEC to the destination. Navigation is based on a reinforcement learning model, which is trained in a virtual environment by using a simulation engine, UNITY. DLA (deep learning architecture) acceleration chip design and the associated model-to-DLA mapping tool are also developed to facilitate real time edge computing of CNN based image analysis. A 4-wheeled autonomous mover prototype has been built and mounted with developed sensing, navigation and control modules. The evaluation results indicate preliminary success of iAMEC in navigating under a controlled environment.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458563","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458563","autonomous mover;CNN based image analysis;data fusion;navigation;deep learning architecture","Laser radar;Navigation;Computational modeling;Robot vision systems;Prototypes;Computer architecture;Radar imaging","","","","8","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"MRAM-based BER resilient Quantized edge-AI Networks for Harsh Industrial Conditions","V. Parmar; M. Suri; K. Yamane; T. Lee; N. L. Chung; V. B. Naik","Department of Electrical Engineering, Indian Institute of Technology, Delhi, India; Department of Electrical Engineering, Indian Institute of Technology, Delhi, India; GLOBALFOUNDRIES, Singapore; GLOBALFOUNDRIES, Singapore; GLOBALFOUNDRIES, Singapore; GLOBALFOUNDRIES, Singapore",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","We investigate Edge-AI Inference (EAI) architectures based on 22nm FD-SOI embedded-MRAM (eMRAM) using quantized neural networks (QNN) for inference applications in harsh industrial conditions having strong magnetic field and wide operating temperature (-40~125 °C). We achieved best case test accuracy of 98.99% with Quantized-Convolutional Neural Network (QCNN) and 89.94% with Quantized-Multi-layer Perceptron (QMLP) surpassing prior reported results in literature on MNIST dataset. By exploiting BER resilience of QNN, we show that eMRAM based EAI offers a superior magnetic immunity of ≈ 700 Oe at 125 °C (≈ 98% accuracy) without the use of ECC and significant energy saving of ≈ 14% for QCNN and ≈ 11% for QMLP. A detailed analysis on the tradeoff between retention time, write energy and inference accuracy is presented.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458528","MRAM;Edge AI;BNN;QNN;Magnetic Immunity","Service robots;Wearable computers;Conferences;Neural networks;Silicon-on-insulator;Programming;Magnetic fields","","2","","13","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"An 176.3 GOPs Object Detection CNN Accelerator Emulated in a 28nm CMOS Technology","Y. -C. Lu; C. -W. Chen; C. -C. Pu; Y. -T. Lin; J. -K. Jhan; S. -P. Liang; W. -L. Tseng; C. -S. Chen; C. -Y. Yu; H. -W. Wang; H. -H. Shuai; H. Chiueh","Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Taiwan Semiconductor Research Institute, Hsinchu, Taiwan; Taiwan Semiconductor Research Institute, Hsinchu, Taiwan; Taiwan Semiconductor Research Institute, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Object Detection methods are an important subject in the implementations of Artificial Intelligent systems. Many attempts to build up a real-time object detection hardware/software in an SoC are presented in recent research. However, due to the demanding in both memory bandwidth and parallel computing resources, only few designs can fulfill the real-time requirement, which is important for applications or payloads such as Drones, UAVs, and autonomous vehicles. In this paper, issues in design of an SoC based object-detection will be discussed, and an on-going hardware design base on YOLO algorithm and ARC Platform will be presented. With the optimization in both numbers of Processing Element and Memory Bandwidth, An 176.3 GOPs CNN accelerator with 30 fps performance at 400MHz is presented. In addition to the Object-Detection engine, a ZCA image preprocessor and NMS postprocessing are also proposed to simplify the corresponding CNN model and enhance the real-time performance of Object-Detection. The emulation results and demonstration videos in the ARC platform will be presented. The post-layout simulation of current designverified the targeting real-time performance in a 28nm CMOS technology.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458492","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458492","Object Detection;YOLO;Convolution Neural Network;Hardware Accelerator;System-on-Chip;SoC","Semiconductor device modeling;Emulation;Object detection;Bandwidth;CMOS technology;Real-time systems;Engines","","1","","13","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Live Demo: An 176.3 GOPs Object Detection CNN Accelerator Emulated in a 28nm CMOS Technology","Y. -C. Lu; C. -W. Chen; C. -C. Pu; Y. -T. Lin; J. -K. Jhan; S. -P. Liang; W. -L. Tseng; C. -S. Chen; C. -Y. Yu; H. -W. Wang; H. -H. Shuai; H. Chiueh","Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Taiwan Semiconductor Research Institute, Hsinchu, Taiwan; Taiwan Semiconductor Research Institute, Hsinchu, Taiwan; Taiwan Semiconductor Research Institute, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Electrical and Computer Engineering, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","1","This work presents an object detection system based on Tiny YOLOv2 algorithm and ARC platform, and demonstrates on a Synopsys HAPS-80 FPGA.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458495","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458495","","Circuits and systems;Conferences;Object detection;CMOS technology;Artificial intelligence;Field programmable gate arrays","","5","","2","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"FL-HDC: Hyperdimensional Computing Design for the Application of Federated Learning","C. -Y. Hsieh; Y. -C. Chuang; A. -Y. A. Wu","Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","5","Federated learning (FL) is a privacy-preserving learning framework, which collaboratively learns a centralized model across edge devices. Each device trains an independent model with its local dataset and only uploads model parameters to mitigate privacy concerns. However, most FL works focus on deep neural networks (DNNs), whose intensive computation hinders FL from practical realization on resource-limited edge devices. In this paper, we exploit the high energy efficiency properties of hyperdimensional computing (HDC) to propose a federated learning HDC (FL-HDC). In FL-HDC, we bipolarize model parameters to significantly reduce communication costs, which is a primary concern in FL. Moreover, we propose a retraining mechanism with adaptive learning rates to compensate for the accuracy degradation caused by bipolarization. Under the FL scenario, our simulation results show the effectiveness of our proposed FL-HDC across two datasets, MNIST and ISOLET. Compared with the previous work that transmits complete model parameters to the cloud, FL-HDC greatly reduces 23x and 9x communication costs with comparable accuracy in ISOLET and MNIST, respectively.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458526","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458526","Hyperdimensional computing;federated learning;retraining mechanism;adaptive learning rate.","Degradation;Adaptive learning;Adaptation models;Computational modeling;Simulation;Neural networks;Learning (artificial intelligence)","","16","","15","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"The 2020 Low-Power Computer Vision Challenge","X. Hu; M. -C. Chang; Y. Chen; R. Sridhar; Z. Hu; Y. Xue; Z. Wu; P. Pi; J. Shen; J. Tan; X. Lian; J. Liu; Z. Wang; C. -H. Liu; Y. -S. Han; Y. -Y. Sung; Y. Lee; K. -C. Wu; W. -X. Guo; R. Lee; S. Liang; Z. Wang; G. Ding; G. Zhang; T. Xi; Y. Chen; H. Cai; L. Zhu; Z. Zhang; S. Han; S. Jeong; Y. Kwon; T. Wang; J. Pan","Purdue University, West Lafayette, IN, USA; University at Albany, State University of New York, NY, USA; University at Albany, State University of New York, NY, USA; Texas A&M University, College Station, TX, USA; Texas A&M University, College Station, TX, USA; Texas A&M University, College Station, TX, USA; Texas A&M University, College Station, TX, USA; Texas A&M University, College Station, TX, USA; Texas A&M University, College Station, TX, USA; Kwai Inc. Seattle AI lab, Seattle, WA, USA; Kwai Inc. Seattle AI lab, Seattle, WA, USA; Kwai Inc. Seattle AI lab, Seattle, WA, USA; The University of Texas at Austin, Austin, TX, USA; National Chiao Tung University, Hsinchu, Taiwan; National Chiao Tung University, Hsinchu, Taiwan; National Chiao Tung University, Hsinchu, Taiwan; National Chiao Tung University, Hsinchu, Taiwan; National Chiao Tung University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan; SKLCA, Institute of Computing Technology, CAS, Beijing, China; SKLCA, Institute of Computing Technology, CAS, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; Department of Computer Vision Technology (VIS), Baidu Inc.; Department of Computer Vision Technology (VIS), Baidu Inc.; University of California, Berkeley, CA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; Massachusetts Institute of Technology, Cambridge, MA, USA; The State University of New York, Korea; The State University of New York, Korea; Georgia Institute of Technology, Atlanta, GA, USA; Phillips Academy, Andover, MA, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","AI computer vision has advanced significantly in recent years. IoT and edge computing devices such as mobile phones have become the primary computing platform for many end users. Mobile devices such as robots and drones that rely on batteries demand for energy efficient computation. Since 2015, the IEEE Annual International Low-Power Computer Vision Challenge (LPCVC) was held to identify energy-efficient AI and computer vision solutions. The 2020 LPCVC includes three challenge tracks: (1) PyTorch UAV Video Track, (2) FPGA Image Track, and (3) On-device Visual Intelligence Competition (OVIC) Tenforflow Track. This paper summarizes the 2020 winning solutions from the three tracks of LPCVC competitions. Methods and future directions for energy-efficient AI and computer vision research are discussed.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458522","Facebook; Xilinx; Google; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458522","Low-power;computer vision;challenge;drone;scene text;FPGA;model compression;knowledge distilling;NAS","Computer vision;Visualization;Conferences;Energy efficiency;Mobile handsets;Computational efficiency;Artificial intelligence","","","","1","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Compute-in-RRAM with Limited On-chip Resources","A. Lu; X. Peng; S. Yu","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Compute-in-memory (CIM) is a new computing paradigm that addresses the memory-wall problem in the deep learning accelerator. Resistive Random Access Memory (RRAM) is an emerging non-volatile memory that is suitable as on-chip embedded memory to store the weights of the deep neural network (DNN) models. In this paper, first we will review general design considerations of RRAM-CIM prototype chip integrated with CMOS peripheral circuitry such as weight mapping scheme and analog-to-digital conversion requirement. Second, we will discuss the challenges in CIM chip design when the chip area is constrained to hold all the weights of the large-scale DNN models. Finally, we will present a design methodology to enable the runtime reconfiguration of DNN models on a custom CIM chip instance with fixed hardware resources.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458481","compute-in-memory;deep learning accelerator;area constraint;reconfigurable architecture","Semiconductor device modeling;Runtime;Processor scheduling;Nonvolatile memory;Computational modeling;Resistive RAM;Prototypes","","1","","25","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"An Efficient and Low-Power MLP Accelerator Architecture Supporting Structured Pruning, Sparse Activations and Asymmetric Quantization for Edge Computing","W. -C. Lin; Y. -C. Chang; J. -D. Huang","Institute of Electronics, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Institute of Electronics, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Institute of Electronics, National Yang Ming Chiao Tung University, Hsinchu, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","5","Multilayer perceptron (MLP) is one of the most popular neural network architectures used for classification, regression, and recommendation systems today. In this paper, we propose an efficient and low-power MLP accelerator for edge computing. The accelerator has three key features. First, it aligns with a novel structured weight pruning algorithm that merely needs minimal hardware support. Second, it takes advantage of activation sparsity for power minimization. Third, it supports asymmetric quantization on both weights and activations to boost the model accuracy especially when those values are in low-precision formats. Furthermore, the number of PEs is determined based on the available external memory bandwidth to ensure the high PE utilization, which avoids area and energy wastes. Experiment results show that the proposed MLP accelerator with only 8 MACs operates at 1.6GHz using the TSMC 40nm technology, delivers 899GOPS equivalent computing power after structured weight pruning on a well-known image classification model, and achieves an equivalent energy efficiency of 9.7TOPS/W, while the model accuracy loss is less than 0.3% with the help of asymmetric quantization.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458511","multilayer perceptron;model compression;hardware accelerator;algorithm-hardware co-design","Quantization (signal);Computational modeling;Accelerator architectures;Random access memory;Bandwidth;Logic gates;Throughput","","6","","21","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Hardware-Algorithm Co-Design Enabling Efficient Event-based Object Detection","B. Crafton; A. Paredes; E. Gebhardt; A. Raychowdhury","School of ECE, Georgia Institute of Technology, Atlanta, GA; School of ECE, Georgia Institute of Technology, Atlanta, GA; NA; School of ECE, Georgia Institute of Technology, Atlanta, GA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Event-based cameras are a promising alternative to traditional optical cameras for real time computer vision systems. They offer low power, high temporal resolution, and high dynamic range, making them an ideal candidate for resource-constrained edge computing. In this work, we evaluate system level designs using event-based cameras for computer vision on the edge. We present a new technique to improve the object detection performance of event-based cameras, and demonstrate a 1.88 x to 2.17 x improvement in energy efficiency.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458497","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458497","","Computer vision;Image edge detection;Object detection;Vision sensors;Streaming media;Cameras;Optical imaging","","1","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Event-Driven Continuous-Time Feature Extraction for Ultra Low-Power Audio Keyword Spotting","S. Mourrane; B. Larras; A. Cathelin; A. Frappé","STMicroelectronics, Crolles, France; Univ. Lille, CNRS, Centrale Lille, Junia, Univ. Polytechnique Hauts-de-France, UMR 8520 - IEMN, Lille; STMicroelectronics, Crolles, France; Univ. Lille, CNRS, Centrale Lille, Junia, Univ. Polytechnique Hauts-de-France, UMR 8520 - IEMN, Lille",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","In the context of autonomous keyword spotting and sound detection, this paper proposes a low power feature extraction unit generating spectrograms that represent a unique signature allowing the classification of audio signals. This system is composed of a continuous-time digital signal processing feature extractor combined with a convolutional neural network engine. The study evaluates the hardware requirements to implement the feature extraction unit using an advanced CMOS process. Furthermore, a simulation of the complete system using Matlab® reveals that the recognition accuracy remains higher than 90% while offering a power consumption 4000X lower than a conventional discrete time system.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458425","","Discrete-time systems;Semiconductor device modeling;Power demand;Target recognition;Speech recognition;Feature extraction;Hardware","","5","","12","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"A Bio-Inspired Motion Detection Circuit Model for the Computation of Optical Flow: The Spatial-Temporal Filtering Reichardt Model","H. -Y. Wu; W. -T. Kao; H. H. -Y. Ku; C. -T. Wang; C. -C. Hsieh; R. -S. Liu; K. -T. Tang; C. -C. Lo","Department of Physics, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Institute of Systems Neuroscience, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Institute of Systems Neuroscience, National Tsing Hua University, Hsinchu, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Optical flow is the pattern of apparent motion in a visual scene produced by the relative movement between objects and an observer. Optical flow is used in many engineering applications such as optical odometry. A variety of optical-flow algorithms has been proposed in the past few decades; however, most of these algorithms involve complex computation, making them difficult to be implemented in neuromorphic systems that operate based on neural networks. Interestingly, studies have shown that insect visual systems are able to perform complex optical flow algorithms. Inspired by the classic Reichardt motion detection model proposed for insects, we designed a spatial-temporal filtering Reichardt (STR) model. This model computes optical flow based on simple filters in the spatial and temporal domains. The STR model is hardware friendly: it does not require time-consuming iteration processes nor computationally intensive multi-layer convolutional networks, which are typical in other optical flow algorithms. We systematically investigate the performance of the STR model with different parameters including: object size, speed, luminance, and filter forms. We also compare the performance of the STR model to the classical Farneback algorithm, and we demonstrate that the STR model is comparable to the classical algorithms while requiring much less computational power.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458450","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458450","Optical flow;Drosophila;motion detection;bio-inspiration","Optical filters;Biomedical optical imaging;Computational modeling;Biological system modeling;Insects;Optical computing;Filtering algorithms","","1","","13","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Integer Quadratic Integrate-and-Fire (IQIF): A Neuron Model for Digital Neuromorphic Systems","W. -C. Wu; C. -F. Yeh; A. J. White; C. -T. Wang; Z. -W. Yeh; C. -C. Hsieh; R. -S. Liu; K. -T. Tang; C. -C. Lo","Department of Physics, National Tsing Hua University, Hsinchu, Taiwan; Institute of Systems Neuroscience, National Tsing Hua University, Hsinchu, Taiwan; Institute of Systems Neuroscience, National Tsing Hua University, Hsinchu, Taiwan; Institute of Systems Neuroscience, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Institute of Systems Neuroscience, National Tsing Hua University, Hsinchu, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Simulation of a spiking neural network involves solving a large number of differential equations. This is a challenge even for modern computer systems, especially when simulating large-scale neural networks. To address this challenge, we design a neuron model: the Integer Quadratic Integrate-and-Fire (IQIF) neuron. Instead of computing on floating point numbers, as is typical with other spiking neuron models, the IQIF model is computed purely on integers. The IQIF model is a quantized and linearized version of the classic quadratic integrate-and-fire (QIF) model. The IQIF model retains all dynamic characteristics of the QIF model with much lower computation complexity, at the cost of a limited dynamic range of the membrane potential and the synaptic current. We compare IQIF to other spiking neuron models based on their simulation speeds and the number of neuronal behaviors they can perform. We further compare the performance of IQIF with the leaky integrate-and-fire model in a classical decision-making network that exhibits nonlinear attractor dynamics. Our results show that the IQIF neurons are capable of performing computation that other spiking neuron models can do while having the advantages of speed. Moreover, the IQIF model is digital hardware friendly due to its pure integer operation and is therefore easily to be implemented in custom-built neuromorphic systems.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458572","SNN;Neuron model;integer;styling","Neuromorphics;Computational modeling;Neurons;Membrane potentials;Differential equations;Dynamic range;Hardware","","","","25","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Federated Regularization Learning: an Accurate and Safe Method for Federated Learning","T. Su; M. Wang; Z. Wang","School of Electronic Science and Engineering, Nanjing University, P. R. China; School of Electronic Science and Engineering, Nanjing University, P. R. China; School of Electronic Science and Engineering, Nanjing University, P. R. China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Distributed machine learning (ML) and other related techniques such as federated learning are facing a high risk of information leakage. Differential privacy (DP) is commonly used to protect privacy. However, it suffers from low accuracy due to the unbalanced data distribution in federated learning and additional noise brought by DP itself. In this paper, we propose a novel federated learning model that can protect data privacy from the gradient leakage attack and black-box membership inference attack (MIA). The proposed protection scheme makes the data hard to be reproduced and be distinguished from predictions. A small simulated attacker network is embedded as a regularization punishment to defend the malicious attacks. We further introduce a gradient modification method to secure the weight information and remedy the additional accuracy loss. The proposed privacy protection scheme is evaluated on MNIST and CIFAR-10, and compared with state-of-the-art DP-based federated learning models. Experimental results demonstrate that our model can successfully defend diverse external attacks to user-level privacy with negligible accuracy loss.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458510","Federated learning;information leakage","Privacy;Differential privacy;Circuits and systems;Conferences;Machine learning;Collaborative work;Data models","","9","","15","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"A Novel Multi-scale Dilated 3D CNN for Epileptic Seizure Prediction","Z. Wang; J. Yang; M. Sawan","Cutting-Edge Net of Biomedical Research and INnovation (CenBRAIN), School of Engineering, Westlake University, Hangzhou, China; Cutting-Edge Net of Biomedical Research and INnovation (CenBRAIN), School of Engineering, Westlake University, Hangzhou, China; Cutting-Edge Net of Biomedical Research and INnovation (CenBRAIN), School of Engineering, Westlake University, Hangzhou, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Accurate prediction of epileptic seizures allows patients to take preventive measures in advance to avoid possible injuries. In this work, a novel convolutional neural network (CNN) is proposed to analyze time, frequency, and channel information of electroencephalography (EEG) signals. The model uses three-dimensional (3D) kernels to facilitate the feature extraction over the three dimensions. The application of multi-scale dilated convolution enables the 3D kernel to have more flexible receptive fields. The proposed CNN model is evaluated with the CHB-MIT EEG database, the experimental results indicate that our model outperforms the existing state-of-the-art, achieves 80.5% accuracy, 85.8% sensitivity and 75.1% specificity.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458571","Artificial intelligence;Deep learning;Epilepsy;Seizures prediction;CNN;3D convolution","Time-frequency analysis;Solid modeling;Three-dimensional displays;Convolution;Predictive models;Brain modeling;Feature extraction","","16","","27","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Online Detection of Vibration Anomalies Using Balanced Spiking Neural Networks","N. Dennler; G. Haessig; M. Cartiglia; G. Indiveri","Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland; Department of Computer Science, KTH Royal Institute of Technology Stockholm, Sweden; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Vibration patterns yield valuable information about the health state of a running machine, which is commonly exploited in predictive maintenance tasks for large industrial systems. However, the overhead, in terms of size, complexity and power budget, required by classical methods to exploit this information is often prohibitive for smaller-scale applications such as autonomous cars, drones or robotics. Here we propose a neuromorphic approach to perform vibration analysis using spiking neural networks that can be applied to a wide range of scenarios. We present a spike-based end-to-end pipeline able to detect system anomalies from vibration data, using building blocks that are compatible with analog-digital neuromorphic circuits. This pipeline operates in an online unsupervised fashion, and relies on a cochlea model, on feedback adaptation and on a balanced spiking neural network. We show that the proposed method achieves state-of-the-art performance or better against two publicly available data sets. Further, we demonstrate a working proof-of-concept implemented on an asynchronous neuromorphic processor device. This work represents a significant step towards the design and implementation of autonomous lowpower edge-computing devices for online vibration monitoring.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458403","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458403","predictive maintenance;spiking neural networks;cochlea;E-I balance;neuromorphic processor","Vibrations;Performance evaluation;Neuromorphics;Service robots;Image edge detection;Neural networks;Pipelines","","13","","20","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Self-Aware Anomaly-Detection for Epilepsy Monitoring on Low-Power Wearable Electrocardiographic Devices","F. Forooghifar; A. Aminifar; T. Teijeiro; A. Aminifar; J. Jeppesen; S. Beniczky; D. Atienza","Embedded Systems Laboratory, EPFL, Switzerland; Western Norway University of Applied Sciences, Norway; Embedded Systems Laboratory, EPFL, Switzerland; Lund University, Sweden; Aarhus University Hospital, Aarhus, Denmark; Aarhus University Hospital, Aarhus, Denmark; Embedded Systems Laboratory, EPFL, Switzerland",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Low-power wearable technologies offer a promising solution to pervasive epilepsy monitoring by removing the constraints concerning time and location, on one hand, and fulfilling long-term tracking, on the other hand. In the case of epileptic seizures, as the attacks infrequently occur, using an anomaly detection approach reduces the need to record long hours of data for each patient before detecting the successive coming seizures. In this work, by combining the concepts of self-aware system and anomaly detection, we propose an energy-efficient system to detect epileptic seizures on single-lead electrocardiographic signals, which is personalized after analyzing the first seizure of the patient. This system, then, uses a simple anomaly-detection model, whenever the model is deemed reliable, and uses a more complex model otherwise. We show that after the personalization, the number of patients, for which the method provides high sensitivity, can reach 26 out of 43 patients with the false alarm rate (FAR) of 4 alarms/day. Thus, the number of responders to the system is increased by 24%, while the FAR is only increased by one alarm/day, compared to the system that just uses the simple model. This benefit occurs while the system complexity decreases by 27.7% compared to the complex model. After adding the two-level (simple and complex) anomaly-detection, the complexity is tuned between 72.3% and 37.6% of the complex model. Similarly, the sensitivity is tuned between 66.5% and 60.3%.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458555","National Science Foundation; Fondation Botnar; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458555","self-awareness;anomaly detection;low-power;wearable devices;epileptic seizures","Sensitivity;Computational modeling;Biological system modeling;Epilepsy;Time factors;Reliability;Biomedical monitoring","","7","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Two-phase Scheme for Trimming QTMT CU Partition using Multi-branch Convolutional Neural Networks","P. -C. Fu; C. -C. Yen; N. -C. Yang; J. -S. Wang","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, University of California, Davis, CA, USA; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","6","Versatile Video Coding (VVC) initialized in October 2017, will provide the same subjective quality at roughly 50% the bitrate of its predecessor HEVC. VVC introduced a complex structure of quad-tree plus multi-type tree block partitioning (QT + MTT, or QTMT) in each 128 x 128 block. However, it brings more encoding co mplexity. In this work, to tackle this problem effectively, a two-phase scheme for trimming QTMT CU partition using multi-branch CNN is presented. The goal is to predict the (QTMT) depth of QTMT partitioning on the basis of each block of size 32 x 32. In the first phase, a backbone CNN followed by three parallel branches extracts latent features to predict which QT depth and whether using ternary-tree (TT) or not. In the second phase, based on the above prediction information, a huge number of possible (distinct) combinations of QTMT CU partition can be trimmed to reduce computational co mplexity. However, the practice of multiple branches leads to a significant increase in the amount of neural parameters in the CNN and consequently, the total computations of both training and inferencing will be raised significantly. Therefore, effective deep learning modules in MobilenetV2 are applied to downgrade the amount of parameters to an adequate level eventually. The experimental results show that the proposed method achieves 42.341% average saving of encoding time for all VVC test sequences and with 0.71 Bjntegaard-Delta bit-rate (BD-BR) increasing compared with VTM 6.1 in All-intra (AI) configuration.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458479","Versatile Video Coding (VVC);Deep learning;Convolutional neural networks;QTMT CU partition","Training;Deep learning;Conferences;Predictive models;Feature extraction;Encoding;Convolutional neural networks","","9","","17","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Exploiting Memristors for Neuromorphic Reinforcement Learning","C. Shi; J. Lu; Y. Wang; P. Li; M. Tian","School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Memristors have been proposed to build neural networks for their nanoscale size, low power consumption and high density. They are particularly suited to act as synaptic weights between neurons. In this paper, a novel synapse circuit is proposed to enable memristors for on-chip spiking neural network (SNN) reinforcement learning (RL). The proposed synapse circuit consists of 1 memristor and 4 transistors (1M4T) performing reward-modulated spike-timing dependent plasticity (R-STDP). As a type of RL, the R-STDP rule utilizes training sample labels to generate reward/punishment signals to guide weight updates for higher object recognition accuracies. A prototype hardware SNN is constructed with our 1M4T synapse circuit, and was simulated to successfully completes an image pattern recognition task after the memristor-based RSTDP learning. This demonstrates that our 1M4T synapse circuit can realize on-chip neuromorphic RL, exhibiting great potentials for low-cost energy-efficient system applications.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458542","Chongqing Science and Technology Foundation; Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458542","Memristor;Reinforcement learning;STDP;Reward-modulated STDP;Spiking neural network;Neuromorphic systems","Neuromorphics;Memristors;Prototypes;Reinforcement learning;Hardware;System-on-chip;Transistors","","4","","21","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Software/Hardware Co-design for Multi-modal Multi-task Learning in Autonomous Systems","C. Hao; D. Chen",Georgia Institute of Technology; University of Illinois at Urbana-Champaign,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","5","Optimizing the quality of result (QoR) and the quality of service (QoS) of AI-empowered autonomous systems simultaneously is very challenging. First, there are multiple input sources, e.g., multimodal data from different sensors, requiring diverse data preprocessing, sensor fusion, and feature aggregation. Second, there are multiple tasks that require various AI models to run simultaneously, e.g., perception, localization, and control. Third, the computing and control system is heterogeneous, composed of hardware components with varied features, such as embedded CPUs, GPUs, FPGAs, and dedicated accelerators. Therefore, autonomous systems essentially require multi-modal multitask (MMMT) learning which must be aware of hardware performance and implementation strategies. While MMMT learning has been attracting intensive research interests, its applications in autonomous systems are still underexplored. In this paper, we first discuss the opportunities of applying MMMT techniques in autonomous systems, and then discuss the unique challenges that must be solved. In addition, we discuss the necessity and opportunities of MMMT model and hardware co-design, which is critical for autonomous systems especially with power/resource-limited or heterogeneous platforms. We formulate the MMMT model and heterogeneous hardware implementation co-design as a differentiable optimization problem, with the objective of improving the solution quality and reducing the overall power consumption and critical path latency. We advocate for further explorations of MMMT in autonomous systems and software/hardware co-design solutions.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458577","","Location awareness;Power demand;Autonomous systems;Quality of service;Learning (artificial intelligence);Sensor fusion;Hardware","","20","","63","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Multiple-Precision Floating-Point Dot Product Unit for Efficient Convolution Computation","K. Li; W. Mao; X. Xie; Q. Cheng; H. Xie; Z. Dong; H. Yu","School of Microelectronics, Southern University of Science and Technology, Shenzhen, China; School of Microelectronics, Southern University of Science and Technology, Shenzhen, China; School of Microelectronics, Southern University of Science and Technology, Shenzhen, China; School of Microelectronics, Southern University of Science and Technology, Shenzhen, China; Huawei Technologies Co., Ltd.; Huawei Technologies Co., Ltd.; School of Microelectronics, Southern University of Science and Technology, Shenzhen, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","In the computational process of the convolutional neural network (CNN) and high-performance computing, the convolution operation dominates the hardware costs and performance of the overall system. To improve the hardware efficiency, the dot-product units (DPU) have been used for convolution computation. Besides, multiple-precision floating-point (FP) support is essential for the accuracy requirement of various applications. In this work, multiple-precision FP many-term DPU is designed with single instruction multiple data (SIMD) structure. The proposed design also supports multiple-precision operations with configurable multiplier and alignment shifter. FP16 twenty-term, FP32 five-term or FP64 one-term dot product operations can be executed in two successive clock cycles without idle multiplication resources. To speed up the summation process, a carry-select adder (CSLA) is designed with excellent area-delay product (ADP) and power-delay product (PDP) performance. The proposed design is realized in UMC 55-nm process with experiment results. Compared with the state-of-the-art multiple-precision work, the proposed design achieves maximum 3.76 times power performance improvement for FP16 operations. Compared with previous CSLA designs, the proposed work improves ADP and PDP performance by 4.7% and 3.91%, respectively.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458534","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458534","multiple-precision;floating-point;radix-4 booth;multiplier;CSLA;dot product","Convolution;Circuits and systems;Conferences;Hardware;Computational efficiency;Convolutional neural networks;Artificial intelligence","","1","","10","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"EILE: Efficient Incremental Learning on the Edge","X. Chen; C. Gao; T. Delbruck; S. -C. Liu","Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland; Institute of Neuroinformatics, University of Zurich and ETH Zurich, Zurich, Switzerland",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","This paper proposes a fully-connected network training architecture called EILE targeting incremental learning on edge. By using a novel reconfigurable processing element (PE) architecture, EILE avoids explicit transposition of weight matrices required for backpropagation to preserve the same efficient memory access pattern for both the forward (FP) and backward propagation (BP) phases. Experimental results on a Zynq XC7Z100 FPGA with 64 PEs show that EILE achieves 19.2 GOp/s peak throughput and maintains nearly 100 % PE utilization efficiency for both FP and BP with batch sizes from 1 to 32. EILE's small on-chip memory footprint and scalability to match any available off-chip memory bandwidth makes it an attractive ASIC architecture for energy-constrained training.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458554","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458554","deep neural network;hardware accelerator;on-chip training;incremental learning;edge computing;FPGA","Training;Backpropagation;Random access memory;Bandwidth;Speech recognition;Throughput;System-on-chip","","5","","15","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"An 8.62 μ W Processor for Autism Spectrum Disorder Classification using Shallow Neural Network","A. R. Aslam; N. Hafeez; H. Heidari; M. A. B. Altaf","Lahore University of Management Sciences (LUMS), Lahore, Pakistan; Brunel University, London, United Kingdom; University of Glasgow, Scotland, United Kingdom; Lahore University of Management Sciences (LUMS), Lahore, Pakistan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Autism Spectrum Disorder (ASD) is the prevalent child neurological and developmental disorder causing cognitive and behavioral impairments. The early diagnosis is an urgent need for treatment and rehabilitation of ASD patients. This work presents an electroencephalogram (EEG) based ASD classification processor that targets a patch-form factor sensor that can be used for long time monitoring in a wearable environment. The selection of frontal and parietal lobe electrodes causes minimum uneasiness to the children. The proposed and implemented algorithm utilizes only four EEG electrodes. The processor is implemented and validated on Artix-7 FPGA which requires only 26K lookup tables and 15K flip flops. The hardware efficient implementation of the complex kurtosis value and Katz fractal dimension (KFD) features using kurtosis value indicator and KFD indicator with 54% and 38% efficient implementations, respectively, is provided. A hardware feasible shallow neural network architecture is used for the ASD classification. The system classifies the ASD with a high classification accuracy of 85.5% using the power and latency of $8.62 \mu \mathrm{W}$ and 2.25ms, respectively.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458412","Autism;neural network;neurological disorder;processor;wearable devices","Electrodes;Pediatrics;Autism;Medical services;Hardware;Electroencephalography;Parietal lobe","","10","","19","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Contention-aware Adaptive Model Selection for Machine Vision in Embedded Systems","B. Kutukcu; S. Baidya; A. Raghunathan; S. Dey","Department of Electrical and Computer Engineering, University of California, San Diego; Department of Electrical and Computer Engineering, University of California, San Diego; Electrical and Computer Engineering, Purdue University; Department of Electrical and Computer Engineering, University of California, San Diego",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Real-time machine vision applications running on resource-constrained embedded systems face challenges for maintaining performance. An especially challenging scenario arises when multiple applications execute at the same time, creating contention for the computational resources of the system. This contention results in increase in inference delay of the machine vision applications which can be unacceptable for time-critical tasks. To address this challenge, we propose an adaptive model selection framework to mitigate the impact of system contention and prevent unexpected increase in inference delay by trading off the application accuracy minimally. The framework uses a set of hierarchical deep learning models for image classification. It predicts the inference delays of each model and selects the optimal model for each frame considering the system contention. Compared to a fixed individual model with similar accuracy, our framework improves the performance by significantly reducing the inference delay violations against a practical threshold. We implement our framework on Nvidia Jetson TX2 and show that our approach achieves a gain over the individual model by 27.6% reductions in delay violations.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458468","","Adaptation models;Adaptive systems;Embedded systems;Machine vision;Predictive models;Delays;Time factors","","2","","9","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"A Two-Layer LSTM Deep Learning Model for Epileptic Seizure Prediction","S. M. Varnosfaderani; R. Rahman; N. J. Sarhan; L. Kuhlmann; E. Asano; A. Luat; M. Alhawari","Department of Electrical and Computer Engineering, Wayne State University, Detroit, USA; Department of Computer Science, Wayne State University, Detroit, USA; Department of Electrical and Computer Engineering, Wayne State University, Detroit, USA; Department of Data Science and AI, Faculty of Information Technology, Monash University, Clayton, Australia; Department of Pediatrics and Neurology, Wayne State University, Detroit, USA; Department of Pediatrics and Neurology, Wayne State University, Detroit, USA; Department of Electrical and Computer Engineering, Wayne State University, Detroit, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","We propose an efficient seizure prediction model based on a two-layer LSTM using the Swish activation function. The proposed structure performs feature extraction based on the time and frequency domains and uses the minimum distance algorithm as a post-processing step. The proposed model is evaluated on the Melbourne dataset and achieves the highest Area Under Curve (AUC) score of 0.92 and the lowest False Positive Rate (FPR) of 0.147 compared to previous work while having sensitivity and accuracy of 86.8 and 85.1, respectively. The proposed system has a low number of trainable parameters, and thus reducing the complexity of resource-constrained applications.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458539","Deep learning;classification;epilepsy seizure prediction;feature extraction;iEEG;LSTM;Melbourne dataset","Technological innovation;Sensitivity;Frequency-domain analysis;Scattering;Epilepsy;Predictive models;Prediction algorithms","","13","","26","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Ensemble of Pruned Networks for Reliable Classifiers","Z. Gao; H. Zhang; X. Wei; J. Xiao; S. Zeng; G. Ge; Y. Wang; P. Reviriego","Tianjin University, Tianjin, China; Tianjin University, Tianjin, China; Tianjin University, Tianjin, China; Tianjin University, Tianjin, China; School of Electronic Engineering, Tsinghua University, Beijing, China; School of Electronic Engineering, Tsinghua University, Beijing, China; School of Electronic Engineering, Tsinghua University, Beijing, China; Departamento Ingeniería Telemática, Universidad Carlos III de Madrid, Madrid, Spain",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Convolutional Neural Networks (CNNs) have been widely used for image recognition or natural language processing. When CNNs are used in safety-critical applications, their reliability becomes a priority and in particular their tolerance to soft errors. Unfortunately, traditional fault tolerant techniques based on modular redundancy introduce a large overhead, which is not acceptable in many resource-limited embedded systems given the complexity of CNNs. To reduce the cost of protecting CNNs, the use of an ensemble of smaller CNNs has been recently proposed. The idea is that the ensemble cannot only improve the classification accuracy but also protect against errors. To reduce the cost, pruning the CNNs is widely used. In this paper, the combination of both ideas is explored, namely ensembles of CNNs that are built using different pruned versions of the same CNN are analyzed. To evaluate the effectiveness of the idea, VGG16 is used as a case study. Small networks with different sizes are generated by pruning the original VGG16, and a combiner was designed with the ability to detect a faulty base learner. The reliability of different ensembles of the pruned networks is evaluated in PyTorch on Cifar-10. The reliability evaluation results reveal that the ensemble of pruned networks could achieve higher reliability than the Triple Modular Redundancy (TMR) protected VGG16 networks with an overhead lower than double modular redundancy. Complementarity analysis between different pruned VGG nets has been performed to explain the reliability improvement.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458545","Convolutional Neural Networks (CNNs);Reliability;Ensemble;VGG16;Pruning","Image recognition;Redundancy;Fault tolerant systems;Reliability theory;Reliability engineering;Natural language processing;Reliability","","3","","23","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Tile-Based Architecture Exploration for Convolutional Accelerators in Deep Neural Networks","Y. -T. Chen; Y. -X. Yen; C. -T. Chen; T. -Y. Chen; C. -T. Huang; J. -J. Liou; J. -M. Lu","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Electrical Engineering, National Tsing Hua University, Hsinchu, Taiwan",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","This paper presents the tile-based DCNN accelerator architecture. The hierarchical interconnection networks enable distributed data delivery to maximize the data bandwidth both for the conventional and depthwise convolution layers. An exploration tool is also developed to optimize architectural parameters under the trade-off among specific performance, power, and area. The case study shows that our accelerator can outperform the state-of-the-art by up to 36% faster and up to 25% lower energy on different modern DCNNs. The experiment also justifies the scalability of our approach.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458540","","Multiprocessor interconnection;Convolution;Circuits and systems;Scalability;Conferences;Neural networks;Distributed databases","","2","","13","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"MNSIM-TIME: Performance Modeling Framework for Training-In-Memory Architectures","K. Qiu; Z. Zhu; Y. Cai; H. Sun; Y. Wang; H. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Emerging memristors and Processing-In-Memory (PIM) architectures have shown powerful capabilities in improving the computing energy efficiency of neural network (NN) algorithms. Existing work has proposed the memristor-based NN training architecture [2], which can improve more than 10x energy efficiency improvement compared with CMOS-based solutions. In this paper, we propose a behavior-level modeling framework for memristor-based training-in-memory architectures, called MNSIM-TIME. Compared with existing modeling tools, MNSIMTIME supports configurable architecture design and fast hardware performance modeling, which helps researchers to realize efficient design space exploration in the early architecture design stage.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458441","National Natural Science Foundation of China; Beijing Innovation Center for Future Chip; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458441","","Training;Computational modeling;Memristors;Computer architecture;Artificial neural networks;Tools;Energy efficiency","","4","","15","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Improving system latency of AI accelerator with on-chip pipelined activation preprocessing and multi-mode batch inference","W. Chen; Z. Wang; M. Lei; B. Dong; Z. Wang; Y. Yang; C. Chen; W. Guo; C. Liang; Q. Zhang; W. Fang; Z. Yu","Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Shenzhen, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","State-of-the-art neural network accelerators exploit massive computing parallelism to achieve high throughput. However, significant latency is observed on the master-slave-based AI acceleration system which limits its adaptation in real-time applications. Investigation in de-facto GPU system reveals tremendous timing overhead for preprocessing of input activations, which is commonly executed on the host machine through manually customized low-level APIs, nevertheless still results in the moderate utilization rate of processing elements. In this work, we observe one of the key factors for low utilization of processing elements (PEs) is the sparsity of input activations caused by the default DRAM bandwidth utilization (BU) of only 18.75%. To optimize this, we design two on-chip preprocessing modules that merge up-to-five input frames in one DRAM transfer to increase BU to 93.75% and facilitates parallel img2col buffering. Furthermore, the merged activations are simultaneously processed by the neuron engine to reduce the system latency and significantly increases PE utilization. Explicitly, multiple batch inferencing modes including Intra-PE-, temporal-and spatial-sharing are proposed to pipeline with the preprocessing modules, resulting in a 65% ∼ 75% reduction of system latency. The on-chip preprocessing modules incur the physical overheads of 31.0% in area and 28.7% in power consumption under 40nm SMIC standard cell library.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458529","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458529","Machine learning accelerator;latency improvement;multi-mode inference","Power demand;Pipelines;Data preprocessing;Random access memory;Prototypes;Throughput;Real-time systems","","2","","12","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Dynamically-biased Fixed-point LSTM for Time Series Processing in AIoT Edge Device","J. Hu; W. L. Goh; Y. Gao","IC Design Department, Agency for Science, Technology and Research, Institute of Microelectronics, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; IC Design Department, Agency for Science, Technology and Research, Institute of Microelectronics, Singapore",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","In this paper, a Dynamically-Biased Long Short-Term Memory (DB-LSTM) neural network architecture is proposed for artificial intelligence internet of things (AIoT) applications. Different from the conventional LSTM which uses static bias, DB-LSTM adjusts the cell bias dynamically based on the previous status. Hence, a DB-LSTM cell contains information of both the previous output and the current cell state. With more information, the DB-LSTM is able to achieve faster training convergence and better accuracy. Furthermore, weight quantization is performed to reduce the weights to either 1-bit or 2-bit, so that the algorithm can be implemented in portable edge device. With the same 100 epochs training setup, more than 70% loss reduction are achieved for floating 32-bit, 1-bit and 2-bit weights, respectively. The loss degradation due to weight quantization is also negligible. The performance of the proposed model is also validated with the classical air passenger forecasting problem. 0.075 loss and 94.96% accuracy are achieved with 2-bit weight when compared to the ground truth, which is comparable to full-length 32-bit weight.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458508","LSTM;Recurrent Neural Network;fixed-point weight;time series forecasting","Training;Accuracy;Microprocessors;Time series analysis;Computer architecture;Predictive models;Internet of Things;Artificial intelligence;Forecasting;Long short term memory","","2","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Performance of Crossbar based Long Short Term Memory with Aging Memristors","A. Aswani; R. Kumar; J. N. Tripathi; A. James","NeuroAGI Centre, IIITMK, Trivandrum, India; Department of Electrical Engineering, Indian Institute of Technology Jodhpur, Jodhpur, Rajasthan, India; Department of Electrical Engineering, Indian Institute of Technology Jodhpur, Jodhpur, Rajasthan, India; Innovation and Technology, Kerala University of Digital Sciences, Trivandrum, India",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","The Long Short Term Memory (LSTM) neural networks find a wide range of applications in time series prediction problems. The long-term accuracy and reliability of LSTM memristor crossbar array are subjected to the memristor device’s endurance and failures. Memristor aging and its impact on such LSTM’s performance is an open problem. This paper analyzes the effects of different types of aging typically exhibited in memristor devices on the crossbar performance. The performance results are analyzed on two datasets, (1) SMS Spam and (2) IMDB movie review. Our analysis indicated that the different aging type shows different performance deterioration levels in the crossbar based LSTM system. Here, the aging analysis for oxide-based memristor implementation are primarily considered when used in CMOS-Memristor hybrid crossbars.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458402","Memristor aging;crossbar;LSTM;neural networks;CMOS-memristor","Performance evaluation;System performance;Neural networks;Time series analysis;Memristors;Aging;Reliability engineering","","","","15","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Quantized Fully Convolution Neural Network for HW Implementation of Human Posture Recognition","A. Russo; G. D. Licciardo; L. D. Benedetto; A. Rubino; R. Liguori; A. Naddeo; N. Cappetti","Department of Industrial Engineering, University of Salerno, Fisciano, (SA), Italy; Department of Industrial Engineering, University of Salerno, Fisciano, (SA), Italy; Department of Industrial Engineering, University of Salerno, Fisciano, (SA), Italy; Department of Industrial Engineering, University of Salerno, Fisciano, (SA), Italy; Department of Industrial Engineering, University of Salerno, Fisciano, (SA), Italy; Department of Industrial Engineering, University of Salerno, Fisciano, (SA), Italy; Department of Industrial Engineering, University of Salerno, Fisciano, (SA), Italy",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","In this paper, a very tiny HW design of a Quantized Fully Convolutional Neural Network is proposed which demonstrates that accurate Human Posture Recognition can be designed by exploiting only pressure sensors and keeping the computation close to the data sources, according to the edge computing paradigm. The custom design of the QFCN exploits a base-2 quantization scheme to achieve state-of-the-art performances in terms of classification accuracy, together with a very reduced number of mapped physical resources and power consumption. Trained and validated on a public dataset for in-bed posture classification, the QFCN exhibits an accuracy up to 96.77% in recognizing 17 different postures. When prototyped on a Xilinx Artix 7 FPGA the design achieves less than 7 mW dynamic power dissipation and a maximum operation frequency of 26.6 MHz, compatible with an Output Data Rate (ODR) of the sensors of 9.13 kHz.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458487","Convolutional neural network;Quantization scheme;low-power digital design;FPGA custom design","Pressure sensors;Quantization (signal);Power demand;Convolution;Conferences;Neural networks;Power dissipation","","3","","21","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Exploiting Weight Statistics for Compressed Neural Network Implementation on Hardware","P. Kashikar; S. Sinha; A. K. Verma","School of Mathematics and Computer Science, Indian Institute of Technology Goa (IIT Goa), India; School of Mathematics and Computer Science, Indian Institute of Technology Goa (IIT Goa), India; School of Mathematics and Computer Science, Indian Institute of Technology Goa (IIT Goa), India",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Computing hardware like field programmable gate arrays (FPGAs), microcontrollers and microprocessors can have limited compute and on-chip storage resources. This is especially true for computing hardware in Internet of Things (IoT) and low end embedded systems. With the growth in machine and deep learning, it is imperative to build intelligence in these devices. Therefore, this paper proposes exploiting weight statistics to compress floating point based weights in neural networks without any loss in accuracy. The proposed method has been implemented as an optimization pass in open source N2D2 framework. The proposed method thus does not make an assumption that the application can tolerate some accuracy loss which is the case with other methods like quantization, binary weights etc. However, it can also be considered as a further step in optimization after applying existing quantization based methods. The proposed method is able to save nearly 10% on-chip storage requirement, thus reducing the number of Block RAMs (BRAM) in case of FPGAs and the size of on-chip memory (OCM) in case of microcontrollers and microprocessors. We show that layer wise compression gives slightly better compression than global compression. This compression is traded off for execution time overhead in microcontrollers.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458581","","Quantization (signal);Microcontrollers;Neural networks;Memory management;Random access memory;Hardware;System-on-chip","","1","","21","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"End-to-end 100-TOPS/W Inference With Analog In-Memory Computing: Are We There Yet?","G. Ottavi; G. Karunaratne; F. Conti; I. Boybat; L. Benini; D. Rossi","DEI, University of Bologna, Italy; IBM Research Europe; DEI, University of Bologna, Italy; IBM Research Europe; IIS lab, ETH Zurich, Switzerland; DEI, University of Bologna, Italy",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","In-Memory Acceleration (IMA) promises major efficiency improvements in deep neural network (DNN) inference, but challenges remain in the integration of IMA within a digital system. We propose a heterogeneous architecture coupling 8 RISC-V cores with an IMA in a shared-memory cluster, analyzing the benefits and trade-offs of in-memory computing on the realistic use case of a MobileNetV2 bottleneck layer. We explore several IMA integration strategies, analyzing performance, area, and energy efficiency. We show that while pointwise layers achieve significant speed-ups over software implementation, on depthwise layer the inability to efficiently map parameters on the accelerator leads to a significant trade-off between throughput and area. We propose a hybrid solution where pointwise convolutions are executed on IMA while depthwise on the cluster cores, achieving a speed-up of 3x over SW execution while saving 50% of area when compared to an all-in IMA solution with similar performance.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458409","In-memory computing;RISC-V;MobileNetV2","Couplings;Digital systems;Circuits and systems;Conferences;Neural networks;Computer architecture;Throughput","","4","","11","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"An Ultra-Low Latency Multicast Router for Large-Scale Multi-Chip Neuromorphic Processing","C. Ding; Y. Huan; H. Jia; Y. Yan; F. Yang; Z. Zou; L. -R. Zheng","School of Information Science and Technology, Fudan University, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China; School of Information Science and Technology, Fudan University, Shanghai, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Neuromorphic simulation is fundamental to the study of information processing mechanism of the human brain and can further inspire application development of event-driven spiking neural networks. However large-scale neuromorphic simulation requires massive parallelism on multi-chip processing and imposes great challenges on dealing with data transmission latency and congestion problems between chips, especially when the number of simulated neurons reaches to billions or even trillions level. In this paper, we propose an ultra-low-latency on-chip router together with a multicast routing algorithm that focuses on reducing global loads and balancing loads between links. Additionally, we build a large-scale neuromorphic simulation platform consisting of 64 FPGA chips and evaluate the proposed design on it. The experiment results suggest that this design benefits from the proposed multicast routing algorithm in global communication loads and simulation capacity. This work has 4.1% $\sim$ 5.2% reduction of global loads comparing to previous works and can achieve a latency as low as 25ns and a maximum data throughput of 6.25Gbps/chip.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458445","National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458445","","Multicast algorithms;Neuromorphics;Parallel processing;Routing;Throughput;Brain modeling;System-on-chip","","2","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"TempDiff: Temporal Difference-Based Feature Map-Level Sparsity Induction in CNNs with <4% Memory Overhead","U. D. Alwis; M. Alioto","ECE Dept., National University of Singapore, Singapore; ECE Dept., National University of Singapore, Singapore",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","The diffusion of vision sensor nodes in a wide range of applications has given rise to higher computational demand at the edge of the Internet of Things (IoT). Indeed, in-node video sense-making has become essential in the form of high-level tasks such as object detection for visual monitoring, mitigating data deluge from the wireless network to the cloud storage level. In such applications, deep neural networks are well known to be a prime choice, in view of their performance and flexibility. However, such properties come at the cost of high computational requirements at inference time, which directly hamper power efficiency, lifetime and cost of self-powered edge devices. In this paper, a computationally-efficient inference technique is introduced to perform the ubiquitously required task of bounding box-based object detection. The proposed method leverages the correlation among frames in the temporal dimension, uniquely requires minor memory overhead for intermediate feature map storage and architectural changes, and does not require any retraining for immediate deployment in existing vision frameworks. The proposed method achieves 18.3% (35.8%) computation reduction at 3.3% (3.2%) memory overhead, and 3.8% (6.8%) accuracy drop in YOLOv1(VGG16) SSD(VGG16) neural networks under the CAMEL dataset.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458463","Object detection;deep neural networks;computational efficiency;Internet of Things;inference","Performance evaluation;Visualization;Image edge detection;Wireless networks;Memory management;Neural networks;Object detection","","3","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"TCBNN: Error-Correctable Ternary-Coded Binarized Neural Network","C. -D. Tsai; T. -Y. Chen; H. -W. Fu; T. -C. Huang","Department of Electronics Engineering, National Changhua University of Education, Changhua, Taiwan ROC; Department of Electronics Engineering, National Changhua University of Education, Changhua, Taiwan ROC; Department of Electronics Engineering, National Changhua University of Education, Changhua, Taiwan ROC; Department of Electronics Engineering, National Changhua University of Education, Changhua, Taiwan ROC",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Acceleration and reliability are two critical issues of artificial intelligent circuits and systems. In this paper we propose a novel structure of deep neural network in testing stage that converts the trained weights to optimized ternary-coded binary. Within each shallow layer a constant-shift sub-layer with almostfree cost is inserted to transfer all multipliers to a carry save adder/subtractor. Then the activation functions are simplified to piecewise lines with nice slopes for calculation by an adder only. Since input-side layers tend to have self-healing ability, only output-side layers are equipped by AN codes that can be encoded and decoded almost without extra cost in our structure. From experiments and evaluations, our structure can have a higher resolution with error-correcting capability and a performance similar to the BNN.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458459","neural nenvork acceleration;fault-tolerant;error correcting codes;AN codes;ternary-coded binary;range-addressable look-up table","Circuits and systems;Conferences;Artificial neural networks;Reliability theory;Table lookup;Circuit faults;Integrated circuit reliability","","5","","17","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"ECG-TCN: Wearable Cardiac Arrhythmia Detection with a Temporal Convolutional Network","T. M. Ingolfsson; X. Wang; M. Hersche; A. Burrello; L. Cavigelli; L. Benini","ETH Zürich, D-ITET, Switzerland; D-ITET, ETH Zürich, Switzerland; D-ITET, ETH Zurich, Switzerland; DEI, University of Bologna, Italy; Huawei Technologies, Zurich RC, Switzerland; ETH Zürich, D-ITET, Switzerland",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Personalized ubiquitous healthcare solutions require energy-efficient wearable platforms that provide an accurate classification of bio-signals while consuming low average power for long-term battery-operated use. Single lead electrocardiogram (ECG) signals provide the ability to detect, classify, and even predict cardiac arrhythmia. In this paper we propose a novel temporal convolutional network (TCN) that achieves high accuracy while still being feasible for wearable platform use. Experimental results on the ECG5000 dataset show that the TCN has a similar accuracy (94.2%) score as the state-of-the-art (SoA) network while achieving an improvement of 16.5% in the balanced accuracy score. This accurate classification is done with 27x fewer parameters and 37x less multiply-accumulate operations. We test our implementation on two publicly available platforms, the STM32L475, which is based on ARM Cortex M4F, and the GreenWaves Technologies GAP8 on the GAPuino board, based on 1+8 RISC-V CV32E40P cores. Measurements show that the GAP8 implementation respects the real-time constraints while consuming 0.10mJ per inference. With 9.91GMAC/s/W, it is 23.0x more energy-efficient and 46.85x faster than an implementation on the ARM Cortex M4F (0.43GMAC/s/W). Overall, we obtain 8.1% higher accuracy while consuming 19.6x less energy and being 35.1x faster compared to a previous SoA embedded implementation.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458520","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458520","healthcare;time series classification;smart edge computing;machine learning;deep learning","Quantization (signal);Convolution;Microprocessors;Computer architecture;Medical services;Electrocardiography;Tools","","32","","19","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"iELAS: An ELAS-Based Energy-Efficient Accelerator for Real-Time Stereo Matching on FPGA Platform","T. Gao; Z. Wan; Y. Zhang; B. Yu; Y. Zhang; S. Liu; A. Raychowdhury",Beijing Institute of Technology; Georgia Institute of Technology; Beijing Institute of Technology; PerceptIn Inc.; Beijing Institute of Technology; PerceptIn Inc.; Georgia Institute of Technology,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Stereo matching is a critical task for robot navigation and autonomous vehicles, providing the depth estimation of surroundings. Among all stereo matching algorithms, Efficient Large-scale Stereo (ELAS) offers one of the best tradeoffs between efficiency and accuracy. However, due to the inherent iterative process and unpredictable memory access pattern, ELAS can only run at 1.5-3 fps on high-end CPUs and difficult to achieve real-time performance on low-power platforms. In this paper, we propose an energy-efficient architecture for real-time ELAS-based stereo matching on FPGA platform. Moreover, the original computational-intensive and irregular triangulation module is reformed in a regular manner with points interpolation, which is much more hardware-friendly. optimizations, including memory management, parallelism, and pipelining, are further utilized to reduce memory footprint and improve throughput. Compared with Intel i7 CPU and the state-of-the-art $\mathrm{C}\mathrm{P}\mathrm{U}+$FPGA implementation, our FPGA realization achieves up to $ 38.4\times$ and $ 3.32\times$ frame rate improvement, and up to $ 27.1\times$ and $ 1.13\times$ energy efficiency improvement, respectively.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458401","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458401","","Interpolation;Memory management;Throughput;Energy efficiency;Real-time systems;Central Processing Unit;Stereo vision","","11","","7","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient FPGA Implementation of Approximate Singular Value Decomposition based on Shallow Neural Networks","H. Younes; A. Ibrahim; M. Rizk; M. Valle","Department of Electrical, Electronic and Telecommunications Engineering and Naval Architecture, COSMIC Laboratory, University of Genova, Genoa, Italy; Department of Electrical, Electronic and Telecommunications Engineering and Naval Architecture, COSMIC Laboratory, University of Genova, Genoa, Italy; Department of Computer and Communication Engineering, Lebanese International University, Bekaa, Lebanon; Department of Electrical, Electronic and Telecommunications Engineering and Naval Architecture, COSMIC Laboratory, University of Genova, Genoa, Italy",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","This paper presents a novel architecture for the Singular Value Decomposition (SVD) algorithm. The architecture embraces the reductions offered by the use of Approximate Computing (AxC) as a trade-off between complexity and accuracy. A shallow Neural Network (NN) consisting of three layers is used to compute the SVD of an input matrix, offering a comparable Mean Squared Error (MSE) with exact computations. The NN is implemented using High-Level Synthesis (HLS) on a Virtex-7 FPGA device. When compared to an exact implementation of the SVD algorithm, the proposed architecture achieves a computational speedup between 5x and 19x with an average reduced hardware area of up to 80% with a noticeable 6x reduction in the DSP usage.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458453","SVD;Neural Networks;Approximate Computing;Embedded Systems;FPGA;High-Level Synthesis","Support vector machines;Simulation;Scalability;Approximate computing;Computer architecture;Artificial neural networks;Approximation algorithms","","2","","16","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Graph-Based Spatio-Temporal Backpropagation for Training Spiking Neural Networks","Y. Yan; H. Chu; X. Chen; Y. Jin; Y. Huan; L. Zheng; Z. Zou","School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; School of Information Science and Technology, State Key Laboratory of ASIC and System, Fudan University, Shanghai, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Dedicated hardware for spiking neural networks (SNN) reduces energy consumption with spike-driven computing. This paper proposes a graph-based spatio-temporal backpropagation (G-STBP) to train SNN, aiming to enhance spike sparsity for energy efficiency, while ensuring the accuracy. A differentiable leaky integrate-and-fire (LIF) model is suggested to establish the backpropagation path. The sparse regularization is proposed to reduce the spike firing rate with a guaranteed accuracy. GSTBP enables training in any network topologies thanks to graph representation. A recurrent network is demonstrated with spike-sparse rank order coding. The experimental result on rank order coded MNIST shows that the recurrent SNN trained by G-STBP achieves the accuracy of 97.3% using 392 spikes per inference.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458461","National Natural Science Foundation of China; Science and Technology Commission of Shanghai Municipality; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458461","spiking neural network (SNN);spike sparsity;graph-based spatio-temporal backpropagation (G-STBP);leaky integrate-and-fire (LIF);recurrent network","Backpropagation;Training;Energy consumption;Firing;Network topology;Neural networks;Encoding","","5","","10","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient FPGA Implementation of a Convolutional Neural Network for Radar Signal Processing","J. Zhang; Y. Huang; H. Yang; M. Martinez; G. Hickman; J. Krolik; H. Li","Electrical and Computer Engineering Department, Duke University, Durham, USA; Electrical and Computer Engineering Department, Duke University, Durham, USA; Electrical and Computer Engineering Department, Duke University, Durham, USA; Electrical and Computer Engineering Department, Duke University, Durham, USA; Electrical and Computer Engineering Department, Duke University, Durham, USA; Electrical and Computer Engineering Department, Duke University, Durham, USA; Electrical and Computer Engineering Department, Duke University, Durham, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Although neural networks, especially convolutional neural networks (CNNs), have been successfully applied to many domains, there have not found many radar applications mainly due to a paucity of available training data. Focusing on fixed-site radars, this work uses in-situ collected data to train a CNN classifier and suppress clutter components that allow targets to “hide in plain sight.” This paper describes a software and hardware co-design approach for implementing a neural network to improve radar signal processing. At the algorithm level, we propose using the ResNet10 model structure and other optimizations trained using the angle-Doppler spectrum of returns at each range. The FPGA implementation is then carefully optimized to better tradeoff performance and energy efficiency. Experimental results show our approach achieves better performance than conventional methods and exceed the requirement by more than 2.5×. Meanwhile our energy consumption is much lower than other platforms like GPU. Our optimization methods can be applied to other CNN structures for efficiency improvement.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458573","array processing;CNN;energy consumption;FPGA;radar signal processing","Radar clutter;Neural networks;Software algorithms;Training data;Signal processing algorithms;Radar signal processing;Software","","","","12","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient Zero-Activation-Skipping for On-Chip Low-Energy CNN Acceleration","M. Liu; Y. He; H. Jiao","VLSI Lab, School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China; Reconova Technologies Co., Ltd., Xiamen, China; VLSI Lab, School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","A new convolution paradigm is proposed for convolutional neural networks (CNNs) in this paper, which can efficiently skip the storage and computation of zeros in the input feature maps, by exploring the position information of the activations. Furthermore, by reusing the activations as much as possible, load balance is achieved among different processing elements without hardware cost. With the proposed sparse convolution technique, the calculation speed is enhanced by 7.29x for convolutional layers with a sparsity of 90% and by 2.59x for running VGG16 with ImageNet2012 dataset, compared to the traditional convolution method. Implemented in a UMC 55-nm low power CMOS technology, a CNN accelerator with the proposed technique achieves an effective energy efficiency of 1.94 TOPS/W while running at 100 MHz and 1.08 V supply voltage.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458578","Sparse matrix;convolution;sparse storage;input feature map;speedup;energy savings;energy efficiency","Convolution;Circuits and systems;Conferences;CMOS technology;Energy efficiency;Hardware;Computational efficiency","","5","","11","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"LOMA: Fast Auto-Scheduling on DNN Accelerators through Loop-Order-based Memory Allocation","A. Symons; L. Mei; M. Verhelst","Department of Electrical Engineering, MICAS, KU Leuven, Belgium; Department of Electrical Engineering, MICAS, KU Leuven, Belgium; Department of Electrical Engineering, MICAS, KU Leuven, Belgium",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","The scheduling or temporal mapping of a neural network (NN) on a given hardware (HW) accelerator strongly impacts its execution energy and latency. Unfortunately the mapping space is huge and varies a lot in function of the NN-HW combination. Many design space exploration (DSE) frameworks aim at automatically exploring this vast mapping space. Yet, SotA frameworks suffer from being slow (e.g. exhaustive search), inflexible across a wide range of HW architectures (e.g. no support for uneven mapping), or can’t guarantee global optimality (e.g. from relying on user-defined constraints or on random sampling). Moreover, existing frameworks are typically unable to predict the required CPU run-time and peak CPU memory requirements in advance and as such are unable to trade-off search time with optimality in a deterministic manner. This work proposes LOMA, a fast auto-scheduling methodology through loop-order-based memory allocation, which overcomes above bottlenecks. LOMA’s capabilities are demonstrated at scale in finding the optimal schedule of complete MobileNetV3, resp. NASNet.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458493","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458493","","Systematics;Runtime;Artificial neural networks;Optimal scheduling;Search engines;Prediction algorithms;Hardware","","11","","16","Crown","23 Jun 2021","","","IEEE","IEEE Conferences"
"Neural Network Acceleration and Voice Recognition with a Flash-based In-Memory Computing SoC","L. Zhao; S. Gao; S. Zhang; X. Qiu; F. Yang; J. Li; Z. Chen; Y. Zhao","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Flash Billion Semiconductor Co. Ltd, Shanghai, China; Flash Billion Semiconductor Co. Ltd, Shanghai, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Hefei Reliance Memory Ltd, Hefei, China; Hefei Reliance Memory Ltd, Hefei, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","5","AI inference based on novel compute-in-memory devices has shown clear advantages in terms of power, speed and storage density, making it a promising candidate for IoT and edge computing applications. In this work, we demonstrate a fully integrated system-on-chip (SoC) design with embedded Flash memories as the neural network accelerator. A series of techniques from device, design and system perspectives are combined to enable efficient AI inference for resource-constrained voice recognition. 7-bit/cell storage capability and self-adaptive write of novel Flash memories are leveraged to achieve state-of-the-art overall performance. Also, model deployment techniques based on transfer learning are explored to significantly improve the accuracy loss during weight data deployment. Integrated in a compact form factor, the whole voice recognition system can achieve >10 TOPS/W energy efficiency and ~95% accuracy for real-time keyword spotting applications.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458476","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458476","","Performance evaluation;Neural networks;Transfer learning;Speech recognition;Real-time systems;Energy efficiency;Data models","","10","","13","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Memory Efficient Invertible Neural Networks for Class-Incremental Learning","G. Hocquet; O. Bichler; D. Querlioz","CEA, LIST, Gif-sur-Yvette CEDEX, France; CEA, LIST, Gif-sur-Yvette CEDEX, France; Université Paris-Saclay, CNRS, Palaiseau, France",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Recent advances in specialized hardware accelerators for Deep Neural Networks (DNN) training are opening the way for an increasing use of DNN models in embedded systems. At the same time, as new data is continuously acquired, it is becoming a major challenge to build models that are able to deal with a continuous stream of data. The issue lies in how we can update a model without storing all the previous data. In this article, we are interested in the case of learning new classes in a sequential fashion. We propose to adapt the learning procedure of one-versus-all invertible neural networks, a state-of-the-art method in class-incremental learning, to reduce its memory impact. We conduct our experiments on the CIFAR-100 dataset for which we learn each class one after the other. Our results show that the proposed approach is able to perform with a similar accuracy whilst reducing the memory cost by a factor up to five compared to the original implementation.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458549","","Training;Adaptation models;Embedded systems;Quantization (signal);Conferences;Neural networks;Memory management","","","","19","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"An Energy-Efficient Quad-Camera Visual System for Autonomous Machines on FPGA Platform","Z. Wan; Y. Zhang; A. Raychowdhury; B. Yu; Y. Zhang; S. Liu",Georgia Institute of Technology; Beijing Institute of Technology; Georgia Institute of Technology; PerceptIn Inc; Beijing Institute of Technology; PerceptIn Inc,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","In our past few years' of commercial deployment experiences, we identify localization as a critical task in autonomous machine applications, and a great acceleration target. In this paper, based on the observation that the visual frontend is a major performance and energy consumption bottleneck, we present our design and implementation of an energy-efficient hardware architecture for ORB (Oriented-Fast and RotatedBRIEF) based localization system on FPGAs. To support our multi-sensor autonomous machine localization system, we present hardware synchronization, frame-multiplexing, and parallelization techniques, which are integrated in our design. Compared to Nvidia TX1 and Intel i7, our FPGA-based implementation achieves 5.6× and 3.4× speedup, as well as 3.0× and 34.6× power reduction, respectively. I.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458486","Semiconductor Research Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458486","","Location awareness;Visualization;Computer architecture;Visual systems;Energy efficiency;Hardware;Real-time systems","","7","","8","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Automated Tuning of End-to-end Neural Flight Controllers for Autonomous Nano-drones","V. Niculescu; L. Lamberti; D. Palossi; L. Benini","Integrated Systems Laboratory - ETH, Zürich, Switzerland; Department of Electrical, Electronic and Information Engineering, University of Bologna, Italy; Integrated Systems Laboratory - ETH, Zürich, Switzerland; Integrated Systems Laboratory - ETH, Zürich, Switzerland",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Convolutional neural networks (CNNs) are fueling the advancement of autonomous palm-sized drones, i.e., nano-drones, despite their limited power envelope and onboard processing capabilities. Computationally lighter than traditional geometrical approaches, CNNs are the ideal candidates to predict end-to-end signals directly from the sensor inputs to feed to the onboard flight controller. However, these sophisticated CNNs require significant complexity reduction and fine-grained tuning to be successfully deployed aboard a flying nano-drone. To date, these optimizations are mostly hand-crafted and require error-prone, labor-intensive iterative development flows. This work discusses methodologies and software tools to streamline and automate all the deployment stages on a low-power commercial multicore System-on-Chip. We investigate both an industrial closed-source and an academic open-source tool-set with a field-proofed state-of-the-art CNN for autonomous driving. Our results show a $2 \times$ reduction of the memory footprint and a speedup of $1.6 \times$ in the inference time, compared to the original hand-crafted CNN, with the same prediction accuracy.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458550","","Tools;Throughput;Complexity theory;System-on-chip;Software tools;Tuning;Open source software","","1","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"LPE: Logarithm Posit Processing Element for Energy-Efficient Edge-Device Training","Y. Wang; D. Deng; L. Liu; S. Wei; S. Yin","Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Recently, edge-device training has arisen an urgent necessity since it can enhance the model adaptability without causing high transmission cost and privacy issues. Due to the need for a wide data range and high data precision to improve accuracy, DNN training requires much wider floating-point (FP) data for convolution and complicated arithmetics for batch normalization. They lead to massive computation and memory access energy, which yields challenges for power-constrained edge-devices. This paper proposes a novel PE, called LPE, with three innovations to solve this issue. First, LPE stores the operands in the posit format, satisfying both precision and data range with lower bit-width. It reduces training latency and energy for memory access. Second, LPE transfers complicated arithmetics during training into the logarithm domain, including multiplication in convolution layer and division, square, square root in batch normalization layers. It reduces computation energy and improves throughput. Third, LPE contains a two-stage floating-point accumulation unit. It extends the computation range while using the low bit-width accumulator, enhancing precision and reducing power consumption. Evaluated with 28 nm CMOS process, our PE achieves 1.81× power and 1.35× area reduction compared with IEEE 754 float-point 16 (FP16) fused MAC while maintaining the same dynamic range. When performing training with the proposed PE unit, it can achieve 1.97× energy reduction and offer 1.68× speed up.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458421","Beijing Innovation Center for Future Chip; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458421","DNN Training;Posit;Processor;Edge-devices","Training;Technological innovation;Privacy;Power demand;Convolution;Random access memory;Transforms","","2","","7","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"HPPU: An Energy-Efficient Sparse DNN Training Processor with Hybrid Weight Pruning","Y. Wang; Y. Qin; L. Liu; S. Wei; S. Yin","Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Enlightened by the fact that deep-neural-networks (DNNs) are typically highly over-parameterized, weight-pruning-based sparse training (ST) becomes a practical method to reduce training computation and compress models. However, the previous pruning algorithms are either with a coarse-grained pattern or a fine-grained pattern. They lead to a limited pruning ratio or a drastically irregular sparsity distribution, which is computation-intensive or logic-complex for hardware implementation. Meanwhile, the current DNN processors focus on sparse inference but cannot support emerging ST techniques. This paper proposes a co-design approach where the algorithm is adapted to suit the hardware constraints and the hardware exploit the algorithm property to accelerate sparse training. We first present a novel pruning algorithm, hybrid weight pruning, including channel-wise and line-wise pruning. It reaches a considerable pruning ratio while maintaining the hardware friendly property. Then we design a hardware architecture, Hybrid Pruning Processing Unit, HPPU, to accelerate the proposed algorithm. It develops a 2-level active data selector and a sparse convolution engine, which maximize hardware utilization when handling the hybrid sparsity patterns during training. We evaluate HPPU by synthesizing it with 28nm CMOS technology. HPPU achieves 50.1% higher pruning ratio than coarse-grained pruning and 1.53× higher energy-efficiency than fine-grained pruning. The peak energy-efficiency of HPPU is 126.04TFLOPs/W, outperforming state-of-the-art trainable processor GANPU 1.67×. When training a ResNet18 model, HPPU consumes 3.72× less energy and offers 4.69× speedup, and maintains unpruned accuracy.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458410","Beijing Innovation Center for Future Chip; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458410","Weight pruning;sparse training;Deep Neural Network (DNN);energy efficient hardware","Training;Semiconductor device modeling;Program processors;Power demand;Convolution;Neural networks;Hardware","","6","","8","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Analyzing the Energy-Latency-Area-Accuracy Trade-off Across Contemporary Neural Networks","V. Jain; L. Mei; M. Verhelst","Department of Electrical Engineering, MICAS, KU Leuven, Belgium; Department of Electrical Engineering, MICAS, KU Leuven, Belgium; Department of Electrical Engineering, MICAS, KU Leuven, Belgium",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Deep learning provides a wide range of neural networks (NNs) with varying accuracy and complexity, which can be mapped to a wide variety of specialized hardware accelerators. This results in a huge optimization space across energy, latency, area and task accuracy, complicating the assessment of which NNs, resp. which hardware architectures are most efficient as it strongly depends on the hardware platform, resp. suite of algorithms to support. The ultimate goal is to find hardware supporting a broad suite of NNs and to construct NNs which can be efficiently deployed on broad set of hardware platforms. This paper, therefore, brings 3 contributions: 1) an assessment methodology across a suite of networks and architectures; 2) deployment of this analysis for a set of architecture on popular networks for the ImageNet task; 3) derivation of insights from this study on characteristic of optimal architectures for modern NN models, and characteristic of optimal network topologies for modern processor architectures.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458553","","Deep learning;Network topology;Conferences;Memory management;Artificial neural networks;Hardware;System-on-chip","","3","","11","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Design Tools for Resistive Crossbar based Machine Learning Accelerators","I. Chakraborty; S. Roy; S. Sridharan; M. Ali; A. Ankit; S. Jain; A. Raghunathan","School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN; Microsoft Corporation, Mountain View, CA; IBM Research, Yorktown Heights, NY; School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Resistive crossbar based accelerators for Machine Learning (ML) have attracted great interest as they offer the prospect of high density on-chip storage as well as efficient in-memory matrix-vector multiplication (MVM) operations. Despite their promises, they present several design challenges, such as high write costs, overhead of analog-to-digital and digital-to-analog converters and other peripheral circuits, and accuracy degradation due to the the analog nature of in-memory computing coupled with device and circuit level non-idealities. The unique characteristics of crossbar-based accelerators pose unique challenges for design automation. We outline a design flow for crossbar-based accelerators, and elaborate on some key tools involved in such a flow. Specifically, we discuss architectural estimation of metrics such as power, performance and area, and functional simulation to evaluate algorithmic accuracy considering the impact of non-idealities.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458433","","Performance evaluation;Machine learning algorithms;Digital-analog conversion;Estimation;Machine learning;Tools;System-on-chip","","2","","24","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Integer-Only Approximated MFCC for Ultra-Low Power Audio NN Processing on Multi-Core MCUs","M. Fariselli; M. Rusci; J. Cambonie; E. Flamand","Greenwaves Technologies; Greenwaves Technologies, Università di Bologna; Greenwaves Technologies; Greenwaves Technologies",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Given the recent advances in the design of efficient Deep Neural Networks (DNN) for tiny edge devices, the feature extraction frontend has become a computation bottleneck for enabling audio processing on low-end MicroController Units (MCUs). To address this challenge, this work presents novel hardware-aware integer quantization schemes for the MelFrequency Cepstral Coefficients (MFCC) feature extractor. Our high-precision integer-only 32 bit approximated flow does not lead to accuracy degradation with respect to a full-precision implementation when feeding multiple DNN models for Audio Keyword Spotting applications. In contrast, a second low-precision 16-bit approximated MFCC algorithm presents a 0.6% lower accuracy but results $3\times$ faster. Additionally, by leveraging on an 8-cores MCU, GAP8, our solution results $9.8\times$ faster than the full precision MFCC deployed on an FPU-suited MCU. When integrated within an optimized end-to-end system for Keyword Spotting, a GAP8-based audio smart device presents an overall power consumption as low as 3.4mW, demonstrating up to 35 days of lifetime with a single AA battery.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458491","MelSpectrogram;MFCC;Keyword Spotting;MicroControllers;Multi-Core;TinyML","Lead acid batteries;Degradation;Quantization (signal);Power demand;Microcontrollers;Feature extraction;Low-power electronics","","10","","10","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Smart Refrigerator Inventory Management Using Convolutional Neural Networks","T. -H. Lee; S. -W. Kang; T. Kim; J. -S. Kim; H. -J. Lee","Electrical and Computer Engineering, Seoul National University; Electrical and Computer Engineering, Seoul National University; Electrical and Computer Engineering, Seoul National University; Department of Electronic Engineering, SunMoon University; Department of Electronic Engineering, SunMoon University",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","This paper presents a deep learning-based inventory management system for smart refrigerators. Multiple wide-angle cameras are used for object recognition and inventory management in the proposed system. The proposed method generates augmented images by considering various positions, angles, and illumination conditions with automatically generated labels. The cameras mounted on the refrigerator side check for incoming and outgoing objects. The results indicate that the accuracy of object recognition improved when multiple cameras were used. The proposed system was tested with 25 types of objects that included several fruits and drinks. Evaluation reveals that the proposed inventory management system achieved an accuracy of 84.63% for a set of test cases.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458527","convolutional neural network;deep learning;smart refrigerator;fruit recognition","Deep learning;Refrigerators;Conferences;Lighting;Training data;Inventory management;Cameras","","4","","10","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"uPIM: Performance-aware Online Learning Capable Processing-in-Memory","S. Bavikadi; P. R. Sutradhar; A. Ganguly; S. M. P. Dinakarrao","Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Machine learning and AI-based automated systems are gaining increasing attention for real-time intelligent applications by virtue of a superior co-ordination between the software and the hardware within these systems. Although the majority of the automated systems are implementing Convolutional neural networks (CNNs), and Deep Neural Networks (DNNs) on the hardware with impressive accuracy, a significant amount of cost is associated with data movement in these platforms. Recent advancements in processing-in-memory (PIM), a non-von Neumann computing paradigm, have proven to be very effective in minimizing data communication overheads by performing computations within the memory chip. However, these devices are primarily designed as inference engines and therefore have not been adequately investigated for real-time learning capabilities for applications in changing environments. In this work, we introduce uPIM, a PIM architecture that supports a Generative Adversarial Network (GAN)-based performance-aware online learning model for updating the weights with minimal overheads. Our hardware-software co-design approach exhibits superior performance and efficiency in real-time applications like Autonomous Navigation Systems (ANS) by leveraging massive data-level parallelism and ultra-low data movement latency. The evaluations are performed on multiple state-of-the-art deep learning networks like LeNet, AlexNet, ResNet18, 34, 50 on the German Traffic Sign Recognition Benchmark (GTSRB) dataset and the Belgium Traffic Sign Dataset (BTSD) with several data-precisions. The proposed performance-aware, quantization-friendly online learning based PIM architecture achieves an average accuracy of 72% for GTSRB and 83.4% for BTSD dataset under varying environment for CNNs implemented for Traffic Sign Recognition (TSR) with 8-bit fixed point data-precision.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458575","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458575","","Performance evaluation;Neural networks;Computer architecture;Parallel processing;Generative adversarial networks;Real-time systems;Hardware","","7","","16","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Towards Smart and Efficient Health Monitoring Using Edge-enabled Situational-awareness","S. Shahhosseini; A. Kanduri; M. A. Mehrabadi; E. K. Naeini; D. Seo; S. -S. Lim; A. M. Rahmani; N. Dutt","University of California, Irvine, CA, USA; University of Turku, Turku, Finland; University of California, Irvine, CA, USA; University of California, Irvine, CA, USA; Kookmin University, Seoul, South Korea; Kookmin University, Seoul, South Korea; University of California, Irvine, CA, USA; University of California, Irvine, CA, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","IoT applications demand higher Quality of Experience (QoE) to deliver satisfactory services to the end-users. Variable user behavior, diverse application requirements, and dynamic system constraints exacerbate the QoE delivery challenge. Situational-awareness encapsulates the constraints at the user, application, and system levels, quantifiably reflecting system dynamics. Such models can manage diversity among multiple layers of the system stack while improving QoE. In this paper, we investigate the diverse set of requirements and opportunities in edge paradigm that vary over time using health monitoring applications. Towards address this challenge, we propose a situational-awareness model that enables intelligent orchestration of IoT applications across networked and distributed hardware platforms.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458477","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458477","Internet of Things;Health Monitoring;Intelligent System","System dynamics;Circuits and systems;Conferences;Hardware;Quality of experience;Internet of Things;Integrated circuit modeling","","3","","12","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"CoughNet: A Flexible Low Power CNN-LSTM Processor for Cough Sound Detection","H. -A. Rashid; A. N. Mazumder; U. P. K. Niyogi; T. Mohsenin","Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, USA; Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, USA; Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, USA; Department of Computer Science and Electrical Engineering, University of Maryland, Baltimore County, Baltimore, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","The continuing effect of COVID-19 pulmonary infection has highlighted the importance of machine-aided diagnosis for its initial symptoms such as fever, dry cough, fatigue, and dyspnea. This paper attempts to address the respiratory-related symptoms, using a low power scalable software and hardware framework. We propose CoughNet, a flexible low power CNN-LSTM processor that can take audio recordings as input to detect cough sounds in audio recordings. We analyze the three different publicly available datasets and use those as part of our evaluation to detect cough sound in audio recordings. We perform windowing and hyperparameter optimization on the software side with regard to fitting the network architecture to the hardware system. A scalable hardware prototype is designed to handle different numbers of processing engines and flexible bitwidth using Verilog HDL on Xilinx Kintex-7 160t FPGA. The proposed implementation of hardware has a low power consumption of o 290 mW and energy consumption of 2 mJ which is about $99 \times $ less compared to the state-of-the-art implementation.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458509","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458509","","Energy consumption;Power demand;Prototypes;Parallel processing;Audio recording;Hardware;Software","","11","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Energy Efficient Computing with Heterogeneous DNN Accelerators","M. S. Hossain; I. Savidis","Integrated Circuits and Electronics Laboratory, Drexel University, U.S.A.; Integrated Circuits and Electronics Laboratory, Drexel University, U.S.A.",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","The exploration of custom deep neural network (DNN) based accelerators for highly energy constrained edge devices with on-device intelligence is gaining traction in the research community. Despite the superior throughout and performance of custom accelerators as compared to CPUs or GPUs, the energy efficiency and versatility of state-of-the-art DNN accelerators is constrained due to the limited scope of monolithic architectures, where the entire accelerator executes only one model at any given time. In this paper, a multi-voltage domain heterogeneous DNN accelerator architecture is proposed that simultaneously executes multiple models with different power-performance operating points. The proposed architecture and circuits are evaluated with SPICE simulation in a 65 nm CMOS technology. The simulation results indicate that the proposed heterogeneous architecture improves the energy efficiency to 2.04 TOPS/W, while the conventional monolithic and single voltage domain architecture exhibits an energy efficiency of 0.0458 TOPS/W. In addition, the total power consumption of the accelerator SoC is reduced to 1.34 W as compared to the 3.72 W consumed by the baseline architecture when all multiply-and-accumulate (MACs) units operate at a voltage of 0.45 V.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458474","","Semiconductor device modeling;Power demand;Computational modeling;Memory management;Neural networks;Computer architecture;Throughput","","1","","9","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Evaluation of Machine Learning-based Detection against Side-Channel Attacks on Autonomous Vehicle","H. Wang; S. Salehi; H. Sayadi; A. Sasan; T. Mohsenin; P. D. Sai Manoj; S. Rafatirad; H. Homayoun",University of California at Davis; University of California at Davis; California State University at Long Beach; George Mason University; University of Maryland at Baltimore County; George Mason University; University of California at Davis; University of California at Davis,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Autonomous vehicles are becoming increasingly popular, but their reliance on computer systems to sense and operate in the physical world has introduced new security risks. Recent studies have shown that using Cache-based Side-Channel Attacks (SCAs) could infer sensitive users' information (e.g., which route the user is taking) highlighting significant vulnerability posed to today's computer systems. As a result, it is crucial to propose effective detection mechanisms against emerging microarchitectural SCAs on autonomous driving systems. In response, we first identify the threat model and victim applications of autonomous driving systems in this work. Next, we explore the suitability of various machine learning-based classifiers trained by information collected from built-in hardware performance counter registers available in modern autonomous vehicle systems. To this end, various supervised machine learning models are implemented for cache-based SCAs detection and precisely compared and characterized in terms of detection accuracy, robustness, and latency of the detection. Our experiments conducted on an Intel Xeon, which Waymo autonomous driving vendor uses, demonstrate that J48 achieves 99.5% accuracy with the highest efficiency compared with other investigated models.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458488","Autonomous vehicles;side-channel attacks;machine learning;hardware performance counters","Microarchitecture;Systematics;Program processors;Computational modeling;Side-channel attacks;Hardware;Robustness","","4","","24","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Trends in Analog and Digital Intensive Compute-in-SRAM Designs","R. Sehgal; J. P. Kulkarni","Dept. of Electrical and Computer Engineering, University of Texas at Austin, Austin, Texas; Dept. of Electrical and Computer Engineering, University of Texas at Austin, Austin, Texas",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","The unprecedented growth in Deep Neural Networks (DNN) model size has resulted into a massive amount of data movement from off-chip memory to on-chip processing cores in modern Machine Learning (ML) accelerators. Compute-In-Memory (CIM) designs performing DNN computations within memory arrays are being explored to mitigate this `Memory Wall' bottleneck of latency and energy overheads. Among the incumbent embedded memories, the Static Access Random Memory (SRAM) built using high performance logic transistors and interconnects can enable custom CIM designs while offering low pJ/bit access energy, high-endurance, high-performance, and high-bandwidth. The wordline and bitline voltages and pulse-widths are modulated to realize analog or digital domain multiply-and-accumulate (MAC) computations using multiple SRAM bitcell variants. This paper describes the trends in recent CIM-SRAM designs utilizing such analog and digitally-intensive approaches. In an analog CIM-SRAM design, the inputs/activations are transformed into analog voltage or pulsewidth and applied on wordlines and/or bitlines. Multibit MAC computations often involve peripheral data converter circuits which need to be optimized significantly to minimize the area and energy overheads. On the other hand, digitally-intensive CIM-SRAM approaches try to avoid analog circuits by implementing smaller bit-width wordline/bitline computations and utilize sense amplifiers for performing basic logic operations and/or employ small digital logic block next to the SRAM column I/O circuits forming compute-in/near SRAM designs. Key design trends in both the approaches and qualitative comparisons are presented with a perspective on future CIM-SRAM designs.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458576","","Conferences;Neural networks;Memory management;Random access memory;Integrated circuit interconnections;Machine learning;Market research","","6","","21","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Flexible-width Bit-level Compressor for Convolutional Neural Network","J. Zhu; X. Chen; L. Du; H. Geng; Y. Bai; Y. Li; Y. Du; Z. Wang","School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","In this paper, a weight compression technique named Flexible-width Bit-level (FWBL) coding is proposed to compress convolutional neural networks (CNN) models without re-training. FWBL splits the weight parameters into independent size-optimized blocks and uses just-enough bits for each block. Bit-level run-length coding is employed on high bits (HBs) to further compress the redundancy due to non-uniformly distributed weights. We implemented a configurable hardware decoder and synthesize it with TSMC 28nm technology. Results show that FWBL achieves an average compression ratio of 1.6 which is close to the Huffman coding. The decoder has a throughput of 3.7GBps running at 1.1GHz, with a power dissipation of 3.55mW, which is 17.9x and 21x better in throughput and energy efficiency compared with the prior work. Implemented in FPGA, our decoder is 3.36x and 4.96x better than various Huffman decoders in throughput and area efficiency, making it a promising weight compression technique for mobile CNN applications.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458411","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458411","compression;decoder;neural network weight;hardware","Redundancy;Throughput;Energy efficiency;Encoding;Hardware;Decoding;Power dissipation","","6","","16","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"On-chip Pixel Reconstruction using Simple CNN for Sparsely Read CMOS Image Sensor","W. Kisku; A. Kaur; D. Mishra","Department of Electrical Engineering, Indian Institute of Technology, Jodhpur, Rajasthan, India; Department of Electrical Engineering, Indian Institute of Technology, Jodhpur, Rajasthan, India; Department of Computer Science and Engineering, Indian Institute of Technology, Jodhpur, Rajasthan, India",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","5","CMOS image sensors have gained popularity due to low power consumption, high speed and their ability to scale to smaller sizes. These factors can further be improved by either improving the circuitry of the imager or by enhancing the processing capabilities of the entire imaging system through on-chip processing of the pixel information. One such improvement is to read only a small fraction of the pixel array information and reconstruct the original information from it. The on-chip prediction of the unread pixels to reconstruct the acquired image is still an open problem. Here we propose a solution to the problem which relies on on-chip implementation of simple convolutional neural network (CNN) for pixel prediction and image reconstruction. The system strives to selectively read only a minimal number of pixels (10% to 15%), while skipping the rest. This attributes to a considerable saving on power and checks the issue of latency at the ADC. This selective process in reading of only few pixels generates a sparse image signal. The proposed network is trained to reconstruct the estimation of the actual image from the sparse set. The hardware accelerator that incorporates the CNN model also considers hardware limitations and constraints such as power consumption and latency. The PSNR achieved for the SiDCNet variant with SqueezeNet achieves for test images is $\sim$30dB for 85% of pixels skipped, and the quantized model does not lose much in PSNR.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458532","Deep Learning;Convolutional Neural Networks;Compressed Sensing;System On-Chip;Image Sensors","Semiconductor device modeling;Power demand;CMOS image sensors;Hardware;Sensor systems;System-on-chip;Servers","","2","","19","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Energy-Efficient Deep Reinforcement Learning Accelerator Designs for Mobile Autonomous Systems","J. Lee; C. Kim; D. Han; S. Kim; S. Kim; H. -J. Yoo","School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Deep reinforcement learning (DRL) is widely used for autonomous systems including autonomous driving, robots, and drones. DRL training is essential for human-level control and adaptation to rapidly changing environments in mobile autonomous systems. However, acceleration of DRL training has three challenges: 1) large memory access, 2) various data patterns, 3) complex data dependency due to utilization of multiple DNNs. Two CMOS DRL accelerators have been proposed to support high speed, high energy-efficiency DRL training in mobile autonomous systems. One accelerator handles different data patterns with transposable PE architecture and reduces large feature map memory access with top-3 experience compression. The other accelerator supports group-sparse training for weight compression and integrates the on-line DRL task scheduler to support multi-DNNs operations.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458435","Deep reinforcement learning (DRL);edge devices;energy-efficient training accelerators;compression","Training;Circuits and systems;Conferences;Reinforcement learning;Energy efficiency;Mobile handsets;Task analysis","","4","","18","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"Demography-aware COVID-19 Confinement with Game Theory","S. Kasarapu; R. Hassan; S. Rafatirad; H. Homayoun; S. M. P. Dinakarrao","Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA; Department of Electrical and Computer Engineering, University of California Davis, Davis, CA, USA; Department of Electrical and Computer Engineering, University of California Davis, Davis, CA, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","In the last decades, emerging and re-emerging epidemics such as AIDS, measles, SARS, HINI influenza, and tuberculosis cause death to millions of people each year. In response, a large and intensive research is evolving for the design of better drugs and vaccines. However, studies warn that the new pandemics such as Coronavirus (COVID-19) and even deadly pandemics can emerge in the future. The existing confinement approaches rely on large amount of available data to determine policies. Such dependencies could cause an irreversible effect before proper strategies are developed. Furthermore, the existing approaches follow a one-size fits all approach, which might not be effective. In contrast, we develop a game-theory inspired approach that considers societal and economic impacts and formulates the epidemic control as a non-zero sum dynamic game. Further, the proposed approach considers the demographic information leading to providing a tailored solution to each demography. We explore different strategies including masking, social distancing, contact tracing, quarantining, partial-, and full-lockdowns and their combinations and present demography-aware optimal solutions to confine a pandemic with minimal history information and optimal impact on economy.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458525","","COVID-19;Economics;Pandemics;Influenza;Human factors;Games;Vaccines","","1","","14","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
"An Energy-Efficient Hardware Accelerator for Hierarchical Deep Reinforcement Learning","A. Shiri; B. Prakash; A. N. Mazumder; N. R. Waytowich; T. Oates; T. Mohsenin","Department of Computer Science & Electrical Engineering, University of Maryland Baltimore County; Department of Computer Science & Electrical Engineering, University of Maryland Baltimore County; Department of Computer Science & Electrical Engineering, University of Maryland Baltimore County; US Army Research Laboratory; Department of Computer Science & Electrical Engineering, University of Maryland Baltimore County; Department of Computer Science & Electrical Engineering, University of Maryland Baltimore County",2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),"23 Jun 2021","2021","","","1","4","Reinforcement Learning (RL) has shown great performance in solving sequential decision-making and control in dynamic environments problems. Despite its achievements, training Deep Neural Network (DNN) based RL is expensive in terms of time and power because of the large number of episodes required to train agents with high dimensional image representations. At the deployment also, the massive energy footprint of deep neural networks can be a major drawback. Embedded devices as the main deployment platform, are intrinsically resource-constrained and deploying DNN on them is challenging. Consequently, reducing the number of actions taken by the RL agent to learn desired policy, along with the development of efficient hardware architectures for RL is crucial. In this paper, we propose a novel hardware architecture for RL agents based on the learning hierarchical policies method. We show that hierarchical learning with several levels of control improves RL agents training efficiency and the agent converges faster compared to a none hierarchical model and therefore using less power. This is especially true as the environment becomes more complex with multiple objective sub-goals. Our method is important for efficient learning of policies for RL agent, especially when the target platform is a resource constraint embedded device. By performing a systematic neural network architecture search and hardware design space exploration, we implemented an energy-efficient scalable hardware accelerator for the hierarchical RL. Hardware factors of merit such as the latency, throughput, and energy consumption of the accelerator are evaluated with the various processing elements, and model parameters. The most energy-efficient configuration achieves 139 fps throughput with 5.8 mJ energy consumption per classification on Xilinx Artix-7 FPGA. Compared to similar works our design shows up to 3x better energy efficiency.","","978-1-6654-1913-0","10.1109/AICAS51828.2021.9458548","Arm; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9458548","Reinforcement Learning;Energy Efficient Hardware;FPGA;ASIC","Training;Energy consumption;Systematics;Neural networks;Memory management;Reinforcement learning;Throughput","","2","","15","IEEE","23 Jun 2021","","","IEEE","IEEE Conferences"
